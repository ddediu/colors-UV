---
title: "Environment and culture shape both the colour lexicon and the genetics of colour perception"
subtitle: "Supporting information: full analyses and plots"
author: "Mathilde Josserand [mathilde.josserand@gmail.fr], Emma Meeussen, Asifa Majid, and Dan Dediu [ddediu@gmail.com]"
date: "`r date()`"
output:
  html_document:
    highlight: textmate
    theme: journal
    toc: yes
    toc_depth: 6
    toc_float: yes
  word_document: default
  pdf_document:
    toc: yes
    toc_depth: '6'
editor_options:
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa-6th-edition.csl
---

```{r license, include=FALSE, eval=FALSE}
    Code and data accompanying the paper "Environment and culture shape 
    both the colour lexicon and the genetics of colour perception"
    Copyright (C) 2019-2021  Dan Dediu and Mathilde Josserand

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
```

```{r setup, echo=F, message=F, warning=FALSE}
## Knitting options:
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE,                   # default code chunk options
                      fig.width=11, fig.height=6, fig.align="center", comment=NA, # default figure dimensions
                      fig.path="./figures/",                                      # save images to ./figures/
                      dpi=70, dev="jpeg",                                         # please set dpi=300 and comment out dev="jpeg" for high resolution but very big images
                      cache=TRUE, autodep=TRUE);                                  # cache chunks


## Load needed packages:

# Rmarkdown:
library(knitr);
library(pander);

# System/parallel processing:
library(parallel);
library(future);
library(pbapply); # lapply with progress bar

# Data processing:
library(plyr);
library(dplyr);
library(reshape2);

# Plotting:
library(ggplot2);
library(ggrepel); # reduce clutter with text
library(gridExtra); # combine multiple plots
library(ggnewscale); # multiple color scales in the same plot
library(DiagrammeR); # box and arrow diagramns
library(grid); # assemble multiple plots

# Spatial stats and maps:
library(maptools); # as.im.RasterLayer()
library(spatstat); # ppp()
library(scanstatistics); # dist_to_knn() # currently (july 2020) it needs to be manually installed from the CRAN archive https://cran.r-project.org/src/contrib/Archive/scanstatistics/
library(ape); # Moran.I()
library(spdep); # tri2nb()
library(deldir); # deldir()
library(igraph); # graph()
library(Imap); # gdist()
library(geosphere); # geographic distances
library(maps)

# Regression and mediation analysis:
library(lmerTest); # ML mixed-effects models
library(brms); # Bayesian mixed-effects models with Stan
library(bayestestR); # Bayesian model comparison
library(sjstats); # mediation
library(performance); # multicollinearity

# DAGs:
#library(dagitty); # interferes with others, so load it where needed

# Path analysis/SEM:
library(lavaan); # fit models
library(lavaanPlot); # plot models
library(semPlot);

# Decison trees, random forests, and SVMs:
library(caret); # confusionMatrix()
library(partykit); # ctree() and cforest()
library(randomForest); # randomForest()
library(rsample); # initial_split()
library(rminer); # SVMs
#library(e1071); # SVMs
#library(tree);
#library(rpart);
#library(rpart.plot);


# Apparently not needed:
#library(gdata);
#library(maps);
#library(sna);
#library(fields);
#library(tidyr);
#library(geostatsp);
#library(MuMIn);
#library(maps);
#library(data.table);
#library(magrittr);
#library(sfsmisc);
#library(MASS);
#library(rsq);
#library(mediation);
#library(car);
#library(ROCR);
#library(ISLR)
#library(rsample);
#library(FactoMineR);


## Check that the input data exists:
if( !dir.exists("./input_files/") ) stop("The input files folder 'input_files' does not seem to exist!\n");
# ... and (re)generate any missing input files:
if( !all(file.exists("./input_files/data_climate.tsv", 
                     "./input_files/data_humidity.tsv", 
                     "./input_files/data_dist2water.tsv", 
                     "./input_files/data_UV_incidence.tsv",
                     "./input_files/data_elevation.tsv",
                     "./input_files/data_genetics.tsv")) )
{
  # Get the primary data and compute them:
  source("./00_preprocess_data.R", echo=FALSE);
}


## Set seed for reproducibility:
set.seed(42);

## For Bayesian regressions with brms (some might be different for particular models to avoid too few or too many iterations):
brms_ncores  <- max(detectCores(all.tests=TRUE, logical=FALSE), 4, na.rm=TRUE); # try to use multiple cores, if present
brms_iter    <- 5000; brms_warmup <- 2000; brms_thin <- 2; # burn-in and iterations 
brms_control <- list(adapt_delta=0.999, max_treedepth=15); # control params to pass to Stan: avoid the "divergent transitions after warmup" and "max_treedepth" warnings
brms_ci      <- 0.89; brms_rope <- c(-0.01, 0.01); # 89% HDI and a tight ROPE around 0.0 [-0.01, 0.01]
#plan(multiprocess); # parallel kfold using futures

## Various folders:
if( !dir.exists("./figures") ) dir.create("./figures", showWarnings=FALSE); # figures are saved here
if( !dir.exists("./cached_results") ) dir.create("./cached_results", showWarnings=FALSE); # cache expensive results here


## Auxiliary functions:

# Figure and Table caption adapted from https://stackoverflow.com/questions/37116632/rmarkdown-html-number-figures: 
outputFormat = opts_knit$get("rmarkdown.pandoc.to"); # determine the output format of the document
if( is.null(outputFormat) ) outputFormat = ""; # probably not run within knittr
capTabNo = 1; capFigNo = 1; # figure and table caption numbering, for HTML do it manually
#Function to add the Table Number
capTab = function(x){
  if(outputFormat == 'html'){
    x = paste0("**Table ",capTabNo,".** ",x,"")
    capTabNo <<- capTabNo + 1
  }; x
}
#Function to add the Figure Number
capFig = function(x){
  if(outputFormat == 'html'){
    x = paste0("**Figure ",capFigNo,".** ",x,"")
    capFigNo <<- capFigNo + 1
  }; x
}

# Verbal interpretation of Bayes factors (BF):
BF_interpretation <- function(BF, model1_name="m1", model2_name="m2")
{
  if( BF > 100 )   return (paste0("extreme evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 30 )    return (paste0("very strong evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 10 )    return (paste0("strong evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 3 )     return (paste0("moderate evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 1 )     return (paste0("anecdotal evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF == 1 )    return (paste0("no evidence for ",model1_name," nor ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 0.33 )  return (paste0("anecdotal evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 0.10 )  return (paste0("moderate evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 0.033 ) return (paste0("strong evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 0.010 ) return (paste0("very strong evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
  return (paste0("extreme evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
}

# Here I hack brms' kfold code to make it run in parallel using good old mclapply instead of futures
# this avoid random crashes which seem to be due to future, but works only on *NIX (which, for me here, is not an issue)
# Adapted the code from https://github.com/paul-buerkner/brms/blob/master/R/loo.R and https://github.com/paul-buerkner/brms/blob/master/R/kfold.R
if( Sys.info()['sysname'] == "Windows" )
{
  # In Windows, fall back to the stadard implementation in brms:
  add_criterion_kfold_parallel <- function(model, K=10, chains=1)
  {
    return (add_criterion(model, criterion="kfold", K=K, chains=chains));
  }
} else
{
  # On anything else, try to use maclapply:
  add_criterion_kfold_parallel <- function(model, K=10, chains=1)
  {
    model <- restructure(model);
  
    mf <- model.frame(model); 
    attributes(mf)[c("terms", "brmsframe")] <- NULL;
    N <- nrow(mf);
    fold_type <- "random"; folds <- loo::kfold_split_random(K, N);
    Ksub <- seq_len(K);
  
    kfold_results <- mclapply(Ksub, function(k) 
    {
      omitted <- predicted <- which(folds == k);
      mf_omitted <- mf[-omitted, , drop=FALSE];
      
      if( exists("subset_data2", envir=asNamespace("brms")) )
      {
        # Newer versions of brms:
        model_updated <- base::suppressWarnings(update(model, newdata=mf_omitted, data2=brms:::subset_data2(model$data2, -omitted), refresh=0, chains=chains));
        
        lppds <- log_lik(model_updated, newdata=mf[predicted, , drop=FALSE], newdata2=brms:::subset_data2(model$data2, predicted), 
                         allow_new_levels=TRUE, resp=NULL, combine=TRUE, chains=chains);
      } else if( exists("subset_autocor", envir=asNamespace("brms")) )
      {
        # Older versions of brms:
        model2 <- brms:::subset_autocor(model, -omitted, incl_car=TRUE);
        model_updated <- base::suppressWarnings(update(model2, newdata=mf_omitted, refresh=0, chains=chains));
        
        lppds <- log_lik(model_updated, newdata=mf[predicted, , drop=FALSE], allow_new_levels=TRUE, resp=NULL, combine=TRUE, chains=chains);
      } else
      {
        stop("Unknown version of brms!");
      }
  
      return (list("obs_order"=predicted, "lppds"=lppds));
    }, mc.cores=ifelse(exists("brms_ncores"), brms_ncores, detectCores()));
    
    # Put them back in the form expected by the the following unmodifed code:
    obs_order <- lapply(kfold_results, function(x) x$obs_order);
    lppds     <- lapply(kfold_results, function(x) x$lppds);
    
    elpds <- brms:::ulapply(lppds, function(x) apply(x, 2, brms:::log_mean_exp))
    # make sure elpds are put back in the right order
    elpds <- elpds[order(unlist(obs_order))]
    elpd_kfold <- sum(elpds)
    se_elpd_kfold <- sqrt(length(elpds) * var(elpds))
    rnames <- c("elpd_kfold", "p_kfold", "kfoldic")
    cnames <- c("Estimate", "SE")
    estimates <- matrix(nrow = 3, ncol = 2, dimnames = list(rnames, cnames))
    estimates[1, ] <- c(elpd_kfold, se_elpd_kfold)
    estimates[3, ] <- c(-2 * elpd_kfold, 2 * se_elpd_kfold)
    out <- brms:::nlist(estimates, pointwise = cbind(elpd_kfold = elpds))
    atts <- brms:::nlist(K, Ksub, NULL, folds, fold_type)
    attributes(out)[names(atts)] <- atts
    out <- structure(out, class = c("kfold", "loo"))
    
    attr(out, "yhash") <- brms:::hash_response(model, newdata=NULL, resp=NULL);
    attr(out, "model_name") <- "";
    
    model$criteria$kfold <- out;
    model;
  }
}

# Bayesian fit indices for a given model:
brms_fit_indices <- function(model, indices=c("bayes_R2", "loo", "waic", "kfold"), K=10, verbose=TRUE)
{
  if( "bayes_R2" %in% indices )
  {
    if( verbose) cat("R^2...\n");
    #attr(model, "R2") <- bayes_R2(model); 
    model <- add_criterion(model, "bayes_R2"); 
  } else
  {
    # Remove the criterion (if already there):
    if( !is.null(model$criteria) && "bayes_R2" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "bayes_R2") ]] <- NULL;
  }
  
  if( "loo" %in% indices )
  {
    if( verbose) cat("LOO...\n");
    model <- add_criterion(model, "loo"); 
  } else
  {
    # Remove the criterion (if already there):
    if( !is.null(model$criteria) && "loo" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "loo") ]] <- NULL;
  }
  
  if( "waic" %in% indices )
  {
    if( verbose) cat("WAIC...\n");
    model <- add_criterion(model, "waic"); 
  } else
  {
    # Remove the criterion (if already there):
    if( !is.null(model$criteria) && "waic" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "waic") ]] <- NULL;
  }
  
  if( "kfold" %in% indices )
  {
    if( verbose) cat(paste0("KFOLD (K=",K,")...\n"));
    #model <- add_criterion(model, "kfold", K=K, chains=1);
    model <- add_criterion_kfold_parallel(model, K=K, chains=1);
  } else
  {
    # Remove the criterion (if already there):
    if( !is.null(model$criteria) && "kfold" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "kfold") ]] <- NULL;
  }

  gc();
  
  return (model);
}

# Bayesian model comparison:
brms_compare_models <- function(model1, model2, name1=NA, name2=NA, bayes_factor=TRUE, print_results=TRUE)
{
  if( !is.null(model1$criteria) && "bayes_R2" %in% names(model1$criteria) && !is.null(model1$criteria$bayes_R2) &&
      !is.null(model2$criteria) && "bayes_R2" %in% names(model2$criteria) && !is.null(model2$criteria$bayes_R2) )
  {
    R2_1_2 <- (mean(model1$criteria$bayes_R2) - mean(model2$criteria$bayes_R2));
  } else
  {
    R2_1_2 <- NA;
  }
  
  if( bayes_factor )
  {
    invisible(capture.output(bf_1_2 <- brms::bayes_factor(model1, model2)$bf));
    bf_interpret_1_2 <- BF_interpretation(bf_1_2, ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")); 
  }
  else
  {
    bf_1_2 <- NULL; bf_interpret_1_2 <- NA;
  }
  
  if( !is.null(model1$criteria) && "loo" %in% names(model1$criteria) && !is.null(model1$criteria$loo) &&
      !is.null(model2$criteria) && "loo" %in% names(model2$criteria) && !is.null(model2$criteria$loo) )
  {
    loo_1_2 <- loo_compare(model1, model2, criterion="loo", model_names=c(ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")));
  } else
  {
    loo_1_2 <- NA;
  }
  
  if( !is.null(model1$criteria) && "waic" %in% names(model1$criteria) && !is.null(model1$criteria$waic) &&
      !is.null(model2$criteria) && "waic" %in% names(model2$criteria) && !is.null(model2$criteria$waic) )
  {
    waic_1_2 <- loo_compare(model1, model2, criterion="waic", model_names=c(ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")));
    mw_1_2 <- model_weights(model1, model2, weights="waic", model_names=c(ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")));
  } else
  {
    waic_1_2 <- NA; 
    mw_1_2 <- NA;
  }
  
  if( !is.null(model1$criteria) && "kfold" %in% names(model1$criteria) && !is.null(model1$criteria$kfold) &&
      !is.null(model2$criteria) && "kfold" %in% names(model2$criteria) && !is.null(model2$criteria$kfold) )
  {
    kfold_1_2 <- loo_compare(model1, model2, criterion="kfold", model_names=c(ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")));
  } else
  {
    kfold_1_2 <- NA;
  }
  
  if( print_results )
  {
    cat(paste0("\nComparing models '",ifelse(!is.na(name1), name1, "model1"),"' and '",ifelse(!is.na(name2), name2, "model2"),"':\n\n"));
    cat(paste0("\ndelta R^2 = ",sprintf("%.1f%%",100*R2_1_2),"\n\n"));
    cat(bf_interpret_1_2,"\n\n");
    cat("\nLOO:\n"); print(loo_1_2);
    cat("\nWAIC:\n"); print(waic_1_2);
    cat("\nKFOLD:\n"); print(kfold_1_2);
    cat("\nModel weights (WAIC):\n"); print(mw_1_2);
    cat("\n");
  }
  
  gc();
  
  return (list("R2"=R2_1_2, "BF"=bf_1_2, "BF_interpretation"=bf_interpret_1_2, "LOO"=loo_1_2, "WAIC"=waic_1_2, "KFOLD"=kfold_1_2, "model_weights_WAIC"=mw_1_2));
}

# Standard error of the mean:
std <- function(x) sd(x)/sqrt(length(x))

# Root Mean Square Error (RMSE) between observed (y) and predicted (yrep) values:
rmse <- function(y, yrep)
{
  yrep_mean <- colMeans(yrep)
  sqrt(mean((yrep_mean - y)^2))
}

# Panel function for the pairwise plots:
panel.cor <- function(x, y){
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- cor.test(x, y, method="pearson");
  rho <- cor.test(x, y, method="spearman");
  txt <- sprintf("r=%.2f\n(p=%.4g)\nrho=%.2f\n(p=%.4g)", 
                 r$estimate, r$p.value, rho$estimate, rho$p.value)
  cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex=1.2)
}
upper.panel<-function(x, y){
  points(x,y, pch = 21, bg="lightgray", col="gray30")
  abline(lm(y ~ x), col="blue", lwd=2);
}
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "lightgray", ...)
}


## Draw diagrams:

# Draw an edge depending on its regression estimate
.gr_edge <- function(model, iv, precision=2, col_neg="blue", lty_neg="solid", col_pos="red", lty_pos="solid", col_null="gray", lty_null="dashed")
{
  if( is.null(model) )
  {
    paste0('[label = "ns", style = "',lty_null,'", color = "',col_null,'"]');
  } else
  {
    v <- fixef(model)[iv, "Estimate"];
    if( is.null(v) )
    {
      paste0('[label = "ns", style = "',lty_null,'", color = "',col_null,'"]');
    } else
    {
      paste0('[label = "',round(v, precision),'", style = "',ifelse(v<0,lty_neg,lty_pos),'", color = "',ifelse(v<0,col_neg,col_pos),'"]');
    }
  }
}


# Generalise the code in sjstats:::mediation.brmsfit() for more than one mediator:
.mediation_multiple_mediators_brms <- function(model, response, treatment, mediators, prob=0.95, typical="median", rope.range="default")
{
  # Get the DVs, the treatment and the mediation models:
  dv <- insight::find_response(model, combine = TRUE);
  treatment <- sjstats:::fix_factor_name(model, treatment);
  mediator.models <- which(dv %in% mediators); treatment.model <- which(!(dv %in% mediators));
  mediators <- vapply(mediators, function(x) sjstats:::fix_factor_name(model, x), character(1));
  dv <- names(dv);
  
  # Get the coefficient for the treatment and the direct effect:
  coef_treatment <- sprintf("b_%s_%s", dv[treatment.model], treatment);
  eff.direct <- model %>% brms::posterior_samples(pars=coef_treatment, fixed=TRUE) %>% dplyr::pull(1);
  eff.direct_HDI <- bayestestR::hdi(eff.direct, ci=prob);
  
  # Get the coefficients for the mediators and their effects:
  coef_mediators <- sprintf("b_%s_%s", dv[treatment.model], mediators);
  eff.mediators <- lapply(coef_mediators, function(x) model %>% brms::posterior_samples(pars=x, fixed=TRUE) %>% dplyr::pull(1)); names(eff.mediators) <- coef_mediators;
  
  # Get the coefficients for the indirect effects and their estimates:
  coef_indirects <- sprintf("b_%s_%s", dv[mediator.models], treatment);
  tmp.indirects <- lapply(seq_along(coef_indirects), function(i) brms::posterior_samples(model, pars=c(coef_indirects[i], coef_mediators[i]), fixed=TRUE)); names(tmp.indirects) <- coef_indirects;
  eff.indirects <- do.call(cbind, lapply(tmp.indirects, function(x) x[,1] * x[,2]));
  eff.indirects_HDI <- do.call(rbind, lapply(1:ncol(eff.indirects), function(i) bayestestR::hdi(eff.indirects[,i], ci=prob)));
  
  # Get the total effect:
  eff.total <- rowSums(eff.indirects) + eff.direct;
  eff.total_HDI <- bayestestR::hdi(eff.total, ci=prob);
  
  # The proportion mediated:
  prop.mediated <- apply(eff.indirects, 2, function(x) sjmisc::typical_value(x, fun=typical)) / sjmisc::typical_value(eff.total, fun=typical);
  
  # The HDI and standard errors:
  hdi_eff <- lapply(1:ncol(eff.indirects), function(i) bayestestR::hdi(eff.indirects[,i] / eff.total, ci=prob)); names(hdi_eff) <- colnames(eff.indirects);
  prop.se <- vapply(hdi_eff, function(x) (x$CI_high - x$CI_low)/2, numeric(1));
  prop.hdi <- do.call(rbind, lapply(1:length(prop.se), function(i) prop.mediated[i] + c(-1, 1) * prop.se[i]));
  
  # The return value as a data.frame that contains all the important info:
  ret_val <- data.frame("effect"  =c("total",                             # total  
                                     "direct",                            # direct  
                                     rep("indirect", length(mediators))), # indirect
                        "mediator"=c(NA,         # total 
                                     NA,         # direct       
                                     mediators), # indirect
                        "estimate"=c(sjmisc::typical_value(eff.total, fun=typical),                                                                 # total 
                                     sjmisc::typical_value(eff.direct, fun=typical),                                                                # direct
                                     vapply(1:ncol(eff.indirects), function(i) sjmisc::typical_value(eff.indirects[,i], fun=typical), numeric(1))), # indirect
                        "estimate_HDI_low" =c(eff.total_HDI$CI_low[1],   # total  
                                              eff.direct_HDI$CI_low[1],  # direct 
                                              eff.indirects_HDI$CI_low), # indirect
                        "estimate_HDI_high"=c(eff.total_HDI$CI_high[1],   # total 
                                              eff.direct_HDI$CI_high[1],  # direct 
                                              eff.indirects_HDI$CI_high), # indirect
                        "p_ROPE"=c(rope(eff.total, range=rope.range, ci=prob)$ROPE_Percentage[1],                                                                 # total
                                   rope(eff.direct, range=rope.range, ci=prob)$ROPE_Percentage[1],                                                                # direct
                                   vapply(1:ncol(eff.indirects), function(i) rope(eff.indirects[,i], range=rope.range, ci=prob)$ROPE_Percentage[1], numeric(1))), # indirect
                        "proportion"=c(1.00,                      # total  
                                       1.00 - sum(prop.mediated), # direct
                                       prop.mediated),            # indirect
                        "proportion_HDI_low" =c(1.00,                     # total  
                                                1.00 - sum(prop.hdi[,2]), # direct
                                                prop.hdi[,1]),            # indirect
                        "proportion_HDI_high"=c(1.00,                     # total  
                                                1.00 - sum(prop.hdi[,1]), # direct
                                                prop.hdi[,2])             # indirect
  );
  attr(ret_val, "response") <- response;
  attr(ret_val, "treatment") <- treatment;
  attr(ret_val, "mediators") <- mediators;
  attr(ret_val, "prob") <- prob;
  attr(ret_val, "typical") <- typical;
  attr(ret_val, "rope.range") <- rope.range;
  
  return (ret_val);
}

# Pretty print mediation results (as a data.frame):
.print_multiple_mediators_brms <- function(x)
{
  ret_val <- data.frame("outcome"=attr(x,"response"),
                        "mediators"=paste0(attr(x,"mediators"), collapse=" "),
                        "effect"=paste0(x$effect, ifelse(x$effect == "indirect", paste0(" (",x$mediator,")") ,"")),
                        "size"=sprintf("%.2f; %.0f%%HDI [%.2f, %.2f]; p=%.2g%s", 
                                       x$estimate, attr(x,"prob")*100, x$estimate_HDI_low, x$estimate_HDI_high, x$p_ROPE, gtools::stars.pval(x$p_ROPE)),
                        "proportion"=sprintf("%.1f%% [%.1f%%, %.1f%%]",
                                             100*x$proportion, 100*x$proportion_HDI_low, 100*x$proportion_HDI_high));
  ret_val$effect <- gsub("se_v_redYes", "vowel reduction", ret_val$effect);
  ret_val$effect <- gsub("se_v_delYes", "vowel deletion",  ret_val$effect);
  ret_val$effect <- gsub("se_v_devYes", "vowel devoicing", ret_val$effect);
  ret_val;
}

# Fit mediation analysis T -> M -> O
.fit_mediation_model <- function(d=d_colors, 
                                 outcome="exists_blue", outcome_name="blue", 
                                 treatment="latitude_r", treatment_name="latitude", 
                                 mediator="UVB_r", mediator_name="UVB", 
                                 family_mediator=gaussian(), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                 cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                 save_model=FALSE, show_results=FALSE)
{
  # Mediation with brms:
  f_mediator <- bf(formula(paste0(mediator," ~ ",treatment," + ",                ranefs)), family=family_mediator);
  f_outcome  <- bf(formula(paste0(outcome, " ~ ",mediator, " + ",treatment," + ",ranefs)), family=family_outcome);
  model <- brm(f_mediator + f_outcome + set_rescor(FALSE), # uncorrelated residuals
               save_all_pars=TRUE, # needed for Bayes factors
               sample_prior=TRUE,  # needed for hypotheses tests
               data=d, cores=cores, iter=iter, warmup=warmup, thin=thin, control=control);
  #model <- brms_fit_indices(model);
  model_summary <- summary(model); if(show_results) { print(model_summary); mcmc_plot(model, type="trace"); mcmc_plot(model); }
  if( show_results) { sjstats::mediation(model); }
  model_mediation <- .mediation_multiple_mediators_brms(model, response=outcome, treatment=treatment, mediators=c(mediator)); if( show_results) { print(model_mediation); }
  #.print_multiple_mediators_brms(model_mediation);

  # Return and possibly save the results:
  ret_val <- list("outcome"=outcome, "outcome_name"=outcome_name,
                  "treatment"=treatment, "treatment_name"=treatment_name,
                  "mediator"=mediator, "mediator_name"=mediator_name,
                  "formulas"=c("mediator"=paste0(mediator," ~ ",treatment," + ",ranefs), "outcome"=paste0(outcome, " ~ ",mediator, " + ",treatment," + ",ranefs)),
                  "brms_param"=list("cores"=cores, "iter"=iter, "warmup"=warmup, "thin"=thin, "control"=control),
                  "model"=if(save_model){ model } else { NULL }, "summary"=list("fixed"=model_summary$fixed), "mediation"=model_mediation);
  return (ret_val);
}

# Plot the results of a mediation analysis T -> M -> O using DiagrammeR:
.plot_mediation_model <- function(model_summary, model_mediation, 
                                  outcome=NA,   outcome_name="",   outcome_col="gray95",
                                  treatment=NA, treatment_name="", treatment_col="gray95",
                                  mediator=NA,  mediator_name="",  mediator_col="gray95",
                                  edge_ns_col="gray80", edge_neg_col="blue", edge_pos_col="red",
                                  summary_col_ns="gray50", summary_col_neg="blue", summary_col_pos="red")
{
  .edge_col <- function(edge_lo, edge_hi)
  {
    if(edge_lo < 0 && 0 < edge_hi)
      return (edge_ns_col) # ns
    else if( edge_hi < 0 )
      return (edge_neg_col) # < 0
    else
      return (edge_pos_col); # > 0
  }
  
  .eff_col <- function(eff_estim, eff_p)
  {
    if(eff_p >= 0.05)
      return (summary_col_ns) # ns
    else if( eff_estim < 0 )
      return (summary_col_neg) # < 0
    else
      return (summary_col_pos); # > 0
  }
  
  T_O <- model_summary$fixed[grep(paste0(gsub("_","",outcome,fixed=TRUE), "_", treatment), rownames(model_summary$fixed), fixed=TRUE),];
  T_O_0in <- as.numeric(T_O["l-95% CI"] < 0 && 0 < T_O["u-95% CI"])+1;
  T_O_col <- .edge_col(T_O["l-95% CI"], T_O["u-95% CI"]);
  T_M <- model_summary$fixed[grep(paste0(gsub("_","",mediator,fixed=TRUE), "_", treatment), rownames(model_summary$fixed), fixed=TRUE),];
  T_M_0in <- as.numeric(T_M["l-95% CI"] < 0 && 0 < T_M["u-95% CI"])+1;
  T_M_col <- .edge_col(T_M["l-95% CI"], T_M["u-95% CI"]);
  M_O <- model_summary$fixed[grep(paste0(gsub("_","",outcome,fixed=TRUE), "_", mediator), rownames(model_summary$fixed), fixed=TRUE),];
  M_O_0in <- as.numeric(M_O["l-95% CI"] < 0 && 0 < M_O["u-95% CI"])+1;
  M_O_col <- .edge_col(M_O["l-95% CI"], M_O["u-95% CI"]);
  TotEf <-  model_mediation[model_mediation$effect == "total",];
  Tot_col <- .eff_col(TotEf["estimate"], TotEf["p_ROPE"]);
  DirEf <-  model_mediation[model_mediation$effect == "direct",];
  Dir_col <- .eff_col(DirEf["estimate"], DirEf["p_ROPE"]);
  IndEf <-  model_mediation[model_mediation$effect == "indirect",];
  Ind_col <- .eff_col(IndEf["estimate"], IndEf["p_ROPE"]);
  
  DiagrammeR::grViz(paste0('
    digraph mediation_d2l_lat {
  
    # the graph:
    graph [overlap = true]
    rankdir="LR";
  
    # the nodes:
    node [shape = box, style = "filled"]
    T    [label = "',treatment_name,'", tooltip = "treatment", fillcolor = "',treatment_col,'"]; 
    M    [label = "',mediator_name,'",  tooltip = "mediator",  fillcolor = "',outcome_col,'"]; 
    R    [label = "',outcome_name,'",   tooltip = "outcome",   fillcolor = "',mediator_col,'"]; 
  
    # the edges:
    edge [style = "solid", color = "black"]
    T -> R [label="',sprintf("%.2f [%.2f, %.2f]", T_O["Estimate"], T_O["l-95% CI"], T_O["u-95% CI"]),'", tooltip="treatment -> outcome", 
            style="',c("solid","dashed")[T_O_0in],'", color="',T_O_col,'", fontcolor="',T_O_col,'"]
    T -> M [label="',sprintf("%.2f [%.2f, %.2f]", T_M["Estimate"], T_M["l-95% CI"], T_M["u-95% CI"]),'", tooltip="treatment -> mediator", 
            style="',c("solid","dashed")[T_M_0in],'", color="',T_M_col,'", fontcolor="',T_M_col,'"]
    M -> R [label="',sprintf("%.2f [%.2f, %.2f]", M_O["Estimate"], M_O["l-95% CI"], M_O["u-95% CI"]),'", tooltip="mediator -> outcome", 
            style="',c("solid","dashed")[M_O_0in],'", color="',M_O_col,'", fontcolor="',M_O_col,'"]
    
    # summary:
    te [shape="none", style="empty", fontcolor="',Tot_col,'", 
        tooltip="Total effect", label="',sprintf("Total effect = %.2f [%.2f, %.2f], p = %.4g", 
                                                 TotEf["estimate"], TotEf["estimate_HDI_low"], TotEf["estimate_HDI_high"], TotEf["p_ROPE"]),'"]
    de [shape="none", style="empty", fontcolor="',Dir_col,'", 
        tooltip="Direct effect", label="',sprintf("Direct effect = %.2f [%.2f, %.2f], p = %.4g", 
                                                 DirEf["estimate"], DirEf["estimate_HDI_low"], DirEf["estimate_HDI_high"], DirEf["p_ROPE"]),'"]
    ie [shape="none", style="empty", fontcolor="',Ind_col,'", 
        tooltip="Indirect effect", label="',sprintf("Indirect effect = %.2f [%.2f, %.2f], p = %.4g", 
                                                 IndEf["estimate"], IndEf["estimate_HDI_low"], IndEf["estimate_HDI_high"], IndEf["p_ROPE"]),'"]
    # trick for aligning them:
    te -> de [style = "none", color = "none"]
    de -> ie [style = "none", color = "none"]
  }
  '));
}


```

<style>
caption, .caption {
color: #555555;
font-weight: bold;
font-size: 105%;
text-align: left}

a[hreflang]:before{}
</style>



```{r load and prepare data, error=FALSE, include=FALSE}
# Load the world map and change it for Pacific-centered plotting:
mapWorld <- map_data("world", wrap=c(-20,340), ylim=c(-70,100));

# Load the colors data:
d_colors <- read.table("./input_files/data_colors.csv", header=TRUE, sep=",", quote='"', stringsAsFactors=FALSE); # comma-separated double-quoted CVS file

# Add the various info, both for the languages and for the families.
# For humidity, keep only the mean annual median and IQR as all the other measures are very highly correlated with these
# For UV, do not use the individual frequency bands, but the wide categories "A" and "B" (and overall):
d_climate    <- read.table("./input_files/data_climate.tsv",      header=TRUE, sep="\t", quote="");
d_humidity   <- read.table("./input_files/data_humidity.tsv",     header=TRUE, sep="\t", quote="");
d_dist2water <- read.table("./input_files/data_dist2water.tsv",   header=TRUE, sep="\t", quote="");
d_uv         <- read.table("./input_files/data_UV_incidence.tsv", header=TRUE, sep="\t", quote="");
d_elevation  <- read.table("./input_files/data_elevation.tsv",    header=TRUE, sep="\t", quote="");
d_genetics   <- read.table("./input_files/data_genetics.tsv",     header=TRUE, sep="\t", quote="");

# for languages ...
d_colors <- merge(d_colors, d_climate[,c("glottocode", "clim_PC1", "clim_PC2", "clim_PC3")], by="glottocode", all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_humidity[,c("glottocode", "humidity_mean_annual_median", "humidity_mean_annual_IQR")], by="glottocode", all.x=TRUE, all.y=FALSE);
names(d_colors)[ncol(d_colors)-c(1,0)] <- c("hum_median", "hum_IQR");
d_colors <- merge(d_colors, d_dist2water[,c("glottocode", "dist2ocean", "dist2lakes", "dist2rivers", "dist2water")], by="glottocode", all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_uv[,c("glottocode", "UV_A_mean", "UV_A_sd", "UV_B_mean", "UV_B_sd", "UV_mean", "UV_sd")], by="glottocode", all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_elevation[,c("glottocode", "elevation")], by="glottocode", all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_genetics, by="glottocode", all.x=TRUE, all.y=FALSE);

# ... and for families
d_colors <- merge(d_colors, d_climate[,c("glottocode", "clim_PC1", "clim_PC2", "clim_PC3")], by.x="glottocode_family", by.y="glottocode", suffixes=c("","_family"), all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_humidity[,c("glottocode", "humidity_mean_annual_median", "humidity_mean_annual_IQR")], by.x="glottocode_family", by.y="glottocode", suffixes=c("","_family"), all.x=TRUE, all.y=FALSE);
names(d_colors)[ncol(d_colors)-c(1,0)] <- c("hum_median_family", "hum_IQR_family");
d_colors <- merge(d_colors, d_dist2water[,c("glottocode", "dist2ocean", "dist2lakes", "dist2rivers", "dist2water")], by.x="glottocode_family", by.y="glottocode", suffixes=c("","_family"), all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_uv[,c("glottocode", "UV_A_mean", "UV_A_sd", "UV_B_mean", "UV_B_sd", "UV_mean", "UV_sd")], by.x="glottocode_family", by.y="glottocode", suffixes=c("","_family"), all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_elevation[,c("glottocode", "elevation")], by.x="glottocode_family", by.y="glottocode", suffixes=c("","_family"), all.x=TRUE, all.y=FALSE);

# Rescale variables (we use the suffix _r for them):
# - elevation: log(+1) because there are some elevations set to 0 meters, so increase everything by 1m to avoid -Inf:
d_colors$elevation_r          <- log2(d_colors$elevation + 1);
d_colors$elevation_family_r   <- log2(d_colors$elevation_family + 1);
# - distance to water bodies: log() as distances to water are never 0.0, so no need to guard against -Inf here:
d_colors$dist2water_r         <- log2(d_colors$dist2water);
d_colors$dist2rivers_r        <- log2(d_colors$dist2rivers);
d_colors$dist2ocean_r         <- log2(d_colors$dist2ocean);
d_colors$dist2lakes_r         <- log2(d_colors$dist2lakes);
d_colors$dist2water_family_r  <- log2(d_colors$dist2water_family);
d_colors$dist2rivers_family_r <- log2(d_colors$dist2rivers_family);
d_colors$dist2ocean_family_r  <- log2(d_colors$dist2ocean_family);
d_colors$dist2lakes_family_r  <- log2(d_colors$dist2lakes_family);
# - longitude and latitude: cosine (degrees -> radians):
d_colors$latitude_r           <- 1.0 - cos(d_colors$latitude*(pi/180)); # make sure these longitudes are still highest at the poles
d_colors$longitude_r          <- cos(d_colors$longitude*(pi/180));
d_colors$latitude_family_r    <- 1.0 - cos(d_colors$latitude_family*(pi/180));
d_colors$longitude_family_r   <- cos(d_colors$longitude_family*(pi/180));
# - UV incidence: z-score:
d_colors$UV_A_mean_r          <- scale(d_colors$UV_A_mean);
d_colors$UV_A_sd_r            <- scale(d_colors$UV_A_sd);
d_colors$UV_B_mean_r          <- scale(d_colors$UV_B_mean);
d_colors$UV_B_sd_r            <- scale(d_colors$UV_B_sd);
d_colors$UV_mean_r            <- scale(d_colors$UV_mean);
d_colors$UV_sd_r              <- scale(d_colors$UV_sd);
d_colors$UV_A_mean_family_r   <- scale(d_colors$UV_A_mean_family);
d_colors$UV_A_sd_family_r     <- scale(d_colors$UV_A_sd_family);
d_colors$UV_B_mean_family_r   <- scale(d_colors$UV_B_mean_family);
d_colors$UV_B_sd_family_r     <- scale(d_colors$UV_B_sd_family);
d_colors$UV_mean_family_r     <- scale(d_colors$UV_mean_family);
d_colors$UV_sd_family_r       <- scale(d_colors$UV_sd_family);
# - climate: z-score:
d_colors$clim_PC1_r           <- scale(d_colors$clim_PC1);
d_colors$clim_PC2_r           <- scale(d_colors$clim_PC2);
d_colors$clim_PC3_r           <- scale(d_colors$clim_PC3);
d_colors$clim_PC1_family_r    <- scale(d_colors$clim_PC1_family);
d_colors$clim_PC2_family_r    <- scale(d_colors$clim_PC2_family);
d_colors$clim_PC3_family_r    <- scale(d_colors$clim_PC3_family);
# - daltonism:
d_colors$daltonism_r          <- pmax(d_colors$daltonism/100, 0.0001); # prepare for Beta regression: transform it from % into proportions, and replace 0 by a very small proportion (0.01%)

# - genetics: z-score:
s <- grep("gen_D",names(d_colors),fixed=TRUE); for( i in s ){ d_colors <- cbind(d_colors, scale(d_colors[,i])); names(d_colors)[ncol(d_colors)] <- paste0(names(d_colors)[i],"_r"); }

# Alternative view of the globe: all longitudes > 180 are "flipped" to negative values:
d_colors$longitude_180        <- ifelse(d_colors$longitude <= 180,        d_colors$longitude,        d_colors$longitude - 360);
d_colors$longitude_family_180 <- ifelse(d_colors$longitude_family <= 180, d_colors$longitude_family, d_colors$longitude_family - 360);

# Ensure factors have the right levels and contrasts:
d_colors$exists_blue <- factor(d_colors$exists_blue, levels=c("no", "yes"));
d_colors$subsistence <- factor(d_colors$subsistence, levels=c("HG", "AGR"));

# Collapse the families with few languages into the "Other"category (for plotting purposes):
d_colors$glottocode_family_other <- ifelse(d_colors$glottocode_family %in% c("abkh1242", "ainu1252", "araw1281", "ayma1253", 
                                                                             "basq1248", "chib1249", "hadz1240", "japo1237", 
                                                                             "jiva1245", "kore1284", "otom1299", "pama1250", 
                                                                             "pano1259", "taik1256", "ticu1244", "tupi1275", 
                                                                             "utoa1244", "yano1268"),
                                           "Other", 
                                           as.character(d_colors$glottocode_family));

# Just the language families data:
d_colors_families <- unique(d_colors[, grep("_family", names(d_colors), fixed=TRUE) ]);

# Just the coordinates:
d_coords <- cbind(d_colors[,c("glottocode_family", "latitude_family", "longitude_family", "longitude_family_180")], "type"="family");
names(d_coords) <- c("glottocode", "latitude", "longitude", "longitude_180", "type");
d_coords <- rbind(cbind(d_colors[,c("glottocode", "latitude", "longitude", "longitude_180")], "type"="language"), d_coords);
d_coords <- unique(d_coords);

 
# The Delaunay triangulation:
# This is based on code from Cysouw, M., Dediu, D., & Moran, S. (2012). Comment on "Phonemic Diversity Supports a Serial Founder Effect Model of Language Expansion from Africa." Science, 335(6069), 657. https://doi.org/10.1126/science.1208841, 
# updated to run on R 3.6 by Marc Tang, and massively changed to accommodate more dense data by Dan Dediu:

# The barriers that nearest-neighbor links cannot cross:
.generate_barrier <- function(p, n_points=100) # p is a vector of (x,y) coordinates, i.e p=c(x1,y1, x2,y2, ... xn,yn)
{
  if( length(p) < 2 || length(p) %%2 != 0 ) stop("p must be of the form c(x1,y1, x2,y2, ... xn,yn).");
  t <- seq(0, 1, length.out=n_points);
  i <- 1; x <- y <- c();
  while( i <= length(p)-2 )
  {
    x <- c(x, t*p[i] + (1-t)*p[i+2]);
    y <- c(y, t*p[i+1] + (1-t)*p[i+3]);
    i <- i+2;
  }
  return (data.frame("longitude"=x, "latitude"=y));
}
geo_barriers <- rbind(.generate_barrier(c(-20,35.92, -5.65,35.92, -5.20,36.03, -1.32,36.29, 5.62,37.94, 11.32,37.72, 15.45,34.91, 32.82,32.68, 35.14, 35.53)), # Meditterean
                      .generate_barrier(c(34.30,43.47, 41.25,41.61)), # Black sea
                      .generate_barrier(c(52.90,46.27, 49.06,44.80, 51.47,40.20, 51.57,36.87)), # Caspian sea
                      .generate_barrier(c(59.00,17.59, 59.00,-50.00)), # Indian ocean (Africa)
                      .generate_barrier(c(89.34,21.04, 89.34,-20.00)), # Indian ocean (India)
                      .generate_barrier(c(89.34,-20.00, 126.42,-12.44, 131.42,-9.79, 140.00,-10.18)),  # Australia (Sunda)
                      .generate_barrier(c(157.93,-26.00, 157.93,-50.00)),               # Australia (Pacific)
                      .generate_barrier(c(124.55,29.58, 127.87,23.27, 134.33,30.00, 143.50,33.00)), # Pacific rim (1)
                      .generate_barrier(c(-145.00,30.00, -83.00,-22.00, -82.00,-50.00)), # Pacific rim (2)
                      .generate_barrier(c(-96.60,25.69, -80.59,24.17, -74.43,21.58, -68.72,20.29, -19.99,20.29)), # Caribbean
                      .generate_barrier(c(-168.99,68.03, -168.99,80.00)), # Chukchi sea
                      .generate_barrier(c(55.89,69.30, 46.80,71.78, 46.80,80.00)), # Arctic
                      .generate_barrier(c(3.50,70.00, 3.50,80.00)), # North Atlantic
                      .generate_barrier(c(10.70,-5.58, -15.00,-19.00)), # Africa (Gulf of Guinea)
                      .generate_barrier(c(-65.22,-45.25, -35.00,-45.25)), # Tierra del Fuego
                      NULL
);
geo_barriers$longitude <- ifelse( !is.na(geo_barriers$longitude) & geo_barriers$longitude < -20, 360 + geo_barriers$longitude, geo_barriers$longitude ); # Pacific-centered

tmp <- deldir(d_coords$longitude[d_coords$type == "language"], d_coords$latitude[d_coords$type == "language"], # the languages
              dpl=list(x=geo_barriers$longitude, y=geo_barriers$latitude), suppressMsge=TRUE);                 # "boundaries" as dummy points


# Remove the connections to the dummy points (the "boundaries"):
delaunay_triang <- tmp$delsgs;
delaunay_triang <- delaunay_triang[ delaunay_triang$ind1 <= tmp$n.data & delaunay_triang$ind2 <= tmp$n.data, ]; # keep only the actual language points (& deal with duplication)

# Extract the network data:
delaunay_edges <- rbind(delaunay_triang[,c(5,6)], delaunay_triang[,c(6,5)]);
delaunay_edges <- cbind(delaunay_edges, rep(1, times=dim(delaunay_edges)[1]));
attr(delaunay_edges,"n")      <- sum(d_coords$type == "language");
attr(delaunay_edges,"vnames") <- d_coords$glottocode;

# Create the list of vertices for the network:
vertices <- unname(unlist(as.list(data.frame(t(delaunay_triang[,c("ind1","ind2")]), stringsAsFactors=FALSE))));
delaunay_graph <- igraph::graph(edges=vertices, n=sum(d_coords$type == "language"), directed=FALSE);

# Extract all the neighbors:
neighbor_list <- igraph::ego(delaunay_graph);
neighbor_list <- plyr::ldply(neighbor_list, rbind);
colnames(neighbor_list) <- c("glottocode", paste0("Delaunay_N",1:(ncol(neighbor_list)-1)));
for( i in 1:ncol(neighbor_list) ) neighbor_list[,i] <- d_coords$glottocode[ neighbor_list[,i] ]; # use the glottocodes

# ... and merge them with the data:
d_colors <- merge(d_colors, neighbor_list, by="glottocode", all.x=TRUE, all.y=FALSE); # NA is used if there are no neighbours >= N

# Use short names for UV-B variables:
d_colors$UVB        <- d_colors$UV_B_mean;        d_colors$UVB_r        <- d_colors$UV_B_mean_r; 
d_colors$UVB_family <- d_colors$UV_B_mean_family; d_colors$UVB_family_r <- d_colors$UV_B_mean_family_r; 


# Point Pattern Analysis:
# Store x and y coords in two vectors:
longitudes <- d_colors$longitude; latitudes <- d_colors$latitude;

# The spatial ranges:
xrange <- range(longitudes, na.rm=TRUE); yrange <- range(latitudes, na.rm=TRUE);

# Create ppp's:
ppp_languages           <- ppp(longitudes, latitudes, xrange, yrange); # without marks
ppp_languages_daltonism <- ppp(longitudes, latitudes, xrange, yrange, marks=d_colors$daltonism); # with daltonism as marks
ppp_languages_blue      <- ppp(longitudes, latitudes, xrange, yrange, marks=d_colors$exists_blue); # with exists_blue as marks
ppp_languages_blue_num  <- ppp(longitudes, latitudes, xrange, yrange, marks=as.numeric(d_colors$exists_blue)); # with exists_blue (as numbers) as marks

# Convert from the ppp format to the sp format:
sp_languages              <- data.frame(longitude=d_colors$longitude_180, latitude=latitudes);
coordinates(sp_languages) <- ~ longitude + latitude;
proj4string(sp_languages) <- CRS("+proj=longlat +datum=WGS84");
sp_languages$daltonism    <- d_colors$daltonism;
sp_languages$exists_blue  <- d_colors$exists_blue;


# Geographic and Delanunay neightbour distances:
# The shortest pairwise distances on the WGS84 ellipsoid in Km:
d_geo_dists <- matrix(NA, nrow=nrow(d_colors), ncol=nrow(d_colors), dimnames=list(d_colors$glottocode, d_colors$glottocode));
for( i in 1:(ncol(d_geo_dists)-1) )
{
  for( j in (i+1):ncol(d_geo_dists) )
  {
    d_geo_dists[i,j] <- d_geo_dists[j,i] <- distGeo(p1=d_colors[i,c("longitude_180", "latitude")], p2=d_colors[j,c("longitude_180", "latitude")]) / 1000.0;
  }
}
diag(d_geo_dists) <- 0.0;

# The inverse distance matrix:
d_geo_dists_inv <- 1.0 / d_geo_dists; diag(d_geo_dists_inv) <- 0.0;

# Delaunay neighbours:
nb_delaunay <- tri2nb(sp_languages);
for(i in 1:nrow(d_colors))
{
  nb_delaunay[[i]] <- which(d_colors$glottocode %in% na.omit(as.character(d_colors[i, grep("Delaunay_N", colnames(d_colors), fixed=TRUE)])));
}
nb_delaunay_spweights <- nb2listw(nb_delaunay, style='B') # convert it to spatial weights
#plot(nb_delaunay, d_colors[,c("longitude","latitude")]); # check it
#plot(nb_delaunay_spweights, d_colors[,c("longitude","latitude")]); # check it
```


# Data

This dataset is built starting from the data accompanying @brown_color_2004, to which Emma Meeussen added several new populations (and checked the pre-existing coding) using a variety of primary and secondary sources (see @meeussen_colour_2015 for details), supplemented with several environmental variables by Mathilde Josserand (see @josserand_speaking_2020).

Thus, it is important to note that the databases (the one accompanying @brown_color_2004 and ours) are similar but they are not exact copies. Firstly, our dataset contains more languages (142 compared to 118). Although it may look like an extension of the prior research, our database was not a superset of the earlier database. In fact, in some cases where the same source was used, different incidence percentages were chosen as more informative or our judgement differed from the authors’ with respect to linguistic or ethnologic classification, resulting in slight differences between the databases [@meeussen_colour_2015].


## Populations and languages

In our dataset, there are `r length(unique(d_colors$glottocode))` populations, each uniquely identified by the [Glottolog](https://glottolog.org/) code (the *glottocode*) of the primary language they speak (the matching was done manually).


## Geographic location

The geographic coordinates of the populations were retrieved from the [Glottolog](https://glottolog.org/) based on their *glottocode*s.
As per @brown_color_2004, we computed the cosine of these *latitude*s and the *longitude*s to be used in the statistical models; while `cos(latitude)` captures the closeness to the equator and ranging between 0.0 (one of the poles) and 1.0 (the equator), we use here `1.0 - cos(latitude)` so that they are 0.0 at the equator and 1.0 at the poles, converting the "natural" interpretation of latitudes, and `cos(longitude)`range between -1.0 and 1.0 (corresponding to -180 and 180 degrees, respectively); 
please note that longitude can be coded (and plotted) either as ranging between 0° and 360°, or between -180° and +180°, when we need to avoid the International Date Line (IDL) producing an artefactual boundary.

```{r populations map, fig.cap=capFig("Map of the populations in our sample (with language names)."), eval=FALSE, include=FALSE}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, aes(x=longitude, y=latitude), color="black") +
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=L1), color="blue", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  NULL;
```

In the following plot, we plot exactly the same information but using a different format:

```{r populations map 2, fig.cap=capFig("Map of the populations in our sample (with language names and details for Europe and the Indian subcontinent)."), , fig.height=10, fig.width=11}
# Inset Europe:
submap_europe <- mapWorld[mapWorld$lat<70 & mapWorld$lat>36 & mapWorld$long>-20 & mapWorld$long<35,]
d_colors_europe <- d_colors[d_colors$latitude>36 & d_colors$latitude<70 & d_colors$longitude<35 & d_colors$longitude>-20,]
europe <- ggplot() + theme_bw() +
  geom_polygon(data=submap_europe, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors_europe, aes(x=longitude, y=latitude), color="black") +
  geom_label_repel(data=d_colors_europe, aes(x=longitude, y=latitude, label=L1), color="blue", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  NULL;

# Inset India:
submap_india <- mapWorld[mapWorld$lat>5 & mapWorld$lat<33& mapWorld$long>58 & mapWorld$long<99 ,]
d_colors_india <- d_colors[d_colors$latitude>5 & d_colors$latitude<33 & d_colors$longitude>58 & d_colors$longitude<99,]
india <- ggplot() + theme_bw() +
  geom_polygon(data=submap_india, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors_india, aes(x=longitude, y=latitude), color="black") +
  geom_label_repel(data=d_colors_india, aes(x=longitude, y=latitude, label=L1), color="blue", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  NULL;

# World map:
d_colors_all <- d_colors[!(d_colors$latitude>36 & d_colors$latitude<70 & d_colors$longitude<35 & d_colors$longitude>-20) & !(d_colors$latitude>5 & d_colors$latitude<33 & d_colors$longitude>58 & d_colors$longitude<99),]

all <- ggplot() + theme_bw() +
  geom_rect(data=mapWorld, mapping=aes(xmin=-10, xmax=35, ymin=36, ymax=70), size=0.3, color="black", alpha=0.1) +
  geom_rect(data=mapWorld, mapping=aes(xmin=60, xmax=97, ymin=5, ymax=33),  size=0.3, color="black", alpha=0.1) +
  geom_polygon(data=mapWorld[mapWorld$lat>-60,], aes(x=long, y=lat, group=group), fill="grey", alpha=0.7) + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses +
  geom_point(data=d_colors_all, aes(x=longitude, y=latitude), color="black") +
  geom_label_repel(data=d_colors_all, aes(x=longitude, y=latitude, label=L1), color="blue", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  NULL;

# Assemble the image:
grid.newpage(); # new page
pushViewport(viewport(layout = grid.layout(5, 2)));
define_region <- function(row, col){ viewport(layout.pos.row = row, layout.pos.col = col); } 
print(all,    vp = define_region(1:3, 1:2));
print(europe, vp = define_region(4:5, 1));
print(india,  vp = define_region(4:5, 2));
```

```{r geographic location transform, fig.cap=capFig("Relationship between raw and transformed values for latitude (left, blue) and longitude (right, red); please note that, as opposed to the map above, longitude was shifted so that longitudes > 180° are negative (i.e., centered on 0°)."), fig.height=4*1, fig.width=4*2}
grid.arrange(ggplot(data=d_colors, aes(x=latitude, y=latitude_r),) + theme_bw() +
               xlab("Latitude (raw, in °, negative = south)") + ylab("Latitude transformed = 1 - cos(latitude)") +
               geom_point(alpha=0.5, color="blue") +
               NULL,
             ggplot(data=d_colors, aes(x=longitude_180, y=longitude_r),) + theme_bw() +
               xlab("Longitude (raw, in °, negative = west)") + ylab("Longitude transformed = cos(longitude)") +
               geom_point(alpha=0.5, color="red") +
               NULL,
             ncol=2);
```


## Elevation

We obtained elevation (altitude, in meters) using [Mapzen](https://en.wikipedia.org/wiki/Mapzen) data, which is still available (July 2020) on the [Terrain Tiles on Amazon Web Services](https://aws.amazon.com/public-datasets/terrain/) and accessible through the [`elevatr` package](https://cran.r-project.org/web/packages/elevatr/vignettes/introduction_to_elevatr.html#get_raster_elevation_data); please note that for the analyses, we use `log(elevation + 1)` (we added 1m to avoid errors for the locations recorded at sea level, 0m).

```{r summary elevation}
pander(summary(d_colors$elevation));
```

```{r distribution of elevation, fig.cap=capFig("Distribution of elevation.")}
ggplot(d_colors, aes(x=elevation)) +
  theme_bw() +
  geom_histogram(color="tomato", fill="tomato", alpha=0.5)+
  ggtitle("Distribution of elevation") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map elevation, fig.cap=capFig("Map of elevation (color scale).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=elevation), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=elevation), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Elevation (m)") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```

```{r elevation transform, fig.cap=capFig("Relationship between raw and transformed values for elevation."), fig.height=4*1, fig.width=4*1}
ggplot(data=d_colors, aes(x=elevation, y=elevation_r),) + theme_bw() +
  xlab("Elevation (raw, in meters)") + ylab("Elevation transformed = log(elevation + 1)") +
  geom_point(alpha=0.5, color="blue") +
  NULL;
```


## Climate and ecology 

We reused the code from @bentz_evolution_2018 to extract historical data on global weather and climate from [WorldClim](https://www.worldclim.org/) for the period 1960-1990, encoded in 19 variables covering various measures (such as temperature, seasonality or precipitation). 
As in @bentz_evolution_2018, we conduced a A Principal Component Analysis (PCA) for our data only, and we found similar results, namely that the first two principal components (PCs) explain most of the data and have meaningful interpretations: the PC1 explains 49.7% of the variance and reflects low seasonality, wet and hot climate, whereas PC2 explains 24.7% of the variance and reflects high seasonality, hot and dry climate; PC3 explains only 8.6% and its interpretation is less straightforward; see @bentz_evolution_2018 for details (please note that the sign of the PCs is arbitrary). 
For the analyses, these variables were z-scored (thus, the relationship between raw and transformed values is linear).


### Climate PC1

```{r summary PC1}
# Climate PC1
pander(summary(d_colors$clim_PC1))
```

```{r distribution of PC1, fig.cap=capFig("Distribution of climate PC1.")}
ggplot(d_colors, aes(x=clim_PC1)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  ggtitle("Distribution of climate PC1") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map PC1, fig.cap=capFig("Map of climate PC1 (color scale).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=clim_PC1), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Climate PC1") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```

### Climate PC2

```{r summary PC2}
# Climate PC2
pander(summary(d_colors$clim_PC2))
```

```{r distribution of PC2, fig.cap=capFig("Distribution of climate PC2.")}
ggplot(d_colors, aes(x=clim_PC2)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  ggtitle("Distribution of climate PC2") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map PC2, fig.cap=capFig("Map of climate PC2 (color scale).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=clim_PC2), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Climate PC2") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```

### Climate PC3

```{r summary PC3}
# Climate PC3
pander(summary(d_colors$clim_PC3))
```

```{r distribution of PC3, fig.cap=capFig("Distribution of climate PC3.")}
ggplot(d_colors, aes(x=clim_PC3)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  ggtitle("Distribution of climate PC3") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map PC3, fig.cap=capFig("Map of climate PC3 (color scale).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=clim_PC3), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Climate PC3") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```


## Humidity 

We obtained specific humidity ("the mass of water vapour in a unit mass of moist air, usually expressed as grams of vapour per kilogram of air"; [Encyclopaedia Britannica](https://www.britannica.com/science/specific-humidity)) data from the [NOAA](http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP-NCAR/.CDAS-1/.MONTHLY/.Diagnostic/.above_ground/.qa/datafiles.html), given as monthly extractions between 1 January 1949 and the download date, and we computed the overall mean, median, standard deviation and IQR across all measurements, as well as the mean across the yearly means, medians, standard deviations and IQRs.
However, all these measures are highly correlated by type (see plots below), such that we only retain here the mean of the yearly medians and IQRs.
We did not transform these variables for the analyses.

```{r echo=FALSE, message=FALSE, fig.cap=capFig("Pairwise correlations between measures of central tendency for humidity."), fig.width=8, fig.height=8}
pairs(d_humidity[,c("humidity_overall_mean", "humidity_overall_median", "humidity_mean_annual_mean", "humidity_mean_annual_median")], 
      lower.panel=panel.cor, diag.panel=panel.hist, upper.panel=upper.panel); 
```

```{r echo=FALSE, message=FALSE, fig.cap=capFig("Pairwise correlations between measures of dispersion for humidity."), fig.width=8, fig.height=8}
pairs(d_humidity[,c("humidity_overall_sd", "humidity_overall_IQR", "humidity_mean_annual_sd", "humidity_mean_annual_IQR")], 
      lower.panel=panel.cor, diag.panel=panel.hist, upper.panel=upper.panel); 
```

### Median (mean of yearly medians)

```{r summary median humidity}
pander(summary(d_colors$hum_median))
```

```{r distribution of median humidity, fig.cap=capFig("Distribution of median humidity.")}
ggplot(d_colors, aes(x=hum_median)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  ggtitle("Distribution of median humidity") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map median humidity, fig.cap=capFig("Map of median humidity (color scale).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=hum_median), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Median humidity") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```

### Variation (mean of yearly IQRs)

```{r summary IQR humidity}
pander(summary(d_colors$hum_IQR))
```

```{r distribution of IQR humidity, fig.cap=capFig("Distribution of variation in humidity (IQR).")}
ggplot(d_colors, aes(x=hum_IQR)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  ggtitle("Distribution of variation in humidity (IQR)") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map IQR humidity, fig.cap=capFig("Map of variation in humidity (IQR) (color scale).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=hum_IQR), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "IQR humidity") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```


## Distance to bodies of water

We reused the code from @bentz_evolution_2018 to compute the distances from each of our populations to the nearest lake, ocean, river, and water in general, using [OpenStreetMap](http://openstreetmapdata.com/) raster files[^openstreetmaps]. 
For the analyses, we use the `log` of these distances.

```{r distance to water transform, fig.cap=capFig("Relationship between raw and transformed values for distance to large bodies of water. For left to right and top to bottom, distances to: lakes, rivers, oceans and water in general."), fig.height=4*2, fig.width=4*2}
grid.arrange(ggplot(data=d_colors, aes(x=dist2lakes, y=dist2lakes_r)) + theme_bw() +
               xlab("Distance to lakes (raw, in Km)") + ylab("Distance transformed = log(distance)") +
               geom_point(alpha=0.5, color="blue") +
               NULL,
             ggplot(data=d_colors, aes(x=dist2rivers, y=dist2rivers_r)) + theme_bw() +
               xlab("Distance to rivers (raw, in Km)") + ylab("Distance transformed = log(distance)") +
               geom_point(alpha=0.5, color="blue") +
               NULL,
             ggplot(data=d_colors, aes(x=dist2ocean, y=dist2ocean_r)) + theme_bw() +
               xlab("Distance to ocean/sea (raw, in Km)") + ylab("Distance transformed = log(distance)") +
               geom_point(alpha=0.5, color="blue") +
               NULL,
             ggplot(data=d_colors, aes(x=dist2water, y=dist2water_r)) + theme_bw() +
               xlab("Distance to water (raw, in Km)") + ylab("Distance transformed = log(distance)") +
               geom_point(alpha=0.5, color="blue") +
               NULL,
             ncol=2);
```

[^openstreetmaps]: Unfortunately, the primary data from [OpenStreetMap](http://openstreetmapdata.com/), the ["Reduced waterbodies as raster masks"](http://openstreetmapdata.com/data/water-reduced-raster) seems to no longer be available for download as of July 2020, so we used here the original data downloaded in March 2018 by Dan Dediu and used in the @bentz_evolution_2018 paper.


### Distance to lakes

```{r summary dist lakes}
# Distance to lakes
pander(summary(d_colors$dist2lakes));
```

```{r distribution dist lakes, fig.cap=capFig("Distribution of distances to lakes.")}
ggplot(d_colors, aes(x=dist2lakes)) +
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  #ggtitle("Distribution of distances to lakes") +
  xlab("Dist. to lakes (km)") + 
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map dist lakes, fig.cap=capFig("Map of distances to lakes (color scale).")}
ggplot() + 
  theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=round(dist2lakes,0)), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=dist2lakes), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Dist. to lakes (km)") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```

### Distance to rivers

```{r summary dist rivers}
# Distance to lakes
pander(summary(d_colors$dist2rivers));
```

```{r distribution dist rivers, fig.cap=capFig("Distribution of distances to rivers.")}
ggplot(d_colors, aes(x=dist2rivers)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  #ggtitle("Distribution of log distance to rivers") +
  xlab("Dist. to rivers (km)") + 
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map dist rivers, fig.cap=capFig("Map of distances to rivers (color scale).")}
ggplot() + 
  theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=round(dist2rivers,0)), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=dist2rivers), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Dist. to rivers (km)") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```


### Distance to oceans

```{r summary dist oceans}
# Distance to oceans
pander(summary(d_colors$dist2ocean));
```

```{r distribution dist oceans, fig.cap=capFig("Distribution of distances to oceans.")}
ggplot(d_colors, aes(x=dist2ocean)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  #ggtitle("Distribution of log distance to oceans") +
  xlab("Dist. to oceans (km)") + 
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map dist oceans, fig.cap=capFig("Map of distances to oceans (color scale).")}
ggplot() + 
  theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=round(dist2ocean,0)), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=dist2ocean), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Dist. to oceans (km)") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```

### Distance to water

```{r summary dist water}
# Distance to water
pander(summary(d_colors$dist2water));
```

```{r distribution dist water, fig.cap=capFig("Distribution of distances to water.")}
ggplot(d_colors, aes(x=dist2water)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  #ggtitle("Distribution of log distance to v") +
  xlab("log dist water (km)") + 
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map dist water, fig.cap=capFig("Map of distances to water (color scale).")}
ggplot() + 
  theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=round(dist2water,0)), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=dist2water), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Dist. to water (km)") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```


## UV incidence

The incidence of ultra-violet light (UV) was calculated from the data available from the [NASA Total Ozone Mapping Spectrometer (TOMS)](http://toms.gsfc.nasa.gov/ery_uv/new_uv/)[^uv] for the year 1998, in order to replicate the procedure in @brown_color_2004. 
These data contain daily measures of UV radiation received by the human body at four wavelengths (305 nm, 310 nm, 320 nm, and 380 nm) in Joules per square meter (J/m^2^), taking into account the thickness of the ozone layer in the stratosphere, the amount of cloud cover, the elevation, and how high the sun is in the sky. 
Here, we computed the mean and standard deviation for the whole year for UV-A (315 nm to 400 nm), for UV-B (280 nm to 315 nm) and for the full spectrum; these were further z-scored for the statistical analyses (thus, the relationship between raw and transformed values is linear).

[^uv]: Unfortunately, the primary data from [TOMS](http://toms.gsfc.nasa.gov/ery_uv/new_uv/) seems to no longer be available for download as of July 2020, so we used here the original data downloaded in 2012 by Dan Dediu and used in the @meeussen_colour_2015 MSc thesis.

### UV-A

Summaries for mean and standard deviation:
```{r summary UV-A}
pander(summary(d_colors$UV_A_mean))
pander(summary(d_colors$UV_A_sd))
```

```{r distribution UV-A, fig.cap=capFig("UV-A (mean and standard deviation).")}
grid.arrange(ggplot(d_colors, aes(x=UV_A_mean)) + 
               theme_bw() +
               geom_density(color="tomato", fill="tomato", alpha=0.5)+
               ggtitle("Distribution of mean UV-A") +
               theme(plot.title = element_text(size = 10, face = "bold")),
             ggplot(d_colors, aes(x=UV_A_sd)) + 
               theme_bw() +
               geom_density(color="tomato", fill="tomato", alpha=0.5)+
               ggtitle("Distribution of sd UV-A") +
               theme(plot.title = element_text(size = 10, face = "bold")),
             ncol=2);
```

```{r map UV-A, fig.cap=capFig("Map of UV-A (mean and standard deviation)."), fig.height=12}
grid.arrange(ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=UV_A_mean), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "Mean UV-A") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=UV_A_sd), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "SD UV-A") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ncol=1);
```

### UV-B

Summaries for mean and standard deviation:
```{r summary UV-B}
pander(summary(d_colors$UV_B_mean))
pander(summary(d_colors$UV_B_sd))
```

```{r distribution UV-B, fig.cap=capFig("UV-B (mean and standard deviation).")}
grid.arrange(ggplot(d_colors, aes(x=UV_B_mean)) + 
               theme_bw() +
               geom_density(color="tomato", fill="tomato", alpha=0.5)+
               ggtitle("Distribution of mean UV-B") +
               theme(plot.title = element_text(size = 10, face = "bold")),
             ggplot(d_colors, aes(x=UV_B_sd)) + 
               theme_bw() +
               geom_density(color="tomato", fill="tomato", alpha=0.5)+
               ggtitle("Distribution of sd UV-B") +
               theme(plot.title = element_text(size = 10, face = "bold")),
             ncol=2);
```

```{r map UV-B, fig.cap=capFig("Map of UV-B (mean and standard deviation)."), fig.height=12}
grid.arrange(ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=UV_B_mean), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "Mean UV-B") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=UV_B_sd), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "SD UV-B") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ncol=1);
```

### UV (overall)

Summaries for mean and standard deviation:
```{r summary UV}
pander(summary(d_colors$UV_mean))
pander(summary(d_colors$UV_sd))
```

```{r distribution UV, fig.cap=capFig("UV (mean and standard deviation).")}
grid.arrange(ggplot(d_colors, aes(x=UV_mean)) + 
               theme_bw() +
               geom_density(color="tomato", fill="tomato", alpha=0.5)+
               ggtitle("Distribution of mean UV-B") +
               theme(plot.title = element_text(size = 10, face = "bold")),
             ggplot(d_colors, aes(x=UV_sd)) + 
               theme_bw() +
               geom_density(color="tomato", fill="tomato", alpha=0.5)+
               ggtitle("Distribution of sd UV-B") +
               theme(plot.title = element_text(size = 10, face = "bold")),
             ncol=2);
```

```{r map UV, fig.cap=capFig("Map of UV (mean and standard deviation)."), fig.height=12}
grid.arrange(ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=UV_mean), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "Mean UV") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=UV_sd), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "SD UV") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ncol=1);
```



## Log of population size

The log of population size was obtained from @bentz_evolution_2018.

```{r summary popsize}
# population size
pander(summary(d_colors$log_popSize))
```

```{r distribution popsize, fig.cap=capFig("Log population size.")}
ggplot(d_colors, aes(x=log_popSize)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  #ggtitle("Distribution of population size (log)") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map popsize, fig.cap=capFig("Map of log population size.")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=round(log_popSize,1)), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=log_popSize), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Log(pop. size)") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```


## Subsistence strategy

Subsistence strategy was obtained mainly from [AUTOTYP](http://www.autotyp.uzh.ch/) [@bickel_autotyp_2017] as coded in [@blasi_human_2019], supplemented with information from other databases such as [D-Place](https://d-place.org/) [@kirby_dplace_2016] and [Seshat](http://seshatdatabank.info/) [@turchin_seshat_2015]. 
This is represented by the binary variable *subsistence* with values 'HG' for populations whose subsistence mode is based on hunting, fishing, gathering and/or foraging, and 'AGR' for populations with subsistence modes centered around food production. 

```{r summary subsistence}
pander(summary(d_colors$subsistence))
```

```{r distribution subsistence, fig.cap=capFig("Subsistence strategy.")}
ggplot(data =d_colors)+
  theme_bw() +
  geom_bar(aes(x=as.factor(subsistence), fill=as.factor(subsistence)), alpha=0.75) + 
  labs(x="Hunting-gathering subsistence strategy?", fill="")+
  guides(fill=FALSE) +
  scale_fill_viridis_d()
```

```{r map subsistence, fig.cap=capFig("Map of subsistence strategy.")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data= d_colors, shape=21, aes(x = longitude, y = latitude, fill=subsistence), color="red") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "subsistence?") +
  scale_fill_viridis_d()
```


## Color vocabulary

@brown_color_2004 and @meeussen_colour_2015 have collected information about the color vocabularies according to the Basic Color Categories of @berlin_basic_1991. 
Here, we used only one variable, namely the *presence of a specific term for blue* (variable *exists_blue* with values "yes" or "no").

For more precise information on the way information about the color vocabularies have been collected, please refer to the following paragraph, extracted from @meeussen_colour_2015. If no detailed linguistic information was available on the language under investigation, an informed decision was made based on available geographical and ethnic cues in the original source combined with language maps from Ethnologue [@lewis_ethnologue_2014]. For each entry in the database, the corresponding native language (L1) and its ISO 639-3 three letter language identifier, obtained from Ethnologue, were added. If applicable, a second language (L2) was also included.
In an extension of the database, each language was recorded in a seperate entry, which included
name, ISO code, and a variable indicating whether the language had a seperate term for ‘blue’. In order to collect the colour terms per language, dictionary data for each language were obtained from written or online dictionaries, online word lists and the @brown_color_2004 database. In case no such list or dictionary was available, dictionary data from the second language or a closely related language were used. In two cases, data from a protolanguage were used. The linguistic resources can be found in Appendix A in @meeussen_colour_2015.


### Specific term for 'blue'

```{r summary word for blue}
# Visualize data
pander(summary(d_colors$exists_blue))
```

```{r distribution word for blue, fig.cap=capFig("Is there a specific term for 'blue'?")}
# DISTRIBUTION color vocab
ggplot(data =d_colors)+
  theme_bw() +
  geom_bar(aes(x=as.factor(exists_blue), fill=as.factor(exists_blue)), alpha=0.75) + 
  labs(x="Is there a specific term for 'blue'?", fill="")+
  guides(fill=FALSE) +
  scale_fill_viridis_d()
```

```{r map word for blue, fig.cap=capFig("Map of specific terms for 'blue'.")}
# plot exists_blue on map
ggplot() + theme_bw() + 
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data= d_colors, shape=21, aes(x = longitude, y = latitude, fill=exists_blue), color="red") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Term for 'blue'?") +
  scale_fill_viridis_d() + 
  NULL

```



## Abnormal red/green color perception

Data on the incidence of abnormal red/green color perception for males was collected from 85 references, using the Ishihara test [@ishihara_tests_1917], the anomaloscope [@knoll_anomaloscope_1968], the [Holmgren-Thomson wool test](https://www.psych.utoronto.ca/sites/www.psych.utoronto.ca/files/Professor%20Holmgren%E2%80%99s%20Test%20For%20Colour%20Blindness.pdf) [@thomson_instrument_1880], or the [Hardy-Rand-Rittler pseudoisochromatic plate test](https://www.good-lite.com/Details.cfm?ProdID=107) [@hardy_hrr_1954]; see @meeussen_colour_2015 and @josserand_speaking_2020 for details. 
These data represent the percent of red/green "color blind" males in the population, and, for our data, varies between 0% and 11%. 
Overall "color blindness" rates were used, unless specific information was available; we include only *deuteranopia*, *deuteranomaly*, *protanopia* and *protanomaly*, and we specifically did not include data on *tritanopia* (as this concerns abnormal color perception in the yellow/blue range). 
We excluded from the analyses samples which did not distinguish between male and female, or which had data for less than 50 individuals.
We named here this variable *daltonism*, but, while this is a very short and evocative variable name, as discussed above, we emphatically also considered milder forms of red/green abnormal color perception.
For some statistical analyses that do not tolerate 0s (beta regression), we replaced the reported 0% by a very small percentage (0.0001%).

The complete list of references used to obtain these data can be found in Appendix B in @meeussen_colour_2015.

```{r summary daltonism}
pander(summary(d_colors$daltonism))
```

```{r distribution of daltonism, fig.cap=capFig("Incidence of red/green abnormal color perception.")}
# Plot
ggplot(d_colors, aes(x=daltonism)) + 
  theme_bw() +
  geom_histogram(color="tomato", fill="tomato", alpha=0.5)+
  xlab("Incidence of red/green abnormal color perception (%)") +
  #ggtitle("Distribution of daltonism") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map word for daltonism, fig.cap=capFig("Map of incidence of red/green abnormal color perception (color scale).")}
ggplot() + theme_bw() + 
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=paste0(round(daltonism,1),"%")), 
                   color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data= d_colors, aes(x = longitude, y = latitude, fill=daltonism), shape = 21, color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "% red/green abn. perc.") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;

```


## Language family

For each language, we obtained its family affiliation from the [Glottolog](https://glottolog.org/) [@hammarstrom_glottolog_2018]; there are `r length(unique(d_colors$glottocode_family))` unique language families, most languages belonging to the Indo-European (indo1319), Atlantic-Congo (atla1278) and Afro-Asiatic (afro1255). 

```{r summary families}
# language family
pander(table(d_colors$glottocode_family))
```

```{r distribution families, fig.cap=capFig("The distribution of languages across families. Only the 15 most represented families are explicitely shown, the others being gathered in the umbrella category 'Other'.")}
# Make sure "Other" is the last one:
d_colors$glottocode_family_other <- factor(d_colors$glottocode_family_other, levels=c(sort(setdiff(unique(d_colors$glottocode_family_other), "Other")), "Other"));
ggplot(data =d_colors)+
  theme_bw() +
  geom_bar(aes(x=as.factor(glottocode_family_other), fill=as.factor(glottocode_family_other)), alpha=0.5) + 
  labs(x="Language families", fill="") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  scale_fill_viridis_d() + 
  guides(fill=FALSE)
```

```{r map family, fig.cap=capFig("Map of the main language families. Only the 15 most represented language families have individual colors.")}
# Use an other variable to plot: only 15 main language families, other families are gathered under "Other"
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=glottocode_family, fill=glottocode_family_other), 
                   color="black", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=glottocode_family_other), color="black") +
  theme(legend.position = "none") +
  scale_fill_viridis_d(begin=0.2) + 
  NULL
```

### Putative origins of (macro-)families

The putative geographic origins of the language (macro-)families (latitude and longitude) were obtained from [@wichmann_homelands_2010], supplemented with information from the [Glottolog](https://glottolog.org/) [@hammarstrom_glottolog_2018].

```{r map family origins, fig.cap=capFig("Origins of the putative origins of language (macro-)families.")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors_families, aes(x=longitude_family, y=latitude_family, label=macro_family), 
                   color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors_families, shape=21, aes(x=longitude_family, y=latitude_family, fill=macro_family), color="black", alpha=0.75) +
  theme(legend.position = "none") +
  scale_fill_viridis_d() + 
  NULL
```

We also collected data corresponding to these locations for elevation, climate and ecology, humidity, distance to bodies of water and UV incidence; please note that (a) these locations are highly speculative, and (b) the information associated does not necessarily reflect the state of the world at the time when the proto-languages were spoken.


## Macroarea

We collected the macroareas as given by the [Glottolog](https://glottolog.org/) [@hammarstrom_glottolog_2018].

```{r map macroareas, fig.cap=capFig("Map of populations in our sample highlighting their macroarea.")}
# macroarea
pander(table(d_colors$macroarea));

ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, aes(x=longitude, y=latitude, color=macroarea, shape=macroarea)) +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(color="Macroarea", shape="Macroarea") +
  scale_color_viridis_d() + 
  NULL
```



## Genetic distances between populations

Unfortunately, we could not obtain information about the *opsin* genes ([*OPN1MW*](https://www.omim.org/entry/300821) and [*OPN1LW*](https://www.omim.org/entry/300822)) for the populations in our sample. 
However, using a set of microhaplotypes and SNPs from the [ALFRED database](https://alfred.med.yale.edu/) [@rajeevan_alfred_2003], and the [Arlequin](http://cmpg.unibe.ch/software/arlequin35/) [@excoffier_arlequin_2010] software package, we computed the overall genetic distances between the populations.
We imputed the missing values in the obtained distances matrix with the ultrametric methd [@lapointe_estimating_1995;@de_soete_ultrametric_1984].
Please see [Appendix I. Distance matrices] for more details.

Finally, we applied classic multi-dimensional scaling (`cmdscale`) to this imputed distance matrix, and we decided to retain the first 10 dimensions (resulting in a goodness-of-fit of 10.5%).
These dimensions should represent an overall genetic similarity between our populations.
For the statistical analyses, these were z-scored (thus, the relationship between raw and transformed values is linear).



## Delaunay neighbors

We computed the [Delaunay triangulation](https://en.wikipedia.org/wiki/Delaunay_triangulation) of our language locations taking into account the constraints to population movement imposed by large bodies of water; this is based on the method developed in @cysouw_comment_2012, adapted by M. Tang and D. Dediu.
This results in a list of neighbors for each population in our dataset.

```{r plot Delaunay, fig.cap=capFig("Restricted Delaunay triangulation for the languages in our dataset (in a Pacific-centered view). The yellow dots are the language locations, the thin black lines the connections to their immediate neighbours, and the thick red lines represent barriers to movement.")}
# Plot this Delaunay network:
ggplot() + theme_bw() +
  geom_polygon(data=map_data('world', wrap=c(-20,340), ylim=c(-70,100)), aes(x=long, y=lat, group=group), fill="grey") + # landmasses as polygons (Pacific-centered)
  geom_curve(data=delaunay_triang, aes(x=x1, y=y1, xend=x2, yend=y2), color="black", curvature=0, alpha=0.5) +
  geom_point(data=d_coords[d_coords$type == "language",], aes(x=longitude, y=latitude), shape=21, alpha=0.5, color="blue", fill="yellow", size=2) + # languages
  geom_point(data=geo_barriers, aes(x=longitude, y=latitude), col="red", shape=20, size=1) + # "barriers"
  theme(legend.position = "none",
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.title = element_blank(),
        legend.text=element_blank()) +
  NULL; 
```



## Input files and variables

These data can be found in the following input files (collected in the `./input_files` folder); "pre-processed" means produced by the associated `R` script `00_preprocess_data.R` (automatically, if needed); when transformed, the variable names usually takes the suffix "_r"; when applied to families, the variable takes the suffix "_family":

| Data                        | Input file              | Pre-processed?       | Variable name(s) | Transformed? | Also for families? |
|-----------------------------|-------------------------|----------------------|------------------|--------------|--------------------|
| Populations and languages   | `data_colors.csv`       | No | *glottocode*, *L1* | No | No |
| Language family             | `data_colors.csv`       | No | *glottocode_family*, *macro_family* | No | Yes |
| Geographic locations        | `data_colors.csv`       | No | *latitude*, *longitude* | `cos()` | Yes |
| Elevation                   | `data_elevation.tsv`    | From [Mapzen](https://en.wikipedia.org/wiki/Mapzen) | *elevation* | `log(1+)` | Yes |
| Climate and ecology         | `data_climate.tsv`      | From [WorldClim](https://www.worldclim.org/)        | *clim_PC1*, *clim_PC2*, *clim_PC3* | z-scored | Yes |
| Humidity                    | `data_chumidity.tsv`    | From [NOAA](http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP-NCAR/.CDAS-1/.MONTHLY/.Diagnostic/.above_ground/.qa/datafiles.html) | *hum_median*, *hum_IQR* | No | Yes |
| Distance to bodies of water | `data_dist2water.tsv`   | From [OpenStreetMap](http://openstreetmapdata.com/) | *dist2ocean*, *dist2lakes*, *dist2rivers*, *dist2water* | `log()` | Yes |
| UV incidence                | `data_UV_incidence.tsv` | From [NASA TOMS](http://toms.gsfc.nasa.gov/ery_uv/new_uv/) | *UV_A_mean*, *UV_A_sd*, *UV_B_mean*, *UV_B_sd*, *UV_mean*, *UV_sd* | z-scored | Yes |
| Log of population size      | `data_colors.csv`       | No | *log_popSize* | No (already `log`'d) | No |
| Subsistence strategy        | `data_colors.csv`       | No | *subsistence* | No | No |
| Specific term for 'blue'    | `data_colors.csv`       | No | *exists_blue* | No | No |
| Abnormal red/green color perception  | `data_colors.csv` | No | *exists_blue* | % &rarr; proportion (division by 100); 0% &rarr; 0.01% | No |
| Putative origins of (macro-)families | `data_colors.csv` | No | *latitude_family*, *longitude_family* | `cos()` | Yes |
| Macroarea                   | `data_colors.csv`       | No | *macroarea* | No | Yes |
| Genetic distances           | `data_cgenetics.tsv`    | From [ALFRED](https://alfred.med.yale.edu/) | *gen_D1* .. *gen_D10* | z-scored | No |
| Delaunay neighbors          | computed                | From languages' geographic locations | *Delaunay_N1*, *Delaunay_N2*, .. | No | No |



# Notes on methods

## Bayesian mixed-effects regressions

For `brms`, to select the best model, we used Bayes factors, WAIC, LOO and KFOLD.
Please note that, form `brms`, there might be differences between Bayes factors, on the one hand, and WAIC/LOO/KFOLD, on the other, due to the default use of improper priors (see, for example, https://stats.stackexchange.com/questions/407964/bayes-factors-and-predictive-accuracy-in-model-comparison-in-rstan-brms); therefore, we will both methods for model selection.


## Mediation analysis

For the mediation analysis, we used a Bayesian approach as implemented by `brms`: in essence, given a treatment" $T$ (for example, latitude), a mediator $M$ (for example, UV incidence), as an outcome $O$ (for example, the existence of a specific word for 'blue'), the mediation model looks like:

```{r fig.cap=capFig("Visual representation of a mediation, showing the individual estimates (edges, $a$, $b$ and $c$) and the effects (total, direct and indirect). We may use line style and color to differentiate ns, negative and positive edges. Likewise, we may report the total, direct and indirect effects on the diagram itself using colors to distinguish significant and ns effects.")}
# Mediation:
DiagrammeR::grViz('
  digraph mediation_simple {

  # the graph:
  graph [overlap = true]
  rankdir="LR";
  
  # show types of edges:
  1 [style="none", shape="none", label="", tooltip=""]
  2 [style="none", shape="none", label="", tooltip=""]
  1 -> 2 [style="dashed", color="gray80", fontcolor="gray80", label="ns", tooltip="ns"]
  3 [style="none", shape="none", label="", tooltip=""]
  4 [style="none", shape="none", label="", tooltip=""]
  3 -> 4 [style="solid", color="blue", fontcolor="blue", label="< 0 (negative)", tooltip="negative (< 0)"]
  5 [style="none", shape="none", label="", tooltip=""]
  6 [style="none", shape="none", label="", tooltip=""]
  5 -> 6 [style="solid", color="red", fontcolor="red", label="> 0 (positive)", tooltip="negative (> 0)"]

  # the nodes:
  node [shape = box, style = "filled", fillcolor = "gray95"]
  T [label = "treatment"]; 
  M [label = "mediator"]; 
  O [label = "outcome"]; 

  # the edges:
  edge [style = "solid", color = "black"]
  T->O [label = "c"]
  T->M [label = "a"]
  M->O [label = "b"]
  
  # summary:
  te [shape="none", style="empty", fontcolor="blue",   tooltip="Total effect",    label="Total effect = ab+c < 0 [95%HDI], p < 0.05"]
  de [shape="none", style="empty", fontcolor="gray50", tooltip="Direct effect",   label="Direct effect = c [95%HDI], ns"]
  ie [shape="none", style="empty", fontcolor="red",    tooltip="Indirect effect", label="Indirect effect = ab > 0 [95%HDI], p < 0.05"]
  te -> de [style = "none", color = "none"]
  de -> ie [style = "none", color = "none"]
}
')
```

The *total effect* (i.e., the overall influence of $T$ on $O$, defined as $a \cdot b + c$) is decomposed into the *direct effect* (the arrow $T \longrightarrow O$, defined as being equal to $c$) and the *indirect effect* "flowing" through $M$ ($T \longrightarrow M \longrightarrow O$, defined as $a \cdot b$).
These are estimated by fitting the two mixed-effects regressions (with *family* and *macroarea* as random effects) to the data jointly:

$$
\begin{array}{l}
  M \sim T + (1 | family) + (1 | macroarea) \\
  O \sim T + M + (1 | family) + (1 | macroarea)
\end{array}
$$

Please note that in our plots, we may draw the edges differently depending on the "significance" and value of their estimates ($a$, $b$ and $c$). 


# Results


## Spatial distribution of the data

How is our data distributed in geographical space?
Is it random, or, if not, how does it deviate from spatial randomness?
In this context, randomness means *complete spatial randomness* (CSR), where points are independent of each other, have the same likelihood of being found at any location, and their position is modeled by a Poisson distribution [@spielman_point_2017;@mediation_spatstat].

Comparing the distribution of our data to a CSR process using the *χ*^2^ test based on quadrat counts, clearly rejects spatial randomness:

```{r chi-sq CSR tests}
pander(quadrat.test(ppp_languages, method="Chisq", nx=5, ny=15), 
       caption=capTab("Test of CSR using quadrat counts unsing *χ*^2^."));
pander(quadrat.test(ppp_languages, method="MonteCarlo", nx=5, ny=15, conditional=TRUE), 
       caption=capTab("Test of CSR using quadrat counts unsing *χ*^2^ (conditional Monte Carlo)."));
```

The same is found using the Kolmogorov-Smirnov test:

```{r kolmogorov-smironov, fig.cap=capFig("Kolmogorov-Smirnov versus the CSR using on two dimensions.")}
k <- cdf.test(ppp_languages, density(ppp_languages));
pander(k, caption=capTab("Spatial Kolmogorov-Smirnov test of CSR in two dimensions."));
#plot(k, main="");
```

Given that our locations are not randomly distributed, do they tend to be regularly spaced or clustered?
The *G* function (using nearest neighbor distances) shows that the distances between the nearest neighbors are shorter than expected for a Poisson process, suggesting a clustering.
The *F* function (using empty space distances) shows that the observed values are larger than the expected one, indicating that empty space distances in our empirical point pattern are shorter than for a Poisson process, suggesting again clustering. 
Finally, the *K* function (using pairwise distance) also suggests clustering.

```{r G function, fig.cap=capFig("The *G*, *F* and *K* functions (one per panel, from left to right). *r* is the distance between points. Expected distribution: blue; all others are the observed distributions with various edge correction methods (see help for `Gest` in package `spatstat`)."), fig.width=9, fig.height=4}
par(mfrow=c(1,3));
plot(Gest(ppp_languages), main="G function");
plot(Fest(ppp_languages), main="F function");
plot(Kest(ppp_languages), main="K function");
par(mfrow=c(1,1));
```

Thus, it is clear (and not surprising) that our datapoints are *significantly clustered*, reflecting, on the one hand, the actual patterning of linguistic diversity constrained by geography, ecology and climate, and, on the other, vagaries of data availability for the color vocabulary and the incidence of abnormal color perception.


## Spatial autocorrelation

Spatial autocorrelation describes the degree to which locations are similar to each other at different distances, with respect to a given variable. 
We use here Moran’s *I* [@moran_notes_1950], which is a measure of global spatial autocorrelation, with either the inverse of the shortest geographical distance ("as the crow flies") on the WGS84 ellipsoid, or the nearest neighbor distance on the Delaunay triangulation, as the weight matrix.

```{r moran I}
# Inverse distance:
moran_I_inv_daltonism <- Moran.I(d_colors$daltonism, d_geo_dists_inv, scaled=TRUE);
moran_I_inv_blue      <- Moran.I(as.numeric(d_colors$exists_blue == "yes"), d_geo_dists_inv, scaled=TRUE); # convert to numeric (0 = "no")
moran_I_inv_UV_A_mean <- Moran.I(d_colors$UV_A_mean, d_geo_dists_inv, scaled=TRUE);
moran_I_inv_UV_B_mean <- Moran.I(d_colors$UV_B_mean, d_geo_dists_inv, scaled=TRUE);
# Dealunay neighbours:
moran_I_del_daltonism <- moran.test(d_colors$daltonism, nb_delaunay_spweights);
moran_I_del_blue      <- moran.test(as.numeric(d_colors$exists_blue == "yes"), nb_delaunay_spweights); # convert to numeric (0 = "no")
moran_I_del_UV_A_mean <- moran.test(d_colors$UV_A_mean, nb_delaunay_spweights);
moran_I_del_UV_B_mean <- moran.test(d_colors$UV_B_mean, nb_delaunay_spweights);
```

Using the inverse of the geographic distance, Moran's *I* finds *significant positive autocorrelations* for the incidence of red/green abnormal color perception (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_inv_daltonism$observed, moran_I_inv_daltonism$expected, moran_I_inv_daltonism$p.value)`), the incidence of UV-A  (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_inv_UV_A_mean$observed, moran_I_inv_UV_A_mean$expected, moran_I_inv_UV_A_mean$p.value)`) and UV-B (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_inv_UV_B_mean$observed, moran_I_inv_UV_B_mean$expected, moran_I_inv_UV_B_mean$p.value)`), the presence of a specific word for blue (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_inv_blue$observed, moran_I_inv_blue$expected, moran_I_inv_blue$p.value)`).
Likewise, using the Delaunay neighbors, Moran's *I* finds *significant positive autocorrelations* for the incidence of red/green abnormal color perception (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_del_daltonism$estimate["Moran I statistic"], moran_I_del_daltonism$estimate["Expectation"], moran_I_del_daltonism$p.value)`), the incidence of UV-A  (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_del_UV_A_mean$estimate["Moran I statistic"], moran_I_del_UV_A_mean$estimate["Expectation"], moran_I_del_UV_A_mean$p.value)`) and UV-B (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_del_UV_B_mean$estimate["Moran I statistic"], moran_I_del_UV_B_mean$estimate["Expectation"], moran_I_del_UV_B_mean$p.value)`), the presence of a specific word for blue (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_del_blue$estimate["Moran I statistic"], moran_I_del_blue$estimate["Expectation"], moran_I_del_blue$p.value)`).




## Hypothesis 1: UV &rarr; specific word for 'blue'

Here we test the first hypothesis, linking UV incidence and the existence of a specific word for blue.


### Variation between families and macroareas

First, we need to investigate the variation between families and macroareas, as we want to model them as random effects in our models.

```{r blue null model, include=FALSE}
# Null model (intercept) -> word for blue:
if( !all(file.exists("./cached_results/b_0__blue.RData")) )
{
  # check the random effects:
  # ICCs:
  m_0__blue <- glmer(exists_blue ~ 1 + (1 | glottocode_family) + (1 | macroarea), family=binomial(), data=d_colors);
  summary(m_0__blue);
  icc_fm <- performance::icc(m_0__blue);
  # - family:
  m_0__blue_family <- update(m_0__blue, . ~ . - (1 | glottocode_family)); 
  summary(m_0__blue_family);
  icc_m <- performance::icc(m_0__blue_family);
  anova(m_0__blue, m_0__blue_family);
  # - macroarea:
  m_0__blue_macroarea <- update(m_0__blue, . ~ . - (1 | macroarea)); 
  summary(m_0__blue_macroarea);
  icc_f <- performance::icc(m_0__blue_macroarea);
  anova(m_0__blue, m_0__blue_macroarea);
  
  # brms:
  b_0__blue <- brm(exists_blue ~ 1 + (1 | glottocode_family) + (1 | macroarea), # the null model for exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_0__blue <- brms_fit_indices(b_0__blue);
  summary(b_0__blue); mcmc_plot(b_0__blue, type="trace"); mcmc_plot(b_0__blue);
  # - family:
  b_0__blue_family <- update(b_0__blue, . ~ . - (1 | glottocode_family), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10)); 
  b_0__blue_family <- brms_fit_indices(b_0__blue_family);
  summary(b_0__blue_family); mcmc_plot(b_0__blue_family, type="trace"); mcmc_plot(b_0__blue_family);
  compare_0_family__blue <- brms_compare_models(b_0__blue, b_0__blue_family, "both raneff", "macroarea only"); # macroarea only marginally better?
  # - macroarea:
  b_0__blue_macroarea <- update(b_0__blue, . ~ . - (1 | macroarea), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10)); 
  b_0__blue_macroarea <- brms_fit_indices(b_0__blue_macroarea);
  summary(b_0__blue_macroarea); mcmc_plot(b_0__blue_macroarea, type="trace"); mcmc_plot(b_0__blue_macroarea);
  compare_0_macroarea__blue <- brms_compare_models(b_0__blue, b_0__blue_macroarea, "both raneff", "family only"); # both only marginally better?
  # -> let's keep both...
  
  # Save results:
  save(b_0__blue, 
       icc_fm, icc_f, icc_m,
       file="./cached_results/b_0__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_0__blue.RData");
}

```

The ICC of including separately family is `r round(100*icc_f$ICC_adjusted,1)`%, and of macroarea is `r round(100*icc_m$ICC_adjusted,1)`%; thus, we will include both in our models.


### Potential predictors - blue

Here we look at the potential predictors of the existence of a specific word for 'blue' individually.


#### UV &rarr; 'blue' [@lindsey_color_2002;@brown_color_2004]

Here we check if there is a relationship between measures of UV incidence and the existence of a word for 'blue'; as per @lindsey_color_2002 and @brown_color_2004, this relationship should be *negative* (i.e., higher UV-B incidence should reduce the probability of a word for 'blue').

```{r fig.cap=capFig("Probability of having a specific word for 'blue' function of UV incidence at the locations of the *languages*, showing the jittered data points, the densities (colored violins) and boxplots (black). Left: mean incidence across 1998, right: standard deviation of incidence over the days of 1998; top: UV-A, bottom: UV-B."), fig.height=2*5, fig.width=2*4}
grid.arrange(ggplot(d_colors, aes(y=UV_A_mean, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y=expression("Mean UV-A (J/m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=UV_A_sd, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y=expression("Std.dev. UV-A (J/m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,   
             ggplot(d_colors, aes(y=UV_B_mean, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y=expression("Mean UV-B (J/m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=UV_B_sd, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y=expression("Std.dev. UV-B (J/m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,   
             ncol=2);
```

```{r fig.cap=capFig("Probability of having a specific word for 'blue' function of UV incidence at the *origins of language families*, showing the jittered data points, the densities (colored violins) and boxplots (black). Left: mean incidence across 1998, right: standard deviation of incidence over the days of 1998; top: UV-A, bottom: UV-B."), fig.height=2*5, fig.width=2*4, eval=FALSE, include=FALSE}
grid.arrange(ggplot(d_colors, aes(y=UV_A_mean_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y=expression("Mean UV-A (J/m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=UV_A_sd_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y=expression("Std.dev. UV-A (J/m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,   
             ggplot(d_colors, aes(y=UV_B_mean_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y=expression("Mean UV-B (J/m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=UV_B_sd_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y=expression("Std.dev. UV-B (J/m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,   
             ncol=2);
```

```{r include=FALSE}
# UV incidence -> word for blue:
if( !all(file.exists("./cached_results/b_uv__blue.RData")) )
{
  # For languages:
  # UV_A_mean:
  b_uvam__blue <- brm(exists_blue ~ 1 + UV_A_mean_r + I(UV_A_mean_r^2) + (1 | glottocode_family) + (1 | macroarea), # UV_A_mean -> exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                              prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_uvam__blue <- brms_fit_indices(b_uvam__blue);
  summary(b_uvam__blue); mcmc_plot(b_uvam__blue, type="trace"); mcmc_plot(b_uvam__blue);
  (h_uvam__blue <- brms::hypothesis(b_uvam__blue, c("UV_A_mean_r = 0", "UV_A_mean_r < 0", "IUV_A_mean_rE2 = 0")));
  (hdi_uvam__blue <- hdi(b_uvam__blue, ci=0.95));
  # Try to remove the quadratic effect:
  b_uvam__blue_2 <- update(b_uvam__blue, . ~ . - I(UV_A_mean_r^2), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_uvam__blue_2 <- brms_fit_indices(b_uvam__blue_2);
  summary(b_uvam__blue_2); mcmc_plot(b_uvam__blue_2, type="trace"); mcmc_plot(b_uvam__blue_2);
  (h_uvam__blue_2 <- brms::hypothesis(b_uvam__blue_2, c("UV_A_mean_r = 0", "UV_A_mean_r < 0")));
  (hdi_uvam__blue_2 <- hdi(b_uvam__blue_2, ci=0.95));
  brms_compare_models(b_uvam__blue_2, b_uvam__blue, "linear only", "linear + quadratic"); # quadratic is not needed, linear is enough...
  compare_0_uvam__blue <- brms_compare_models(b_0__blue, b_uvam__blue_2, "null", "+ UV_A_mean"); # clear negative linear effect of UV_A_mean!
  # The model to keep:
  b_uvam__blue <- b_uvam__blue_2; h_uvam__blue <- h_uvam__blue_2; hdi_uvam__blue <- hdi_uvam__blue_2;
  
  # UV_A_sd:
  b_uvas__blue <- brm(exists_blue ~ 1 + UV_A_sd_r + I(UV_A_sd_r^2) + (1 | glottocode_family) + (1 | macroarea), # UV_A_sd -> exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                              prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_uvas__blue <- brms_fit_indices(b_uvas__blue);
  summary(b_uvas__blue); mcmc_plot(b_uvas__blue, type="trace"); mcmc_plot(b_uvas__blue);
  (h_uvas__blue <- brms::hypothesis(b_uvas__blue, c("UV_A_sd_r = 0", "IUV_A_sd_rE2 = 0")));
  (hdi_uvas__blue <- hdi(b_uvas__blue, ci=0.95));
  compare_0_uvas__blue <- brms_compare_models(b_0__blue, b_uvas__blue, "null", "+ UV_A_sd"); # UV_A_sd does not seem to have an effect...
  
  # UV_B_mean:
  b_uvbm__blue <- brm(exists_blue ~ 1 + UV_B_mean_r + I(UV_B_mean_r^2) + (1 | glottocode_family) + (1 | macroarea), # UV_B_mean -> exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                              prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_uvbm__blue <- brms_fit_indices(b_uvbm__blue);
  summary(b_uvbm__blue); mcmc_plot(b_uvbm__blue, type="trace"); mcmc_plot(b_uvbm__blue);
  (h_uvbm__blue <- brms::hypothesis(b_uvbm__blue, c("UV_B_mean_r = 0", "UV_B_mean_r < 0", "IUV_B_mean_rE2 = 0")));
  (hdi_uvbm__blue <- hdi(b_uvbm__blue, ci=0.95));
  # Try to remove the quadratic effect:
  b_uvbm__blue_2 <- update(b_uvbm__blue, . ~ . - I(UV_B_mean_r^2), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_uvbm__blue_2 <- brms_fit_indices(b_uvbm__blue_2);
  summary(b_uvbm__blue_2); mcmc_plot(b_uvbm__blue_2, type="trace"); mcmc_plot(b_uvbm__blue_2);
  (h_uvbm__blue_2 <- brms::hypothesis(b_uvbm__blue_2, c("UV_B_mean_r = 0", "UV_B_mean_r < 0")));
  (hdi_uvbm__blue_2 <- hdi(b_uvbm__blue_2, ci=0.95));
  brms_compare_models(b_uvbm__blue_2, b_uvbm__blue, "linear only", "linear + quadratic"); # quadratic is not needed, linear is enough...
  compare_0_uvbm__blue <- brms_compare_models(b_0__blue, b_uvbm__blue_2, "null", "+ UV_B_mean"); # clear negative effect of UV_B_mean!
  # The model to keep:
  b_uvbm__blue <- b_uvbm__blue_2; h_uvbm__blue <- h_uvbm__blue_2; hdi_uvbm__blue <- hdi_uvbm__blue_2;
  
  # UV_B_sd:
  b_uvbs__blue <- brm(exists_blue ~ 1 + UV_B_sd_r + I(UV_B_sd_r^2) + (1 | glottocode_family) + (1 | macroarea), # UV_B_sd -> exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                              prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_uvbs__blue <- brms_fit_indices(b_uvbs__blue);
  summary(b_uvbs__blue); mcmc_plot(b_uvbs__blue, type="trace"); mcmc_plot(b_uvbs__blue);
  (h_uvbs__blue <- brms::hypothesis(b_uvbs__blue, c("UV_B_sd_r = 0", "IUV_B_sd_rE2 = 0")));
  (hdi_uvbs__blue <- hdi(b_uvbs__blue, ci=0.95));
  compare_0_uvbs__blue <- brms_compare_models(b_0__blue, b_uvbs__blue, "null", "+ UV_B_sd"); # UV_B_sd does not seem to have an effect...
  
  # Compare UV_A_mean and UV_B_mean:
  compare_uvam_uvbm__blue <- brms_compare_models(b_uvam__blue, b_uvbm__blue, "UV_A_mean", "UV_B_mean"); # UV_B_mean fits the data better than UV_A_mean
  
  # As check_collinearity() does not currently work with brm, use glm:
  m_uv__blue <- glmer(exists_blue ~ 1 + UV_A_mean + UV_B_mean + (1 | glottocode_family) + (1 | macroarea),
                      family=binomial(), data=d_colors);
  summary(m_uv__blue);
  (m_uv__blue_VIF <- performance::check_collinearity(m_uv__blue)); # very high VIF -> we must keep only UV_B_mean
  
  # For families:
  # UV_A_mean:
  bf_uvam__blue <- brm(exists_blue ~ 1 + UV_A_mean_family_r + I(UV_A_mean_family_r^2) + (1 | glottocode_family) + (1 | macroarea), # UV_A_mean -> exists_blue
                       family=bernoulli(link="logit"), data=d_colors, 
                       prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                               prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                       save_all_pars=TRUE, # needed for Bayes factors
                       sample_prior=TRUE,  # needed for hypotheses tests
                       cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_uvam__blue <- brms_fit_indices(bf_uvam__blue);
  summary(bf_uvam__blue); mcmc_plot(bf_uvam__blue, type="trace"); mcmc_plot(bf_uvam__blue);
  (hf_uvam__blue <- brms::hypothesis(bf_uvam__blue, c("UV_A_mean_family_r = 0", "UV_A_mean_family_r < 0", "IUV_A_mean_family_rE2 = 0")));
  (hdif_uvam__blue <- hdi(bf_uvam__blue, ci=0.95));
  comparef_0_uvam__blue <- brms_compare_models(b_0__blue, bf_uvam__blue, "null", "+ UV_A_mean"); # null is better...
  # Iterative manual simplification:
  bf_uvam__blue_2 <- update(bf_uvam__blue, . ~ . - I(UV_A_mean_family_r^2), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_uvam__blue_2 <- brms_fit_indices(bf_uvam__blue_2);
  summary(bf_uvam__blue_2); mcmc_plot(bf_uvam__blue_2, type="trace"); mcmc_plot(bf_uvam__blue_2);
  (h_uvam__blue_2 <- brms::hypothesis(bf_uvam__blue_2, c("UV_A_mean_family_r = 0", "UV_A_mean_family_r < 0")));
  (hdi_uvam__blue_2 <- hdi(bf_uvam__blue_2, ci=0.95));
  brms_compare_models(bf_uvam__blue_2, bf_uvam__blue, "simplified", "full"); # simplified is better...
  compare_0_uvam__blue <- brms_compare_models(b_0__blue, bf_uvam__blue_2, "null", "+ UV_A_mean"); # no real evidence for UV-A...

  # UV_A_sd:
  bf_uvas__blue <- brm(exists_blue ~ 1 + UV_A_sd_family_r + I(UV_A_sd_family_r^2) + (1 | glottocode_family) + (1 | macroarea), # UV_A_sd -> exists_blue
                       family=bernoulli(link="logit"), data=d_colors, 
                       prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                               prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                       save_all_pars=TRUE, # needed for Bayes factors
                       sample_prior=TRUE,  # needed for hypotheses tests
                       cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_uvas__blue <- brms_fit_indices(bf_uvas__blue);
  summary(bf_uvas__blue); mcmc_plot(bf_uvas__blue, type="trace"); mcmc_plot(bf_uvas__blue);
  (hf_uvas__blue <- brms::hypothesis(bf_uvas__blue, c("UV_A_sd_family_r = 0", "IUV_A_sd_family_rE2 = 0")));
  (hdif_uvas__blue <- hdi(bf_uvas__blue, ci=0.95));
  comparef_0_uvas__blue <- brms_compare_models(b_0__blue, bf_uvas__blue, "null", "+ UV_A_sd"); # null is better...
  # Iterative manual simplification:
  bf_uvas__blue_2 <- update(bf_uvas__blue, . ~ . - I(UV_A_sd_family_r^2), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_uvas__blue_2 <- brms_fit_indices(bf_uvas__blue_2);
  summary(bf_uvas__blue_2); mcmc_plot(bf_uvas__blue_2, type="trace"); mcmc_plot(bf_uvas__blue_2);
  (h_uvas__blue_2 <- brms::hypothesis(bf_uvas__blue_2, c("UV_A_sd_family_r = 0")));
  (hdi_uvas__blue_2 <- hdi(bf_uvas__blue_2, ci=0.95));
  brms_compare_models(bf_uvas__blue_2, bf_uvas__blue, "simplified", "full"); # simplified is better...
  compare_0_uvas__blue <- brms_compare_models(b_0__blue, bf_uvas__blue_2, "null", "+ UV_A_sd"); # very weak evidence for UV-A...
  
  # UV_B_mean:
  bf_uvbm__blue <- brm(exists_blue ~ 1 + UV_B_mean_family_r + I(UV_B_mean_family_r^2) + (1 | glottocode_family) + (1 | macroarea), # UV_B_mean -> exists_blue
                       family=bernoulli(link="logit"), data=d_colors, 
                       prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                               prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                       save_all_pars=TRUE, # needed for Bayes factors
                       sample_prior=TRUE,  # needed for hypotheses tests
                       cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_uvbm__blue <- brms_fit_indices(bf_uvbm__blue);
  summary(bf_uvbm__blue); mcmc_plot(bf_uvbm__blue, type="trace"); mcmc_plot(bf_uvbm__blue);
  (hf_uvbm__blue <- brms::hypothesis(bf_uvbm__blue, c("UV_B_mean_family_r = 0", "UV_B_mean_family_r < 0", "IUV_B_mean_family_rE2 = 0")));
  (hdif_uvbm__blue <- hdi(bf_uvbm__blue, ci=0.95));
  comparef_0_uvbm__blue <- brms_compare_models(b_0__blue, bf_uvbm__blue, "null", "+ UV_B_mean"); # null is better...
  # Iterative manual simplification:
  bf_uvbm__blue_2 <- update(bf_uvbm__blue, . ~ . - I(UV_B_mean_family_r^2), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_uvbm__blue_2 <- brms_fit_indices(bf_uvbm__blue_2);
  summary(bf_uvbm__blue_2); mcmc_plot(bf_uvbm__blue_2, type="trace"); mcmc_plot(bf_uvbm__blue_2);
  (h_uvbm__blue_2 <- brms::hypothesis(bf_uvbm__blue_2, c("UV_B_mean_family_r = 0", "UV_B_mean_family_r < 0")));
  (hdi_uvbm__blue_2 <- hdi(bf_uvbm__blue_2, ci=0.95));
  brms_compare_models(bf_uvbm__blue_2, bf_uvbm__blue, "simplified", "full"); # simplified is better...
  compare_0_uvbm__blue <- brms_compare_models(b_0__blue, bf_uvbm__blue_2, "null", "+ UV_B_mean"); # no real evidence for UV-B...
  
  # UV_B_sd:
  bf_uvbs__blue <- brm(exists_blue ~ 1 + UV_B_sd_family_r + I(UV_B_sd_family_r^2) + (1 | glottocode_family) + (1 | macroarea), # UV_B_sd -> exists_blue
                       family=bernoulli(link="logit"), data=d_colors, 
                       prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                               prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                       save_all_pars=TRUE, # needed for Bayes factors
                       sample_prior=TRUE,  # needed for hypotheses tests
                       cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_uvbs__blue <- brms_fit_indices(bf_uvbs__blue);
  summary(bf_uvbs__blue); mcmc_plot(bf_uvbs__blue, type="trace"); mcmc_plot(bf_uvbs__blue);
  (hf_uvbs__blue <- brms::hypothesis(bf_uvbs__blue, c("UV_B_sd_family_r = 0", "IUV_B_sd_family_rE2 = 0")));
  (hdif_uvbs__blue <- hdi(bf_uvbs__blue, ci=0.95));
  comparef_0_uvbs__blue <- brms_compare_models(b_0__blue, bf_uvbs__blue, "null", "+ UV_B_sd"); # null is better...
  # Iterative manual simplification:
  bf_uvbs__blue_2 <- update(bf_uvbs__blue, . ~ . - I(UV_B_sd_family_r^2), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_uvbs__blue_2 <- brms_fit_indices(bf_uvbs__blue_2);
  summary(bf_uvbs__blue_2); mcmc_plot(bf_uvbs__blue_2, type="trace"); mcmc_plot(bf_uvbs__blue_2);
  (h_uvbs__blue_2 <- brms::hypothesis(bf_uvbs__blue_2, c("UV_B_sd_family_r = 0")));
  (hdi_uvbs__blue_2 <- hdi(bf_uvbs__blue_2, ci=0.95));
  brms_compare_models(bf_uvbs__blue_2, bf_uvbs__blue, "simplified", "full"); # simplified is not much better...
  compare_0_uvbs__blue <- brms_compare_models(b_0__blue, bf_uvbs__blue_2, "null", "+ UV_B_sd"); # very weak evidence for UV-B...


  
  # Save results:
  save(compare_0_uvam__blue, h_uvam__blue, hdi_uvam__blue, #b_uvam__blue, # no need to save the actual model
       compare_0_uvas__blue, h_uvas__blue, hdi_uvas__blue, #b_uvas__blue, # no need to save the actual model
       compare_0_uvbm__blue, h_uvbm__blue, hdi_uvbm__blue, #b_uvbm__blue, # no need to save the actual model
       compare_0_uvbs__blue, h_uvbs__blue, hdi_uvbs__blue, #b_uvbs__blue, # no need to save the actual model
       compare_uvam_uvbm__blue, m_uv__blue_VIF,
       file="./cached_results/b_uv__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_uv__blue.RData");
}
```

Fitting individual Bayesian mixed-effects logistic regressions (with family and macroarea as random effects) predicting *exists_blue* from the mean and standard deviation of the incidence of UV-A and UV-B for 1998 (both linear and quadratic effects), we found that:

- systematically, the quadratic effects do not contribute,
- the standard deviations (*UV_A_sd* and *UV_B_sd*) do not predict *exists_blue* (*&beta;~UV_A_sd~* = `r round(h_uvas__blue$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_uvas__blue$CI_low[ hdi_uvas__blue$Parameter == "b_UV_A_sd_r" ], hdi_uvas__blue$CI_high[ hdi_uvas__blue$Parameter == "b_UV_A_sd_r" ])`), *p*(*&beta;*=0) = `r sprintf("%.2g",h_uvas__blue$hypothesis$Post.Prob[1])`; *&beta;~UV_B_sd~* = `r round(h_uvbs__blue$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_uvbs__blue$CI_low[ hdi_uvbs__blue$Parameter == "b_UV_B_sd_r" ], hdi_uvbs__blue$CI_high[ hdi_uvbs__blue$Parameter == "b_UV_B_sd_r" ])`), *p*(*&beta;*=0) = `r sprintf("%.2g",h_uvbs__blue$hypothesis$Post.Prob[1])`),
- the means (*UV_A_mean* and *UV_B_mean*) both have negative effects on *exists_blue* (*&beta;~UV_A_mean~* = `r round(h_uvam__blue$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_uvam__blue$CI_low[ hdi_uvam__blue$Parameter == "b_UV_A_mean_r" ], hdi_uvam__blue$CI_high[ hdi_uvam__blue$Parameter == "b_UV_A_mean_r" ])`), *p*(*&beta;*=0) = `r sprintf("%.3g",h_uvam__blue$hypothesis$Post.Prob[1])`, *p*(*&beta;*<0) = `r sprintf("%.3g",h_uvam__blue$hypothesis$Post.Prob[2])`; *&beta;~UV_B_mean~* = `r round(h_uvbm__blue$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_uvbm__blue$CI_low[ hdi_uvbm__blue$Parameter == "b_UV_B_mean_r" ], hdi_uvbm__blue$CI_high[ hdi_uvbm__blue$Parameter == "b_UV_B_mean_r" ])`), *p*(*&beta;*=0) = `r sprintf("%.3g",h_uvbm__blue$hypothesis$Post.Prob[1])`, *p*(*&beta;*<0) = `r sprintf("%.3g",h_uvbm__blue$hypothesis$Post.Prob[2])`),
- the two means are highly multicollinear (VIF~UV_A_mean~ = `r round(m_uv__blue_VIF$VIF[ m_uv__blue_VIF$Parameter == "UV_A_mean" ],1)`, VIF~UV_B_mean~ = `r round(m_uv__blue_VIF$VIF[ m_uv__blue_VIF$Parameter == "UV_B_mean" ],1)`),

```{r, include=FALSE}
load("./cached_results/b_uv__blue.RData");
```

- but *UV_A_mean* fits the data worse than *UV_B_mean* (Bayes factor = `r round(compare_uvam_uvbm__blue$BF,2)`, LOO = `r sprintf("%.2f [SE=%.2f]", compare_uvam_uvbm__blue$LOO["UV_A_mean", "elpd_diff"], compare_uvam_uvbm__blue$LOO["UV_A_mean", "se_diff"])`, WAIC = `r sprintf("%.2f [SE=%.2f]", compare_uvam_uvbm__blue$WAIC["UV_A_mean", "elpd_diff"], compare_uvam_uvbm__blue$WAIC["UV_A_mean", "se_diff"])`, K-fold = `r sprintf("%.2f [SE=%.2f]", compare_uvam_uvbm__blue$KFOLD["UV_A_mean", "elpd_diff"], compare_uvam_uvbm__blue$KFOLD["UV_A_mean", "se_diff"])`). 


There is no (or very marginal) evidence that UV (A or B, mean or sd) incidence at the *origins of the language families* has any effect on the presence of a dedicated word for 'blue'.

Therefore, we will only consider *UV_B_mean* (renamed to *UV-B* for shortness) in the following analyses.



#### Latitude &rarr; 'blue'

The effect of UV-B on the existence of a specific word for 'blue' [@lindsey_color_2002;@brown_color_2004] should be due to the physical negative relationship between the latitude of a location on the Earth's surface and the mean annual UV-B radiation it receives; thus, it is justified to investigate the relationship between latitude and 'blue'. 
This relationship should specifically be *positive* (i.e., higher latitudes receive less UV-B incidence, which increase the chances of having a word for 'blue').

```{r fig.cap=capFig("Probability of having a specific word for 'blue' function of latitude of the *languages*, showing the jittered data points, the densities (colored violins) and boxplots (black). From left to right: the linear effect of latitude and the quadratic effect of latitude (i.e., `latitude`^2^)."), fig.height=1*5, fig.width=3*4}
grid.arrange(ggplot(d_colors, aes(y=latitude, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Latitude (°)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL, 
             ggplot(d_colors, aes(y=latitude^2, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y=expression("latitude"^"2"~" (°"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ncol=2);
```


```{r fig.cap=capFig("Probability of having a specific word for 'blue' function of latitude of the *origins of the language families*, showing the jittered data points, the densities (colored violins) and boxplots (black). From left to right: the linear effect of latitude and the quadratic effect of latitude (i.e., `latitude`^2^)."), fig.height=1*5, fig.width=3*4, eval=FALSE, include=FALSE}
grid.arrange(ggplot(d_colors, aes(y=latitude_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Latitude (°)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL, 
             ggplot(d_colors, aes(y=latitude_family^2, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y=expression("latitude"^"2"~" (°"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ncol=2);
```

```{r include=FALSE}
# Latitude -> word for blue:
if( !all(file.exists("./cached_results/b_lat__blue.RData")) )
{
  # For languages:
  b_lat__blue_all <- brm(exists_blue ~ 1 + latitude_r + I(latitude_r^2) + (1 | glottocode_family) + (1 | macroarea), # latitude -> exists_blue
                         family=bernoulli(link="logit"), data=d_colors, 
                         prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                 prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                         save_all_pars=TRUE, # needed for Bayes factors
                         sample_prior=TRUE,  # needed for hypotheses tests
                         cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_lat__blue_all <- brms_fit_indices(b_lat__blue_all);
  summary(b_lat__blue_all); mcmc_plot(b_lat__blue_all, type="trace"); mcmc_plot(b_lat__blue_all);
  (h_lat__blue <- brms::hypothesis(b_lat__blue_all, c("latitude_r = 0", "latitude_r > 0", "Ilatitude_rE2 = 0", "Ilatitude_rE2 > 0")));
  (hdi_lat__blue <- hdi(b_lat__blue_all, ci=0.95));
  compare_0_lat__blue <- brms_compare_models(b_0__blue, b_lat__blue_all, "null", "+ latitude"); # latitude matters!
  # Try to remove the quadratic effect:
  b_lat__blue_2 <- update(b_lat__blue_all, . ~ . - I(latitude_r^2), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_lat__blue_2 <- brms_fit_indices(b_lat__blue_2);
  summary(b_lat__blue_2); mcmc_plot(b_lat__blue_2, type="trace"); mcmc_plot(b_lat__blue_2);
  (h_lat__blue_2 <- brms::hypothesis(b_lat__blue_2, c("latitude_r = 0", "latitude_r > 0")));
  (hdi_lat__blue_2 <- hdi(b_lat__blue_2, ci=0.95));
  brms_compare_models(b_lat__blue_2, b_lat__blue_all, "linear", "all"); # quadratic does not seem needed
  compare_0_lat__blue <- brms_compare_models(b_0__blue, b_lat__blue_2, "null", "+ latitude"); # latitude matters!
  # -> linear-only model:
  b_lat__blue <- b_lat__blue_2; h_lat__blue <- h_lat__blue_2; hdi_lat__blue <- hdi_lat__blue_2;

  
  # For families:
  bf_lat__blue_all <- brm(exists_blue ~ 1 + latitude_family_r + I(latitude_family_r^2) + (1 | glottocode_family) + (1 | macroarea), # latitude -> exists_blue
                          family=bernoulli(link="logit"), data=d_colors, 
                          prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                  prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                          save_all_pars=TRUE, # needed for Bayes factors
                          sample_prior=TRUE,  # needed for hypotheses tests
                          cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_lat__blue_all <- brms_fit_indices(bf_lat__blue_all);
  summary(bf_lat__blue_all); mcmc_plot(bf_lat__blue_all, type="trace"); mcmc_plot(bf_lat__blue_all);
  (hf_lat__blue <- brms::hypothesis(bf_lat__blue_all, c("latitude_family_r = 0", "latitude_family_r > 0", "Ilatitude_family_rE2 = 0", "Ilatitude_family_rE2 > 0")));
  (hdif_lat__blue <- hdi(bf_lat__blue_all, ci=0.95));
  compare_0_lat__blue <- brms_compare_models(b_0__blue, bf_lat__blue_all, "null", "+ latitude"); # latitude does not seem to matter!
  # Try to remove the quadratic effect:
  bf_lat__blue_2 <- update(bf_lat__blue_all, . ~ . - I(latitude_family_r^2), 
                           cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_lat__blue_2 <- brms_fit_indices(bf_lat__blue_2);
  summary(bf_lat__blue_2); mcmc_plot(bf_lat__blue_2, type="trace"); mcmc_plot(bf_lat__blue_2);
  (hf_lat__blue_2 <- brms::hypothesis(bf_lat__blue_2, c("latitude_family_r = 0", "latitude_family_r > 0")));
  (hdif_lat__blue_2 <- hdi(bf_lat__blue_2, ci=0.95));
  brms_compare_models(bf_lat__blue_2, bf_lat__blue_all, "simplified", "full"); # simplified is marginally better
  compare_0_lat__blue <- brms_compare_models(b_0__blue, bf_lat__blue_2, "null", "+ latitude"); # no effect of latitude...


  # Save results:
  save(compare_0_lat__blue, h_lat__blue, hdi_lat__blue, #b_lat__blue, # no need to save the actual model
       file="./cached_results/b_lat__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_lat__blue.RData");
}
```

When testing simultaneously the influence of *latitude* (linear effect) and *latitude*^2^ (quadratic effect) on the presence of a dedicated word for 'blue', only the linear term has a positive effect: *&beta;~latitude~* = `r round(h_lat__blue$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_lat__blue$CI_low[ hdi_lat__blue$Parameter == "b_latitude_r" ], hdi_lat__blue$CI_high[ hdi_lat__blue$Parameter == "b_latitude_r" ])`), posterior probability *p*(*&beta;*=0) = `r sprintf("%.4g",h_lat__blue$hypothesis$Post.Prob[1])`, *p*(*&beta;*<0) = `r sprintf("%.4g",h_lat__blue$hypothesis$Post.Prob[2])`.

There is no evidence that the latitude of the *origins of language families* has any effect on the existence of a dedicated word for 'blue'.


#### Subsistence &rarr; 'blue'

It has been suggested that various aspects of culture, including the subsistence mode, might have an effect on the existence of a dedicated word for 'blue', and we should specifically expect that AGR populations have a *higher chance* of having a dedicated word for 'blue'.

```{r}
pander(tmp <- table(d_colors$subsistence, ifelse(d_colors$exists_blue=="no", "no 'blue'", "has 'blue'")));
```

```{r fig.cap=capFig("The relationship between subsistence type and the existence of a specific word for 'blue'."), fig.height=5, fig.width=4}
ggplot(as.data.frame(tmp) %>% mutate(Var2 = ifelse(Var2=="has 'blue'","Yes","No")), aes(x=Var1, y=Freq, fill=Var2)) +
  theme_bw() +
  geom_bar(stat="identity", color="black", alpha=0.25) + 
  geom_text(aes(label=Freq), color="black", position = position_stack(vjust = 0.5), size=3.0)+
  xlab("Subsistence") + ylab("Count") + 
  scale_fill_manual("Specific word for 'blue'?", values=c("No"="green", "Yes"="blue")) + 
  NULL;
```

```{r include=FALSE}
# Subsistence -> word for blue:
if( !all(file.exists("./cached_results/b_subsist__blue.RData")) )
{
  # subsistence:
  b_subsist__blue <- brm(exists_blue ~ 1 + subsistence + (1 | glottocode_family) + (1 | macroarea), # subsistence -> exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                              prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_subsist__blue <- brms_fit_indices(b_subsist__blue);
  summary(b_subsist__blue); mcmc_plot(b_subsist__blue, type="trace"); mcmc_plot(b_subsist__blue);
  (h_subsist__blue <- brms::hypothesis(b_subsist__blue, c("subsistenceAGR = 0", "subsistenceAGR > 0")));
  (hdi_subsist__blue <- hdi(b_subsist__blue, ci=0.95));
  compare_0_subsist__blue <- brms_compare_models(b_0__blue, b_subsist__blue, "null", "+ subsistence"); # no clear effect of subsistence...
  
  # Save results:
  save(compare_0_subsist__blue, h_subsist__blue, hdi_subsist__blue, #b_subsist__blue, # no need to save the actual model
       file="./cached_results/b_subsist__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_subsist__blue.RData");
}
```

Subsistence does not seem to make a contribution to predicting the presence of a specific word for blue (*exists_blue*): a Bayesian mixed-effects logistic regression of *exists_blue* on *subsistence* [AGR vs HG] (with family and macroarea as random effects) has *&beta;~AGR-HG~* = `r round(h_subsist__blue$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_subsist__blue$CI_low[ hdi_subsist__blue$Parameter == "b_subsistenceAGR" ], hdi_subsist__blue$CI_high[ hdi_subsist__blue$Parameter == "b_subsistenceAGR" ])`), posterior probability *p*(*&beta;*=0) = `r sprintf("%.4g",h_subsist__blue$hypothesis$Post.Prob[1])`, but this could be due to the very small proportion of HG populations in our sample (`r sprintf("%.1f%%", sum(d_colors$subsistence=="HG")/nrow(d_colors)*100)`).
Interestingly, the specific directional hypothesis of a positive effect of AGR is relatively well supported by the posterior probability *p*(*&beta;*>0) = `r sprintf("%.4g",h_subsist__blue$hypothesis$Post.Prob[2])`.



#### Elevation &rarr; 'blue'

Arguably there may be a *positive* relationship between a location's altitude above sea level and the average amount of UV-B it receives in a year.

```{r fig.cap=capFig("Probability of having a specific word for 'blue' function of elevation (altitude) of the *languages*, showing the jittered data points, the densities (colored violins) and boxplots (black). From left to right: the linear effect of elevation and the quadratic effect of elevation (i.e., `elevation`^2^)."), fig.height=1*5, fig.width=2*4}
grid.arrange(ggplot(d_colors, aes(y=elevation, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="elevation (m)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=elevation^2, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y=expression("elevation"^"2"~" (m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ncol=2);
```

```{r fig.cap=capFig("Probability of having a specific word for 'blue' function of elevation (altitude) of the *origins of language families*, showing the jittered data points, the densities (colored violins) and boxplots (black). From left to right: the linear effect of elevation and the quadratic effect of elevation (i.e., `elevation`^2^)."), fig.height=1*5, fig.width=2*4, eval=FALSE, include=FALSE}
grid.arrange(ggplot(d_colors, aes(y=elevation_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="elevation (m)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=elevation_family^2, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y=expression("elevation"^"2"~" (m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ncol=2);
```

```{r include=FALSE}
# Elevation -> word for blue:
if( FALSE )
{
  # For languages:
  b_alt__blue_all <- brm(exists_blue ~ 1 + elevation_r + I(elevation_r^2) + (1 | glottocode_family) + (1 | macroarea), # elevation -> exists_blue
                         family=bernoulli(link="logit"), data=d_colors, 
                         prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                 prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                         save_all_pars=TRUE, # needed for Bayes factors
                         sample_prior=TRUE,  # needed for hypotheses tests
                         cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_alt__blue_all <- brms_fit_indices(b_alt__blue_all);
  summary(b_alt__blue_all); mcmc_plot(b_alt__blue_all, type="trace"); mcmc_plot(b_alt__blue_all);
  (h_alt__blue <- brms::hypothesis(b_alt__blue_all, c("elevation_r = 0", "elevation_r > 0", "Ielevation_rE2 = 0", "Ielevation_rE2 > 0")));
  (hdi_alt__blue <- hdi(b_alt__blue_all, ci=0.95));
  compare_0_alt__blue <- brms_compare_models(b_0__blue, b_alt__blue_all, "null", "+ elevation"); # elevation seems to have no effect...
  # Try to remove the quadratic effect:
  b_alt__blue_2 <- update(b_alt__blue_all, . ~ . - I(elevation_r^2), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_alt__blue_2 <- brms_fit_indices(b_alt__blue_2);
  summary(b_alt__blue_2); mcmc_plot(b_alt__blue_2, type="trace"); mcmc_plot(b_alt__blue_2);
  (h_alt__blue_2 <- brms::hypothesis(b_alt__blue_2, c("elevation_r = 0", "elevation_r > 0")));
  (hdi_alt__blue_2 <- hdi(b_alt__blue_2, ci=0.95));
  brms_compare_models(b_alt__blue_2, b_alt__blue_all, "- quadratic", "all"); # quadratic does not seem needed
  compare_0_alt__blue <- brms_compare_models(b_0__blue, b_alt__blue_2, "null", "+ elevation"); # no effect of elevation!


  # For families:
  bf_alt__blue_all <- brm(exists_blue ~ 1 + elevation_family_r + I(elevation_family_r^2) + (1 | glottocode_family) + (1 | macroarea), # elevation -> exists_blue
                         family=bernoulli(link="logit"), data=d_colors, 
                         prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                 prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                         save_all_pars=TRUE, # needed for Bayes factors
                         sample_prior=TRUE,  # needed for hypotheses tests
                         cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_alt__blue_all <- brms_fit_indices(bf_alt__blue_all);
  summary(bf_alt__blue_all); mcmc_plot(bf_alt__blue_all, type="trace"); mcmc_plot(bf_alt__blue_all);
  (hf_alt__blue <- brms::hypothesis(bf_alt__blue_all, c("elevation_family_r = 0", "elevation_family_r > 0", "Ielevation_family_rE2 = 0", "Ielevation_family_rE2 > 0")));
  (hdif_alt__blue <- hdi(bf_alt__blue_all, ci=0.95));
  compare_0_alt__blue <- brms_compare_models(b_0__blue, bf_alt__blue_all, "null", "+ elevation"); # elevation seems to have no effect...
  # Iterative manual simplification:
  bf_alt__blue_2 <- update(bf_alt__blue_all, . ~ . - I(elevation_family_r^2), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_alt__blue_2 <- brms_fit_indices(bf_alt__blue_2);
  summary(bf_alt__blue_2); mcmc_plot(bf_alt__blue_2, type="trace"); mcmc_plot(bf_alt__blue_2);
  (hf_alt__blue_2 <- brms::hypothesis(bf_alt__blue_2, c("elevation_family_r = 0", "elevation_family_r > 0")));
  (hdif_alt__blue_2 <- hdi(bf_alt__blue_2, ci=0.95));
  brms_compare_models(bf_alt__blue_2, bf_alt__blue_all, "simplified", "full"); # simplified is better
  compare_0_alt__blue <- brms_compare_models(b_0__blue, bf_alt__blue_2, "null", "+ elevation"); # no real effect of elevation!
}
```

However, there is no effect of *elevation* (linear effect) nor of *elevation*^2^ (quadratic effect), of the languages or of the origins of language families, on the presence of a dedicated word for 'blue' in our data, even when testing the specific directional hypothesis.



#### Climate, ecology and humidity &rarr; 'blue'

As opposed to the above, there do not seem to be good *a priori* reasons why climate, ecology and air humidity should influence the presence of a dedicated word for 'blue'.

```{r fig.cap=capFig("Probability of having a specific word for 'blue' function of climate, ecology and humidity at the location of the *languages*, showing the jittered data points, the densities (colored violins) and boxplots (black). From left to right and top to bottom: climate PC1, PC2 and PC3, median and IQR of humidity."), fig.height=2*5, fig.width=3*4}
grid.arrange(ggplot(d_colors, aes(y=clim_PC1, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Climate PC1") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=clim_PC2, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Climate PC2") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=clim_PC3, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Climate PC3") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=hum_median, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Median humidity") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=hum_IQR, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="IQR humidity") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ncol=3);
```


```{r fig.cap=capFig("Probability of having a specific word for 'blue' function of climate, ecology and humidity at the location of the *origins of the language families*, showing the jittered data points, the densities (colored violins) and boxplots (black). From left to right and top to bottom: climate PC1, PC2 and PC3, median and IQR of humidity."), fig.height=2*5, fig.width=3*4, eval=FALSE, include=FALSE}
grid.arrange(ggplot(d_colors, aes(y=clim_PC1_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Climate PC1") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=clim_PC2_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Climate PC2") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=clim_PC3_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Climate PC3") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=hum_median_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Median humidity") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=hum_IQR_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="IQR humidity") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ncol=3);
```

```{r include=FALSE}
# Climate, ecology & humidity -> word for blue:
if( !all(file.exists("./cached_results/b_clim__blue.RData")) )
{
  # For languages:
  # VIF:
  m_clim__blue_all <- glm(exists_blue ~ 1 + clim_PC1_r + clim_PC2_r + clim_PC3_r + hum_median + hum_IQR, family=binomial(), data=d_colors);
  summary(m_clim__blue_all);
  performance::check_collinearity(m_clim__blue_all); # not much collinearity...
  
  # brms modeling:
  b_clim__blue_all <- brm(exists_blue ~ 1 + clim_PC1_r + clim_PC2_r + clim_PC3_r + hum_median + hum_IQR + 
                            (1 | glottocode_family) + (1 | macroarea), # climate, ecology, humidity -> exists_blue
                          family=bernoulli(link="logit"), data=d_colors, 
                          prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                  prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                          save_all_pars=TRUE, # needed for Bayes factors
                          sample_prior=TRUE,  # needed for hypotheses tests
                          cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_clim__blue_all <- brms_fit_indices(b_clim__blue_all);
  summary(b_clim__blue_all); mcmc_plot(b_clim__blue_all, type="trace"); mcmc_plot(b_clim__blue_all);
  (h_clim__blue <- brms::hypothesis(b_clim__blue_all, c("clim_PC1_r = 0", "clim_PC2_r = 0", "clim_PC3_r = 0", "hum_median = 0", "hum_IQR = 0")));
  (hdi_clim__blue <- hdi(b_clim__blue_all, ci=0.95));
  compare_0_clim__blue <- brms_compare_models(b_0__blue, b_clim__blue_all, "null", "+ climate"); # marginally better...
  # Iterative manual simplification:
  b_clim__blue_1 <- update(b_clim__blue_all, . ~ . - clim_PC2_r - hum_IQR - clim_PC3_r - hum_median, 
                           cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_clim__blue_1 <- brms_fit_indices(b_clim__blue_1);
  summary(b_clim__blue_1); mcmc_plot(b_clim__blue_1, type="trace"); mcmc_plot(b_clim__blue_1);
  (h_clim__blue_1 <- brms::hypothesis(b_clim__blue_1, c("clim_PC1_r = 0")));
  (hdi_clim__blue_1 <- hdi(b_clim__blue_1, ci=0.95));
  brms_compare_models(b_clim__blue_1, b_clim__blue_all, "simplified", "full"); # simplified is better than full
  compare_0_clim__blue <- brms_compare_models(b_0__blue, b_clim__blue_1, "null", "+ clim_PC1"); # positive effect of clim_PC1!
  # -> clim_PC1-only model:
  b_clim__blue <- b_clim__blue_1; h_clim__blue <- h_clim__blue_1; hdi_clim__blue <- hdi_clim__blue_1;
  
  # For language families:
  # VIF:
  mf_clim__blue_all <- glm(exists_blue ~ 1 + clim_PC1_family_r + clim_PC2_family_r + clim_PC3_family_r + hum_median_family + hum_IQR_family, family=binomial(), data=d_colors);
  summary(mf_clim__blue_all);
  performance::check_collinearity(mf_clim__blue_all); # high multicollinearity for clim_PC1_family_r and hum_median_family -> keep the first to be like for languages above:
  mf_clim__blue_all <- update(mf_clim__blue_all, . ~ . - hum_median_family);
  summary(mf_clim__blue_all);
  performance::check_collinearity(mf_clim__blue_all); # moderate multicollinearity for clim_PC3_family_r -> remove it:
  mf_clim__blue_all <- update(mf_clim__blue_all, . ~ . - clim_PC3_family_r);
  summary(mf_clim__blue_all);
  performance::check_collinearity(mf_clim__blue_all); # no multicollinearity now...
  
  # brms modeling:
  bf_clim__blue_all <- brm(exists_blue ~ 1 + clim_PC1_family_r + clim_PC2_family_r + hum_IQR_family + 
                            (1 | glottocode_family) + (1 | macroarea), # climate, ecology, humidity -> exists_blue
                          family=bernoulli(link="logit"), data=d_colors, 
                          prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                  prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                          save_all_pars=TRUE, # needed for Bayes factors
                          sample_prior=TRUE,  # needed for hypotheses tests
                          cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_clim__blue_all <- brms_fit_indices(bf_clim__blue_all);
  summary(bf_clim__blue_all); mcmc_plot(bf_clim__blue_all, type="trace"); mcmc_plot(bf_clim__blue_all);
  (hf_clim__blue <- brms::hypothesis(bf_clim__blue_all, c("clim_PC1_family_r = 0", "clim_PC2_family_r = 0", "hum_IQR_family = 0")));
  (hdif_clim__blue <- hdi(bf_clim__blue_all, ci=0.95));
  compare_0_clim__blue <- brms_compare_models(b_0__blue, bf_clim__blue_all, "null", "+ climate"); # marginally better...
  # Iterative manual simplification:
  bf_clim__blue_1 <- update(bf_clim__blue_all, . ~ . - clim_PC2_family_r - hum_IQR_family, 
                           cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_clim__blue_1 <- brms_fit_indices(bf_clim__blue_1);
  summary(bf_clim__blue_1); mcmc_plot(bf_clim__blue_1, type="trace"); mcmc_plot(bf_clim__blue_1);
  (hf_clim__blue_1 <- brms::hypothesis(bf_clim__blue_1, c("clim_PC1_family_r = 0")));
  (hdif_clim__blue_1 <- hdi(bf_clim__blue_1, ci=0.95));
  brms_compare_models(bf_clim__blue_1, bf_clim__blue_all, "simplified", "full"); # simplified is marginally better than full
  compare_0_clim__blue <- brms_compare_models(b_0__blue, bf_clim__blue_1, "null", "+ clim_PC1"); # no evidence for climate or humidity...

  
  # Save results:
  save(compare_0_clim__blue, h_clim__blue, hdi_clim__blue, #b_clim__blue, # no need to save the actual model
       file="./cached_results/b_clim__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_clim__blue.RData");
}
```

However, when testing simultaneously the influence of the three climate PCs and of the median and IQR of air humidity on the presence of a dedicated word for 'blue', only the first climate PC (*clim_PC1*) has a positive effect: *&beta;~clim_PC1~* = `r round(h_clim__blue$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_clim__blue$CI_low[ hdi_clim__blue$Parameter == "b_clim_PC1_r" ], hdi_clim__blue$CI_high[ hdi_clim__blue$Parameter == "b_clim_PC1_r" ])`), posterior probability *p*(*&beta;*=0) = `r sprintf("%.4g",h_clim__blue$hypothesis$Post.Prob[1])`.

There is no evidence that climate, ecology or humidity at the *origins of the language families* have any effect on the presence of a dedicated word for 'blue'.



#### Distance to water &rarr; 'blue'

While we might imagine that closeness to large bodies of (clean and permanent) water might promote the use of a word for blue to describe their color (thus, a *negative* effect), this is, as far as we know, not been seriously suggested in the literature.

```{r fig.cap=capFig("Probability of having a specific word for 'blue' function of distance to large bodies of water from the locations of the *languages*, showing the jittered data points, the densities (colored violins) and boxplots (black). From left to right and top to bottom, distances to: lakes, rivers, seas/oceans, and any type of body of water."), fig.height=2*5, fig.width=2*4}
grid.arrange(ggplot(d_colors, aes(y=dist2lakes, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Distance to nearest lake (km)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=dist2rivers, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Distance to nearest river (km)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=dist2ocean, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Distance to nearest sea/ocean (km)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=dist2water, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Distance to nearest large body of water (km)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ncol=2);
```


```{r fig.cap=capFig("Probability of having a specific word for 'blue' function of distance to large bodies of water from the locations of the *origins of language families*, showing the jittered data points, the densities (colored violins) and boxplots (black). From left to right and top to bottom, distances to: lakes, rivers, seas/oceans, and any type of body of water."), fig.height=2*5, fig.width=2*4, eval=FALSE, include=FALSE}
grid.arrange(ggplot(d_colors, aes(y=dist2lakes_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Distance to nearest lake (km)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=dist2rivers_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Distance to nearest river (km)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=dist2ocean_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Distance to nearest sea/ocean (km)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ggplot(d_colors, aes(y=dist2water_family, x=exists_blue, fill=exists_blue)) + 
               geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
               theme_bw() +
               labs(x='Word for blue?', y="Distance to nearest large body of water (km)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
               NULL,
             ncol=2);
```

```{r include=FALSE}
# Distance to large bodies of water -> word for blue:
if( !all(file.exists("./cached_results/b_water__blue.RData")) )
{
  # For languages:
  # VIF:
  m_water__blue_all <- glm(exists_blue ~ 1 + dist2lakes_r + dist2rivers_r + dist2ocean_r + dist2water_r, family=binomial(), data=d_colors);
  summary(m_water__blue_all);
  performance::check_collinearity(m_water__blue_all); 
  # high VIF for dist2lakes and dist2water: the first has a lower VIF and is significant, while the second is a composite -> remove dist2water
  
  # brms modeling:
  b_water__blue_all <- brm(exists_blue ~ 1 + dist2lakes_r + dist2rivers_r + dist2ocean_r + 
                             I(dist2lakes_r^2) + I(dist2rivers_r^2) + I(dist2ocean_r^2) + 
                            (1 | glottocode_family) + (1 | macroarea), # distance to water -> exists_blue
                          family=bernoulli(link="logit"), data=d_colors, 
                          prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                  prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                          save_all_pars=TRUE, # needed for Bayes factors
                          sample_prior=TRUE,  # needed for hypotheses tests
                          cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_water__blue_all <- brms_fit_indices(b_water__blue_all);
  summary(b_water__blue_all); mcmc_plot(b_water__blue_all, type="trace"); mcmc_plot(b_water__blue_all);
  (h_water__blue <- brms::hypothesis(b_water__blue_all, c("dist2lakes_r = 0", "Idist2lakes_rE2 = 0", "dist2rivers_r = 0", "Idist2rivers_rE2 = 0", "dist2ocean_r = 0", "Idist2ocean_rE2 = 0")));
  (hdi_water__blue <- hdi(b_water__blue_all, ci=0.95));
  compare_0_water__blue <- brms_compare_models(b_0__blue, b_water__blue_all, "null", "+ water"); # not really better...
  # Iterative manual simplification:
  b_water__blue_1 <- update(b_water__blue_all, . ~ . - I(dist2lakes_r^2) - I(dist2rivers_r^2) - I(dist2ocean_r^2) - dist2rivers_r - dist2ocean_r, 
                           cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_water__blue_1 <- brms_fit_indices(b_water__blue_1);
  summary(b_water__blue_1); mcmc_plot(b_water__blue_1, type="trace"); mcmc_plot(b_water__blue_1);
  (h_water__blue_1 <- brms::hypothesis(b_water__blue_1, c("dist2lakes_r = 0", "dist2lakes_r < 0")));
  (hdi_water__blue_1 <- hdi(b_water__blue_1, ci=0.95));
  brms_compare_models(b_water__blue_1, b_water__blue_all, "simplified", "full"); # simplified is better than full
  compare_0_water__blue <- brms_compare_models(b_0__blue, b_water__blue_1, "null", "+ water"); # negative effect of dist2lakes!
  # -> dist2lakes-only model:
  b_water__blue <- b_water__blue_1; h_water__blue <- h_water__blue_1; hdi_water__blue <- hdi_water__blue_1;

  
  # For language families:
  # VIF:
  mf_water__blue_all <- glm(exists_blue ~ 1 + dist2lakes_family_r + dist2rivers_family_r + dist2ocean_family_r + dist2water_family_r, family=binomial(), data=d_colors);
  summary(mf_water__blue_all);
  performance::check_collinearity(mf_water__blue_all); 
  # moderate VIF for dist2rivers_family and dist2water_family: the first has a lower VIF and is significant, while the second is a composite -> remove dist2water_family
  mf_water__blue_all <- update(mf_water__blue_all, . ~ . - dist2water_family_r);
  summary(mf_water__blue_all);
  performance::check_collinearity(mf_water__blue_all); # nu multicollenarity...
  
  # brms modeling:
  bf_water__blue_all <- brm(exists_blue ~ 1 + dist2lakes_family_r + dist2rivers_family_r + dist2ocean_family_r + 
                              I(dist2lakes_family_r^2) + I(dist2rivers_family_r^2) + I(dist2ocean_family_r^2) + 
                              (1 | glottocode_family) + (1 | macroarea), # distance to water -> exists_blue
                            family=bernoulli(link="logit"), data=d_colors, 
                            prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                    prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                            save_all_pars=TRUE, # needed for Bayes factors
                            sample_prior=TRUE,  # needed for hypotheses tests
                            cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_water__blue_all <- brms_fit_indices(bf_water__blue_all);
  summary(bf_water__blue_all); mcmc_plot(bf_water__blue_all, type="trace"); mcmc_plot(bf_water__blue_all);
  (hf_water__blue <- brms::hypothesis(bf_water__blue_all, c("dist2lakes_family_r = 0", "Idist2lakes_family_rE2 = 0", "dist2rivers_family_r = 0", "Idist2rivers_family_rE2 = 0", "dist2ocean_family_r = 0", "Idist2ocean_family_rE2 = 0")));
  (hdif_water__blue <- hdi(bf_water__blue_all, ci=0.95));
  compare_0_water__blue <- brms_compare_models(b_0__blue, bf_water__blue_all, "null", "+ water"); # not really better...
  # Iterative manual simplification:
  bf_water__blue_1 <- update(bf_water__blue_all, . ~ . - I(dist2ocean_family_r^2) - I(dist2rivers_family_r^2) - 
                               dist2ocean_family_r - dist2rivers_family_r - I(dist2lakes_family_r^2), 
                           cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  bf_water__blue_1 <- brms_fit_indices(bf_water__blue_1);
  summary(bf_water__blue_1); mcmc_plot(bf_water__blue_1, type="trace"); mcmc_plot(bf_water__blue_1);
  (hf_water__blue_1 <- brms::hypothesis(bf_water__blue_1, c("dist2lakes_family_r = 0")));
  (hdif_water__blue_1 <- hdi(bf_water__blue_1, ci=0.95));
  brms_compare_models(bf_water__blue_1, bf_water__blue_all, "simplified", "full"); # simplified is better than full
  compare_0_water__blue <- brms_compare_models(b_0__blue, bf_water__blue_1, "null", "+ water"); # no evidence...

  
  # Save results:
  save(compare_0_water__blue, h_water__blue, hdi_water__blue, #b_water__blue, # no need to save the actual model
       file="./cached_results/b_water__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_water__blue.RData");
}
```

However, when testing simultaneously the influence of the four distances to large bodies of water (both linear and squared) on the presence of a dedicated word for 'blue', only the distance to lakes (*dist2lakes*) has a (linear) negative effect: *&beta;~dist2lakes~* = `r round(h_water__blue$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_water__blue$CI_low[ hdi_water__blue$Parameter == "b_dist2lakes_r" ], hdi_water__blue$CI_high[ hdi_water__blue$Parameter == "b_dist2lakes_r" ])`), posterior probability *p*(*&beta;*=0) = `r sprintf("%.4g",h_water__blue$hypothesis$Post.Prob[1])`; the directional negative effect of distance is also very probable: *p*(*&beta;*<0) = `r sprintf("%.4g",h_water__blue$hypothesis$Post.Prob[2])`.

There is no evidence that distance to large bodies of water from the *origins of the language families* have any effect on the presence of a dedicated word for 'blue'.



#### (log) population size &rarr; 'blue'

Again, there does not seem to be *a priori* suggestions for such a causal link, but we could imagine that larger populations, for various reasons, might have a higher chance of having a dedicated word for 'blue'.

```{r fig.cap=capFig("Probability of having a specific word for 'blue' function of (log) population size, showing the jittered data points, the densities (colored violins) and boxplots (black)."), fig.height=1*5, fig.width=1*4}
ggplot(d_colors, aes(y=log_popSize, x=exists_blue, fill=exists_blue)) + 
  geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
  theme_bw() +
  labs(x='Word for blue?', y="log(population size)") +
  theme(axis.text.x = element_text(size =14, face = "bold"), 
        axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
        axis.title=element_text(size=14),
        legend.title=element_text(size =16),
        legend.text=element_text(size =16),
        legend.position="bottom") +
  scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
  NULL;
```

```{r include=FALSE}
# log population size -> word for blue:
if( !all(file.exists("./cached_results/b_popsize__blue.RData")) )
{
  # brms modeling:
  b_popsize__blue <- brm(exists_blue ~ 1 + log_popSize + (1 | glottocode_family) + (1 | macroarea), # population size -> exists_blue
                             family=bernoulli(link="logit"), data=d_colors, 
                             prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                     prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                             save_all_pars=TRUE, # needed for Bayes factors
                             sample_prior=TRUE,  # needed for hypotheses tests
                             cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_popsize__blue <- brms_fit_indices(b_popsize__blue);
  summary(b_popsize__blue); mcmc_plot(b_popsize__blue, type="trace"); mcmc_plot(b_popsize__blue);
  (h_popsize__blue <- brms::hypothesis(b_popsize__blue, c("log_popSize = 0", "log_popSize > 0")));
  (hdi_popsize__blue <- hdi(b_popsize__blue, ci=0.95));
  compare_0_popsize__blue <- brms_compare_models(b_0__blue, b_popsize__blue, "null", "+ pop_size"); # clear positive effect of population size!

  # Save results:
  save(compare_0_popsize__blue, h_popsize__blue, hdi_popsize__blue, #b_popsize__blue, # no need to save the actual model
       file="./cached_results/b_popsize__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_popsize__blue.RData");
}
```

However, it turns out that (log) population size has a strong positive effect on the presence of a dedicated word for 'blue': *&beta;~popSize~* = `r round(h_popsize__blue$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_popsize__blue$CI_low[ hdi_popsize__blue$Parameter == "b_log_popSize" ], hdi_popsize__blue$CI_high[ hdi_popsize__blue$Parameter == "b_log_popSize" ])`), posterior probability *p*(*&beta;*=0) = `r sprintf("%.4g",h_popsize__blue$hypothesis$Post.Prob[1])`; the directional positive effect is also very probable: *p*(*&beta;*>0) = `r sprintf("%.4g",h_popsize__blue$hypothesis$Post.Prob[2])`.



#### Genetic similarity &rarr; 'blue'

While most cases of abnormal color perception do have a strong genetic basis, it is unclear if variation in the presence of a dedicated word for 'blue' between languages should correlate with the genetic similarity between the populations that speak those languages.

```{r fig.cap=capFig("Probability of having a specific word for 'blue' function of between-populations genetic distances, showing the jittered data points, the densities (colored violins) and boxplots (black). From left to right and top to bottom, the first MDS components of the genetic distances matrix."), fig.height=2*5, fig.width=5*3}
grid.arrange(grobs=lapply(names(d_colors)[grep("gen_D[[:digit:]][[:digit:]]?$",names(d_colors))], function(s)
  ggplot(d_colors, aes(y=get(s), x=exists_blue, fill=exists_blue)) + 
    geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
    theme_bw() +
    labs(x='Word for blue?', y=s) +
    theme(axis.text.x = element_text(size =14, face = "bold"), 
          axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
          axis.title=element_text(size=14),
          legend.title=element_text(size =16),
          legend.text=element_text(size =16),
          legend.position="bottom") +
    scale_fill_manual('Blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Blue?', values=c("yes"="blue", "no"="green")) + 
    NULL),
  ncol=5);
```

```{r include=FALSE}
# Genetic distances -> word for blue:
if( !all(file.exists("./cached_results/b_gen__blue.RData")) )
{
  # VIF:
  m_gen__blue_all <- glm(exists_blue ~ 1 + gen_D1_r + gen_D2_r + gen_D3_r + gen_D4_r + gen_D5_r + gen_D6_r + gen_D7_r + gen_D8_r + gen_D9_r + gen_D10_r, 
                         family=binomial(), data=d_colors);
  summary(m_gen__blue_all);
  performance::check_collinearity(m_gen__blue_all); # no multicollinearity (as expected)
  
  # brms modeling:
  b_gen__blue_all <- brm(exists_blue ~ 1 + gen_D1_r + gen_D2_r + gen_D3_r + gen_D4_r + gen_D5_r + gen_D6_r + gen_D7_r + gen_D8_r + gen_D9_r + gen_D10_r + 
                            (1 | glottocode_family) + (1 | macroarea), # distance to water -> exists_blue
                          family=bernoulli(link="logit"), data=d_colors, 
                          prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                  prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                          save_all_pars=TRUE, # needed for Bayes factors
                          sample_prior=TRUE,  # needed for hypotheses tests
                          cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_gen__blue_all <- brms_fit_indices(b_gen__blue_all);
  summary(b_gen__blue_all); mcmc_plot(b_gen__blue_all, type="trace"); mcmc_plot(b_gen__blue_all);
  (h_gen__blue <- brms::hypothesis(b_gen__blue_all, c("gen_D1_r = 0", "gen_D2_r = 0", "gen_D3_r = 0", "gen_D4_r = 0", "gen_D5_r = 0", "gen_D6_r = 0", "gen_D7_r = 0", "gen_D8_r = 0", "gen_D9_r = 0", "gen_D10_r = 0")));
  (hdi_gen__blue <- hdi(b_gen__blue_all, ci=0.95));
  compare_0_gen__blue <- brms_compare_models(b_0__blue, b_gen__blue_all, "null", "+ genetics"); # not really better...
  # Iterative manual simplification:
  b_gen__blue_1 <- update(b_gen__blue_all, . ~ . - gen_D8_r - gen_D5_r - gen_D3_r - gen_D9_r - gen_D2_r - gen_D10_r, 
                           cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_gen__blue_1 <- brms_fit_indices(b_gen__blue_1);
  summary(b_gen__blue_1); mcmc_plot(b_gen__blue_1, type="trace"); mcmc_plot(b_gen__blue_1);
  (h_gen__blue_1 <- brms::hypothesis(b_gen__blue_1, c("gen_D1_r = 0", "gen_D4_r = 0", "gen_D6_r = 0", "gen_D7_r = 0")));
  (hdi_gen__blue_1 <- hdi(b_gen__blue_1, ci=0.95));
  brms_compare_models(b_gen__blue_1, b_gen__blue_all, "simplified", "full"); # simplified is better than full
  compare_0_gen__blue <- brms_compare_models(b_0__blue, b_gen__blue_1, "null", "+ genetics"); # significant effects of genetics!
  # keep this simplified model:
  b_gen__blue <- b_gen__blue_1; h_gen__blue <- h_gen__blue_1; hdi_gen__blue <- hdi_gen__blue_1;

  # Save results:

    save(compare_0_gen__blue, h_gen__blue, hdi_gen__blue, #b_gen__blue, # no need to save the actual model
       file="./cached_results/b_gen__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_gen__blue.RData");
}
```

Indeed, it turns out that several MDS dimensions of the genetic distances matrix influence the presence of a dedicated word for 'blue': 

- *gen_D1*: *&beta;~gen_D1~* = `r round(h_gen__blue$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_gen__blue$CI_low[ hdi_gen__blue$Parameter == "b_gen_D1_r" ], hdi_gen__blue$CI_high[ hdi_gen__blue$Parameter == "b_gen_D1_r" ])`), posterior probability *p*(*&beta;*=0) = `r sprintf("%.4g",h_gen__blue$hypothesis$Post.Prob[1])`;
- *gen_D4*: *&beta;~gen_D4~* = `r round(h_gen__blue$hypothesis$Estimate[2],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_gen__blue$CI_low[ hdi_gen__blue$Parameter == "b_gen_D4_r" ], hdi_gen__blue$CI_high[ hdi_gen__blue$Parameter == "b_gen_D4_r" ])`), posterior probability *p*(*&beta;*=0) = `r sprintf("%.4g",h_gen__blue$hypothesis$Post.Prob[2])`;
- *gen_D6*: *&beta;~gen_D6~* = `r round(h_gen__blue$hypothesis$Estimate[3],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_gen__blue$CI_low[ hdi_gen__blue$Parameter == "b_gen_D6_r" ], hdi_gen__blue$CI_high[ hdi_gen__blue$Parameter == "b_gen_D6_r" ])`), posterior probability *p*(*&beta;*=0) = `r sprintf("%.4g",h_gen__blue$hypothesis$Post.Prob[3])`;
- *gen_D7*: *&beta;~gen_D7~* = `r round(h_gen__blue$hypothesis$Estimate[4],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_gen__blue$CI_low[ hdi_gen__blue$Parameter == "b_gen_D7_r" ], hdi_gen__blue$CI_high[ hdi_gen__blue$Parameter == "b_gen_D7_r" ])`), posterior probability *p*(*&beta;*=0) = `r sprintf("%.4g",h_gen__blue$hypothesis$Post.Prob[4])`.

Geographically, these dimensions represent the following:

```{r map MDS genetics, fig.cap=capFig("Map of the MDS dimensions of the genetic distances matrix that predict the existence of a specific word for 'blue' (color scale)."), fig.width=2*8, fig.height=2*5}
grid.arrange(ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=gen_D1), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "gen_D1") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=gen_D4), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "gen_D4") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=gen_D6), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "gen_D6") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=gen_D7), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "gen_D7") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ncol=2);
```

- *gen_D1* and *gen_D4* seem to capture (to various degrees) low versus high latitudes,
- *gen_D6* and *gen_D4* seem to capture (to various degrees) Europe vs the rest of the world.



### Mediation analyses

There are several possible mediated pathways connecting various variables and the existence of a dedicated word for 'blue'.

#### Latitude &rarr; UV-B &rarr; 'blue'

```{r include=FALSE}
file_name <- "./cached_results/bmed_lat_uvb__blue.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_lat_uvb__blue <- .fit_mediation_model(d=d_colors, 
                                             outcome="exists_blue", outcome_name="blue", 
                                             treatment="latitude_r", treatment_name="latitude", 
                                             mediator="UVB_r", mediator_name="UV-B", 
                                             family_mediator=gaussian(), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                             cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                             save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_lat_uvb__blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_lat_uvb__blue; .plot_mediation_model(m$summary, m$mediation, 
                                               outcome=m$outcome, outcome_name=m$outcome_name, 
                                               treatment=m$treatment, treatment_name=m$treatment_name, 
                                               mediator=m$mediator, mediator_name=m$mediator_name);
```


#### Latitude &rarr; climate &rarr; 'blue'

```{r include=FALSE}
file_name <- "./cached_results/bmed_lat_clim__blue.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_lat_clim__blue <- .fit_mediation_model(d=d_colors, 
                                              outcome="exists_blue", outcome_name="blue", 
                                              treatment="latitude_r", treatment_name="latitude", 
                                              mediator="clim_PC1_r", mediator_name="climate PC1", 
                                              family_mediator=gaussian(), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                              cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                              save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_lat_clim__blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_lat_clim__blue; .plot_mediation_model(m$summary, m$mediation, 
                                                outcome=m$outcome, outcome_name=m$outcome_name, 
                                                treatment=m$treatment, treatment_name=m$treatment_name, 
                                                mediator=m$mediator, mediator_name=m$mediator_name);
```

#### Latitude &rarr; subsistence &rarr; 'blue'

```{r include=FALSE}
file_name <- "./cached_results/bmed_lat_sub__blue.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_lat_sub__blue <- .fit_mediation_model(d=d_colors, 
                                              outcome="exists_blue", outcome_name="blue", 
                                              treatment="latitude_r", treatment_name="latitude", 
                                              mediator="subsistence", mediator_name="subsistence", 
                                              family_mediator=bernoulli("logit"), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                              cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                              save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_lat_sub__blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_lat_sub__blue; .plot_mediation_model(m$summary, m$mediation, 
                                                outcome=m$outcome, outcome_name=m$outcome_name, 
                                                treatment=m$treatment, treatment_name=m$treatment_name, 
                                                mediator=m$mediator, mediator_name=m$mediator_name);
```

#### Latitude &rarr; population size &rarr; 'blue'

```{r include=FALSE}
file_name <- "./cached_results/bmed_lat_psiz__blue.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_lat_psiz__blue <- .fit_mediation_model(d=d_colors, 
                                              outcome="exists_blue", outcome_name="blue", 
                                              treatment="latitude_r", treatment_name="latitude", 
                                              mediator="log_popSize", mediator_name="Population size", 
                                              family_mediator=gaussian(), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                              cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                              save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_lat_psiz__blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
load("./cached_results/bmed_lat_psiz__blue.RData");
m <- bmed_lat_psiz__blue; .plot_mediation_model(m$summary, m$mediation,
                                                outcome=m$outcome, outcome_name=m$outcome_name,
                                                treatment=m$treatment, treatment_name=m$treatment_name,
                                                mediator=m$mediator, mediator_name=m$mediator_name);
```

#### Climate &rarr; subsistence &rarr; 'blue'

```{r include=FALSE}
file_name <- "./cached_results/bmed_clim_sub__blue.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_clim_sub__blue <- .fit_mediation_model(d=d_colors, 
                                              outcome="exists_blue", outcome_name="blue", 
                                              treatment="clim_PC1_r", treatment_name="climate PC1", 
                                              mediator="subsistence", mediator_name="subsistence", 
                                              family_mediator=bernoulli("logit"), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                              cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                              save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_clim_sub__blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_clim_sub__blue; .plot_mediation_model(m$summary, m$mediation, 
                                                outcome=m$outcome, outcome_name=m$outcome_name, 
                                                treatment=m$treatment, treatment_name=m$treatment_name, 
                                                mediator=m$mediator, mediator_name=m$mediator_name);
```

#### Climate &rarr; population size &rarr; 'blue'

```{r include=FALSE}
file_name <- "./cached_results/bmed_clim_psiz__blue.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_clim_psiz__blue <- .fit_mediation_model(d=d_colors, 
                                              outcome="exists_blue", outcome_name="blue", 
                                              treatment="clim_PC1_r", treatment_name="climate PC1", 
                                              mediator="log_popSize", mediator_name="Population size", 
                                              family_mediator=gaussian(), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                              cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                              save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_clim_psiz__blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_clim_psiz__blue; .plot_mediation_model(m$summary, m$mediation, 
                                                outcome=m$outcome, outcome_name=m$outcome_name, 
                                                treatment=m$treatment, treatment_name=m$treatment_name, 
                                                mediator=m$mediator, mediator_name=m$mediator_name);
```


#### UV-B &rarr; subsistence &rarr; 'blue'

```{r include=FALSE}
file_name <- "./cached_results/bmed_uvb_sub__blue.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_uvb_sub__blue <- .fit_mediation_model(d=d_colors, 
                                              outcome="exists_blue", outcome_name="blue", 
                                              treatment="UVB_r", treatment_name="UV-B", 
                                              mediator="subsistence", mediator_name="subsistence", 
                                              family_mediator=bernoulli("logit"), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                              cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                              save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_uvb_sub__blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_uvb_sub__blue; .plot_mediation_model(m$summary, m$mediation, 
                                                outcome=m$outcome, outcome_name=m$outcome_name, 
                                                treatment=m$treatment, treatment_name=m$treatment_name, 
                                                mediator=m$mediator, mediator_name=m$mediator_name);
```

#### UV-B &rarr; population size &rarr; 'blue'

```{r include=FALSE}
file_name <- "./cached_results/bmed_uvb_psiz__blue.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_uvb_psiz__blue <- .fit_mediation_model(d=d_colors, 
                                              outcome="exists_blue", outcome_name="blue", 
                                              treatment="UVB_r", treatment_name="UV-B", 
                                              mediator="log_popSize", mediator_name="Population size", 
                                              family_mediator=gaussian(), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                              cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                              save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_uvb_psiz__blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_uvb_psiz__blue; .plot_mediation_model(m$summary, m$mediation, 
                                                outcome=m$outcome, outcome_name=m$outcome_name, 
                                                treatment=m$treatment, treatment_name=m$treatment_name, 
                                                mediator=m$mediator, mediator_name=m$mediator_name);
```


#### Subsistence &rarr; population size &rarr; 'blue'

```{r include=FALSE}
file_name <- "./cached_results/bmed_subs_psiz__blue.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_subs_psiz__blue <- .fit_mediation_model(d=d_colors, 
                                              outcome="exists_blue", outcome_name="blue", 
                                              treatment="subsistence", treatment_name="subsistence", 
                                              mediator="log_popSize", mediator_name="Population size", 
                                              family_mediator=gaussian(), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                              cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                              save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_subs_psiz__blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_subs_psiz__blue; .plot_mediation_model(m$summary, m$mediation, 
                                                outcome=m$outcome, outcome_name=m$outcome_name, 
                                                treatment=m$treatment, treatment_name=m$treatment_name, 
                                                mediator=m$mediator, mediator_name=m$mediator_name);
```

#### Climate &rarr; subsistence &rarr; population size

```{r include=FALSE}
file_name <- "./cached_results/bmed_clim_subs__psiz.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_clim_subs__psiz <- .fit_mediation_model(d=d_colors, 
                                              outcome="log_popSize", outcome_name="Population size", 
                                              treatment="clim_PC1_r", treatment_name="Climate PC1", 
                                              mediator="subsistence", mediator_name="subsistence", 
                                              family_mediator=bernoulli("logit"), family_outcome=gaussian(), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                              cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                              save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_clim_subs__psiz, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_clim_subs__psiz; .plot_mediation_model(m$summary, m$mediation, 
                                                outcome=m$outcome, outcome_name=m$outcome_name, 
                                                treatment=m$treatment, treatment_name=m$treatment_name, 
                                                mediator=m$mediator, mediator_name=m$mediator_name);
```


#### Extra: what's going on with dist2lakes?

```{r include=FALSE}
file_name <- "./cached_results/bmed_d2l__blue.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation models:
  
  # dist2lakes -> climate -> "blue":
  bmed_d2l_subs__blue <- .fit_mediation_model(d=d_colors, 
                                              outcome="exists_blue", outcome_name="blue", 
                                              treatment="dist2lakes_r", treatment_name="dist2lakes", 
                                              mediator="subsistence", mediator_name="subsistence", 
                                              family_mediator=bernoulli("logit"), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                              cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                              save_model=FALSE, show_results=TRUE);
  
  # dist2lakes -> UV-B -> "blue":
  bmed_d2l_psiz__blue <- .fit_mediation_model(d=d_colors, 
                                             outcome="exists_blue", outcome_name="blue", 
                                             treatment="dist2lakes_r", treatment_name="dist2lakes", 
                                             mediator="log_popSize", mediator_name="Population size", 
                                             family_mediator=gaussian(), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                             cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                             save_model=FALSE, show_results=TRUE);
  
    # dist2lakes -> UV-B -> "blue":
  bmed_lat_d2l__blue <- .fit_mediation_model(d=d_colors, 
                                             outcome="exists_blue", outcome_name="blue", 
                                             treatment="latitude_r", treatment_name="latitude", 
                                             mediator="dist2lakes_r", mediator_name="dist2lakes", 
                                             family_mediator=gaussian(), family_outcome=bernoulli("logit"), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                             cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                             save_model=FALSE, show_results=TRUE);


  # Save results:
  save(bmed_d2l_subs__blue, bmed_d2l_psiz__blue, bmed_lat_d2l__blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name); 
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_d2l_subs__blue; .plot_mediation_model(m$summary, m$mediation, 
                                                outcome=m$outcome, outcome_name=m$outcome_name, 
                                                treatment=m$treatment, treatment_name=m$treatment_name, 
                                                mediator=m$mediator, mediator_name=m$mediator_name);
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_d2l_psiz__blue; .plot_mediation_model(m$summary, m$mediation, 
                                                outcome=m$outcome, outcome_name=m$outcome_name, 
                                                treatment=m$treatment, treatment_name=m$treatment_name, 
                                                mediator=m$mediator, mediator_name=m$mediator_name);
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the outcome is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_lat_d2l__blue; .plot_mediation_model(m$summary, m$mediation, 
                                                outcome=m$outcome, outcome_name=m$outcome_name, 
                                                treatment=m$treatment, treatment_name=m$treatment_name, 
                                                mediator=m$mediator, mediator_name=m$mediator_name);
```




#### Interpretation

Thus, the data supports the following pathways:

```{r fig.cap=capFig("The links supported by the mediation and regression analyses. Edges: solid blue = negative effects, solid red = positive effects, and dashed gray = null effects; the dashed red edge is borderline significant. Nodes: 'blue' in blue is the outcome, 'UV-B' in green is of particular interest here, light yellow have an effect on 'blue', while light gray do not. This should *not* be interpreted as a path diagram!"), fig.width=7, fig.height=4}

DiagrammeR::grViz(paste0('
  digraph full_blue_a {

  # the graph:
  graph [overlap = true]
  rankdir="LR";

  # the nodes:
  node [shape = box, style = "filled", fillcolor = "lightyellow"];
  blue [label = "blue", tooltip = "blue", fillcolor="deepskyblue"]; 
  UVB  [label = "UV-B", tooltip = "UV-B", fillcolor="seagreen1"]; 
  lat  [label = "latitude", tooltip = "latitude"]; 
  clim [label = "climate PC1", tooltip = "climate PC1"]; 
  subs [label = "subsistence", tooltip = "subsistence"]; 
  psiz [label = "pop_size", tooltip = "pop_size"]; 
  d2l  [label = "dist2lakes", tooltip = "dist2lakes"]; 

  # the edges:
  edge [style = "solid", color = "black"];
  
  lat  -> UVB  [label="-", color="blue", fontcolor="blue"]
  lat  -> d2l  [label="-", color="blue", fontcolor="blue"]
  UVB  -> blue  [label="-", color="blue", fontcolor="blue"]
  lat  -> subs  [label="-", color="blue", fontcolor="blue"]
  clim  -> subs  [label="-", color="blue", fontcolor="blue"]
  d2l  -> blue  [label="-", color="blue", fontcolor="blue"]
  
  lat  -> clim  [label="+", color="red", fontcolor="red"]
  subs  -> psiz  [label="+", color="red", fontcolor="red"]
  psiz  -> blue  [label="+", color="red", fontcolor="red"]
  
  lat  -> blue  [label="+", color="gray80", fontcolor="gray80", style="dashed"]
  clim  -> blue  [label="+", color="gray80", fontcolor="gray80", style="dashed"]
  subs -> blue [label="+", color="gray80", fontcolor="gray80", style="dashed"]
}
'))

```

- *latitude* has no direct effect on *blue*, but its overall positive influence is mediated mostly by *UV-B* and, possibly, by *subsistence* and *dist2lakes*;
- similarly, *climate PC1* has no direct effect on *blue*, but its effect is mediated by *subsistence*;
- *subsistence* affects *blue* only through *population size*;
- *distance to lakes*, *UV-B* and *population size* have direct effects on *blue*.


### Path analysis

```{r include=FALSE}
file_name <- "./cached_results/sem_blue.RData";
if( !all(file.exists(file_name)) )
{
  # The data:
  d_colors_lavaan <- d_colors[,c("glottocode", "glottocode_family", "macroarea", 
                                 "exists_blue", "UVB_r", "latitude_r", "dist2lakes_r", "clim_PC1_r", "subsistence", "log_popSize")];
  names(d_colors_lavaan) <- c("glottocode", "glottocode_family", "macroarea", 
                                 "blue", "uvb", "lat", "d2l", "clim", "subs", "psiz");
  # Binary variables recoding (as per https://lavaan.ugent.be/tutorial/cat.html):
  d_colors_lavaan$blue <- ordered(d_colors_lavaan$blue, levels=c("no","yes")); # this is an endogenous (dependent) variable -> make it ordered
  d_colors_lavaan$subs <- ordered(d_colors_lavaan$subs, levels=c("HG","AGR")); # this is an endogenous (dependent) variable -> make it ordered

  
  # The full SEM model:
  sem_blue_full <- '
    # blue:
    blue ~ b_bu * uvb  # uvb  ---> blue 
         + b_bd * d2l  # d2l  ---> blue 
         + b_bs * subs # subs ---> blue 
         + b_bc * clim # clim ---> blue
         + b_bl * lat  # lat  ---> blue
         + b_bp * psiz # psiz  ---> blue

    # UV-B incidence:   
    uvb  ~ b_ul * lat  # lat  ---> uvb  
    uvb ~~ clim
    uvb ~~ d2l
    
    # Climate:
    clim ~ b_cl * lat  # lat  ---> clim
       
    # Distance to lakes:  
    d2l  ~ b_dl * lat  # lat  ---> d2l
    
    # Subsistence:
    subs ~ b_sl * lat  # lat  ---> subs 
         + b_sd * d2l  # d2l  ---> subs
         + b_su * uvb  # uvb  ---> subs
         + b_sc * clim  # clim  ---> subs
         
    # Population size:
    psiz ~ b_ps * subs  # subs  ---> psiz 
         + b_pl * lat  # lat  ---> psiz 
         + b_pd * d2l  # d2l  ---> psiz
         + b_pu * uvb  # uvb  ---> psiz
         + b_pc * clim  # clim  ---> psiz

  ';
  semfit_blue_full <- sem(sem_blue_full, data=d_colors_lavaan, se="robust.sem");
  summary(semfit_blue_full, fit.measures=TRUE, standardize=TRUE, rsquare=TRUE, estimates=TRUE, ci=TRUE);
  lavaanPlot(model=semfit_blue_full, coefs=TRUE, sig=1.00, stand=FALSE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
  semPaths(semfit_blue_full, what="est", edge.label.cex = 1.5, fade = FALSE)
  fitMeasures(semfit_blue_full, c("chisq", "df", "pvalue", "cfi", "tli", "nnfi", "rfi"));
  mi <- modindices(semfit_blue_full);

  # The full SEM model:
  sem_blue_sure <- '
    # blue:
    blue ~ b_bu * uvb  # uvb  ---> blue 
         + b_bd * d2l  # d2l  ---> blue 
         + b_bs * subs # subs ---> blue 
         + b_bc * clim # clim ---> blue
         + b_bl * lat  # lat  ---> blue
         + b_bp * psiz # psiz  ---> blue

    # UV-B incidence:   
    uvb  ~ b_ul * lat  # lat  ---> uvb  
    uvb ~~ clim
    uvb ~~ d2l

    # Climate:
    clim ~ b_cl * lat  # lat  ---> clim
       
    # Distance to lakes:  
    d2l  ~ b_dl * lat  # lat  ---> d2l
    
    # Subsistence:
    subs ~ b_sl * lat  # lat  ---> subs 
    subs ~~ clim
    subs ~~ psiz
    subs ~~ uvb
    subs ~~ d2l
    #subs ~~ lat

    # Population size:
    psiz ~ b_pl * lat  # lat  ---> psiz 
         + b_pc * clim  # clim  ---> psiz
         
    psiz ~~ uvb
    psiz ~~ d2l

  ';
  semfit_blue_sure <- sem(sem_blue_sure, data=d_colors_lavaan, se="robust.sem");
  summary(semfit_blue_sure, fit.measures=TRUE, standardize=TRUE, rsquare=TRUE, estimates=TRUE, ci=TRUE);
  lavaanPlot(model=semfit_blue_sure, coefs=TRUE, sig=1.00, stand=FALSE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
  lavaanPlot(model=semfit_blue_sure, coefs=TRUE, sig=0.05, stand=TRUE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
  semPaths(semfit_blue_sure, what="est", edge.label.cex = 1.5, fade = FALSE)
  fitMeasures(semfit_blue_sure, c("chisq", "df", "pvalue", "cfi", "tli", "nnfi", "rfi"));
  mi <- modindices(semfit_blue_sure);

  
  ## Specific tests:
  ## - test if UV-B affects blue:
  #sem_blue_no_uvb__blue <- paste0(sem_blue_full,'
  #  # constraints (fix parameters to 0):
  #  b_bu == 0 # force uvb -/-> blue
  #');
  #semfit_blue_no_uvb__blue <- sem(sem_blue_no_uvb__blue, data=d_colors_lavaan, se="robust.sem");
  #summary(semfit_blue_no_uvb__blue, fit.measures=TRUE, standardize=TRUE, rsquare=TRUE, estimates=TRUE, ci=TRUE);
  #fitMeasures(semfit_blue_no_uvb__blue, c("chisq", "df", "pvalue", "cfi", "tli", "nnfi", "rfi"));
  #(anova_blue_no_uvb__blue <- anova(semfit_blue_full, semfit_blue_no_uvb__blue)); # p = 0.001919 **

  # Save to file:
  sem_blue <- list("data"=d_colors_lavaan,
                   "model_full"=semfit_blue_full,
                   "model_sure"=semfit_blue_sure);
  save(sem_blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
} 
```


We performed [*path analysis*](https://en.wikipedia.org/wiki/Path_analysis_(statistics)) using [`lavaan`](https://lavaan.ugent.be/tutorial/index.html), and we obtained the following model (built principally from *a priori* reasoning, but also incorporating some theoretically sound suggestions based on [modification indices](https://curranbauer.org/what-are-modification-indices-and-should-i-use-them-when-fitting-sems-to-my-own-data/)):

```{r fig.cap=capFig('The path model with non-standardized coefficients, showing all estimated path estimates. Single-headed arrows represent regressions, double-headed arrows represent covariance and variance (when refering to the same node), and the arrows emerging from triangles are the intercepts; blue edges have negative estimates, red ones positive estimates.'), fig.width=15, fig.height=10}
semPaths(sem_blue$model_full, what="est", edge.label.cex=0.75, fade=FALSE, nCharNodes=0, posCol="red", negCol="blue", layout="tree2");
```

```{r fig.cap=capFig('The path model with standardized coefficients; conventions as above.'), fig.width=15, fig.height=10}
semPaths(sem_blue$model_full, what="std", edge.label.cex=0.75, fade=FALSE, nCharNodes=0, posCol="red", negCol="blue", layout="tree2");
```

```{r fig.cap=capFig('The path model with non-standardized coefficients, showing all path estimates with significance. Single-headed arrows represent regressions, while double-headed arrows represent covariance. Please note that this is not a "standard" SEM/path analysis diagram (see below for such a representation).')}
lavaanPlot(model=sem_blue$model_full, coefs=TRUE, sig=1.00, stand=FALSE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
```

```{r fig.cap=capFig('The path model with standardized coefficients (see @grace_interpreting_2005 about pitfalls in interpreting such coefficients) showing only the significant (at the 0.05 level) path estimates with significance.')}
lavaanPlot(model=sem_blue$model_full, coefs=TRUE, sig=0.05, stand=TRUE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
```

This model fits the data well[^interpret_sem]: `r sprintf("*&chi;*^2^(%d) = %.1f, *p* = %.3g; CFI = %.3f, TLI = %.3f, NNFI = %.3f, RMSEA = %.3f 90%%CI [%.3f, %.3f]", fitMeasures(sem_blue$model_full, c("df")), fitMeasures(sem_blue$model_full, c("chisq")), fitMeasures(sem_blue$model_full, c("pvalue")), fitMeasures(sem_blue$model_full, c("cfi")), fitMeasures(sem_blue$model_full, c("tli")), fitMeasures(sem_blue$model_full, c("nnfi")), fitMeasures(sem_blue$model_full, c("rmsea")), fitMeasures(sem_blue$model_full, c("rmsea.ci.lower")), fitMeasures(sem_blue$model_full, c("rmsea.ci.upper")))`.

We also created a path analysis model keeping only causal arrows where causality is fully assumed, either by our world's knowledge (such as the impact of latitude on UV-B radiation) or by our hypothesis (see [Potential predictors - blue]); other more "uncertain" assumed causal arrows being replaced by correlation links.

```{r fig.cap=capFig('The path model with standardized coefficients (see @grace_interpreting_2005 about pitfalls in interpreting such coefficients) showing only the significant (at the 0.05 level) path estimates with significance.')}
lavaanPlot(model=sem_blue$model_sure, coefs=TRUE, sig=0.05, stand=TRUE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
```

This model also fits the data well: `r sprintf("*&chi;*^2^(%d) = %.1f, *p* = %.3g; CFI = %.3f, TLI = %.3f, NNFI = %.3f, RMSEA = %.3f 90%%CI [%.3f, %.3f]", fitMeasures(sem_blue$model_sure, c("df")), fitMeasures(sem_blue$model_sure, c("chisq")), fitMeasures(sem_blue$model_sure, c("pvalue")), fitMeasures(sem_blue$model_sure, c("cfi")), fitMeasures(sem_blue$model_sure, c("tli")), fitMeasures(sem_blue$model_sure, c("nnfi")), fitMeasures(sem_blue$model_sure, c("rmsea")), fitMeasures(sem_blue$model_sure, c("rmsea.ci.lower")), fitMeasures(sem_blue$model_sure, c("rmsea.ci.upper")))`.


Please note that:

- we coded the endogenous variables *blue* and *subsistence* as `ordered` (as "no" < "yes", and 'HG' <  'AGR', respectively) as per https://lavaan.ugent.be/tutorial/cat.html;
- by default, `lavaan` does not estimate the residual variances of categorical endogenous variables (here, *blue* and *subsistence*), and we did not change this behaviour.

With these, it can be seen that (the full model output is in [Appendix II. Path analysis for 'blue']):

- *latitude* seems to have a direct influence on all the other measures: not only on *UV-B* (-), but also on *climate PC1* (+), *distance to lakes* (-), and even *population size* (-), and (possibly) *subsistence* (-); 
- *blue* is affected by *UV-B* (-), *distance to lakes* (-) and *population size* (+), the first strongly supporting the hypothesis of a physiological effect of UV-B incidence, and the latter supporting the suggestion that "cultural complexity" may favor more complex color vocabularies;
- *climate* influences only *population size* (+);
- *subsistence* influences *blue* only through *population size* (+).

(Please note thar including *longitude* in our model, so as to better capture the geographical patterning of linguistic diversity, did not seem to add anything.)

Thus, this path analysis model, despite its caveats (no modeling of measurement error, no modeling of the hierarchical structure due to language families and macroareas), does support the hypothesis of an important influence of UV-B incidence on the existence of a dedicated word for 'blue' above and beyond other factors (such as climate and subsistence strategy).

[^interpret_sem]: For details about interpreting the results of a path analysis or SEM model, please see [here](https://www.statisticssolutions.com/factor-analysis-sem-path-analysis/), [here](http://davidakenny.net/cm/fit.htm), [here](https://methodenlehre.github.io/SGSCLM-R-course/cfa-and-sem-with-lavaan.html) or [here](https://ninamclean.weebly.com/statistics-and-other-bits-and-pieces/interpreting-results-from-path-analysis), but the main ideas are that the *&chi;*^2^ test should **not** be significant, that the various *fit indices* (such as the Comparative Fit Index (CFI), the Tucker-Lewis Index (TLI), and the Non-normed Fit Index (NNFI)) should be as close to 1.0 as possible (ok if ≥ 0.90), and that the Root Mean Square Error of Approximation (RMSEA) should be as small as possible (≤ 0.05) with a narrow 90% confidence interval (ideally with the lower limit very close to 0.0 and the upper limit ≤ 0.08).

<!-- ### Path analysis - paper format -->

<!-- #### Left-right -->

<!-- ```{r} -->

<!-- # find p-values -->
<!-- standardizedsolution(sem_blue$model_sure) -->

<!-- DiagrammeR::grViz(paste0(' -->
<!--   digraph full_blue_a { -->

<!--   # the graph: -->
<!--   graph [overlap = true, nodesep=0.5] -->
<!--   rankdir="LR"; -->

<!--   # the nodes: -->
<!--   node [shape = box, style = "filled", fillcolor = "lightyellow", fontsize=20]; -->
<!--   blue [label = "Blue", tooltip = "Blue", fillcolor="deepskyblue"];  -->
<!--   UVB  [label = "UV-B", tooltip = "UV-B", fillcolor="seagreen1"];  -->
<!--   lat  [label = "Latitude", tooltip = "Latitude"];  -->
<!--   clim [label = "Climate PC1", tooltip = "Climate PC1"];  -->
<!--   subs [label = "Subsistence", tooltip = "Subsistence"];  -->
<!--   psiz [label = "Population size", tooltip = "Population size"];  -->
<!--   d2l  [label = "Distance to lakes", tooltip = "Distance to lakes"];  -->

<!--   # the edges: -->
<!--   edge [style = "solid", color = "black", fontsize=15]; -->

<!--   lat  -> UVB  [label="-0.96 (0.000)", color="blue", fontcolor="blue"] -->
<!--   lat  -> d2l  [label="-0.40 (0.000)", color="blue", fontcolor="blue"] -->
<!--   lat  -> psiz  [label="-0.45 (0.001)", color="blue", fontcolor="blue"] -->
<!--   lat  -> subs  [label="-0.19 (0.092)", color="gray50", fontcolor="gray50", style="dashed"] -->
<!--   lat  -> clim  [label="0.86 (0.000)", color="red", fontcolor="red"] -->
<!--   lat  -> blue  [label="-0.19 (0.522)", color="gray50", fontcolor="gray50", style="dashed"] -->

<!--   subs -> d2l [dir=both, label="-0.24 (0.036)", color="blue", fontcolor="blue"] -->
<!--   subs -> UVB [dir=both, label="-0.28 (0.012)", color="blue", fontcolor="blue"] -->
<!--   subs -> psiz [dir=both, label="0.55 (0.000)", color="red", fontcolor="red"] -->
<!--   subs -> clim [dir=both, label="0.02 (0.899)", color="gray50", fontcolor="gray50", style="dashed"] -->
<!--   subs -> blue [label="0.20 (0.083)", color="gray50", fontcolor="gray50", style="dashed"] -->

<!--   clim -> psiz [label="0.6 (0.000)", color="red", fontcolor="red"] -->
<!--   clim -> UVB [dir=both, label="-0.23 (0.009)", color="blue", fontcolor="blue"] -->
<!--   clim -> blue [label="-0.01 (0.972)", color="gray50", fontcolor="gray50", style="dashed"] -->

<!--   psiz -> d2l [dir=both, label="-0.28 (0.000)", color="blue", fontcolor="blue"] -->
<!--   psiz -> UVB [dir=both, label="-0.17 (0.015)", color="blue", fontcolor="blue"] -->
<!--   psiz -> blue [label="0.31 (0.001)", color="red", fontcolor="red"] -->

<!--   d2l -> UVB [dir=both, label="0.15 (0.055)", color="gray50", fontcolor="gray50", style="dashed"] -->
<!--   d2l -> blue [label="-0.2 (0.013)", color="blue", fontcolor="blue"] -->

<!--   UVB -> blue [label="-0.65 (0.032)", color="blue", fontcolor="blue"] -->

<!-- } -->
<!-- ')) -->


<!-- ``` -->


<!-- #### Top-bottom  -->

<!-- ```{r} -->

<!-- summary(sem_blue$model_sure, fit.measures=TRUE, standardize=TRUE, rsquare=TRUE, estimates=TRUE, ci=TRUE); -->

<!-- standardizedsolution(sem_blue$model_sure) -->

<!-- DiagrammeR::grViz(paste0(' -->
<!--   digraph full_blue_a { -->

<!--   # the graph: -->
<!--   graph [overlap = true, nodesep=0.5] -->
<!--   rankdir="TB"; -->

<!--   # the nodes: -->
<!--   node [shape = box, style = "filled", fillcolor = "lightyellow"]; -->
<!--   blue [label = "Blue", tooltip = "Blue", fillcolor="deepskyblue"];  -->
<!--   UVB  [label = "UV-B", tooltip = "UV-B", fillcolor="seagreen1"];  -->
<!--   lat  [label = "Latitude", tooltip = "Latitude"];  -->
<!--   clim [label = "Climate PC1", tooltip = "Climate PC1"];  -->
<!--   subs [label = "Subsistence", tooltip = "Subsistence"];  -->
<!--   psiz [label = "Population size", tooltip = "Population size"];  -->
<!--   d2l  [label = "Distance to lakes", tooltip = "Distance to lakes"];  -->

<!--   # the edges: -->
<!--   edge [style = "solid", color = "black"]; -->

<!--   lat  -> UVB  [label="-0.96 (0.000)", color="blue", fontcolor="blue"] -->
<!--   lat  -> d2l  [label="-0.40 (0.000)", color="blue", fontcolor="blue"] -->
<!--   lat  -> psiz  [label="-0.45 (0.001)", color="blue", fontcolor="blue"] -->
<!--   lat  -> subs  [label="-0.19 (0.092)", color="gray50", fontcolor="gray50", style="dashed"] -->
<!--   lat  -> clim  [label="0.86 (0.000)", color="red", fontcolor="red"] -->
<!--   lat  -> blue  [label="-0.19 (0.522)", color="gray50", fontcolor="gray50", style="dashed"] -->

<!--   subs -> d2l [dir=both, label="-0.24 (0.036)", color="blue", fontcolor="blue"] -->
<!--   subs -> UVB [dir=both, label="-0.28 (0.012)", color="blue", fontcolor="blue"] -->
<!--   subs -> psiz [dir=both, label="0.55 (0.000)", color="red", fontcolor="red"] -->
<!--   subs -> clim [dir=both, label="0.02 (0.90)", color="gray50", fontcolor="gray50", style="dashed"] -->
<!--   subs -> blue [label="0.20 (0.083)", color="gray50", fontcolor="gray50", style="dashed"] -->

<!--   clim -> psiz [label="0.60 (0.000)", color="red", fontcolor="red"] -->
<!--   clim -> UVB [dir=both, label="-0.23 (0.009)", color="blue", fontcolor="blue"] -->
<!--   clim -> blue [label="-0.01 (0.97)", color="gray50", fontcolor="gray50", style="dashed"] -->

<!--   psiz -> d2l [dir=both, label="-0.28 (0.000)", color="blue", fontcolor="blue"] -->
<!--   psiz -> UVB [dir=both, label="-0.17 (0.015)", color="blue", fontcolor="blue"] -->
<!--   psiz -> blue [label="0.31 (0.001)", color="red", fontcolor="red"] -->

<!--   d2l -> UVB [dir=both, label="0.15 (0.055)", color="gray50", fontcolor="gray50", style="dashed"] -->
<!--   d2l -> blue [label="-0.2 (0.013)", color="blue", fontcolor="blue"] -->

<!--   UVB -> blue [label="-0.65 (0.032)", color="blue", fontcolor="blue"] -->

<!-- } -->
<!-- ')) -->
<!-- ``` -->

### Predicting 'blue'

We check how good are various techniques at predicting the existence of a dedicated word for 'blue' from a collection of potential predictors.
On the one hand, we estimate this when using the full dataset (i.e., we use the full data both for fitting the model and for computing various measures of goodness of prediction), but also on how well these models generalize by repeatedly generating random training and testing subsets (containing 80% and 20% of the data, respectively; we stratify by macroarea and, except for Bayesian mixed-effects regressions, we do not use the language family; please note that random forests automatically sample the data).

```{r include=FALSE}
file_name <- "./cached_results/traning-testing-splits.RData";
if( !all(file.exists(file_name)) )
{
  ## Use all the methods on all the data as well as on training/testing subsets
  
  # The data:
  d <- d_colors; d$macroarea <- as.factor(d$macroarea); d$glottocode_family <- as.factor(d$glottocode_family);
  
  # Generate the training/testing splits into a training (80%) and a testing (20%) set stratified by macroarea:
  n_train <- 100; # the number of training/testing splits
  train_test_splits <- lapply(1:n_train, function(i) rsample::initial_split(d, prop=0.80, strata="macroarea"));
  
  # Save:
  save(d, n_train, train_test_splits, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
} 

```



#### Multiple regression

We used Bayesian mixed-effects logistic regression to predict the existence of a specific word for 'blue' (*blue*) from all the potential predictors.

```{r include=FALSE}
file_name <- "./cached_results/brms_blue.RData";
if( !all(file.exists(file_name)) )
{
  # Save to file:
  brms_blue <- list("data"=d);

    
  ## On the full data:
  
  # With all potential predictors:
  brms_blue__full <- brm(exists_blue ~ 1 + 
                           # the languages:
                         + UVB_r                                                       # UV-B incidence
                         + longitude_r + latitude_r                                    # geographic location
                         + subsistence  + log_popSize                                  # subsistence strategy and population size
                         + clim_PC1_r + clim_PC2_r + clim_PC3_r + hum_median + hum_IQR # climate, ecology and humidity
                         + elevation_r                                                 # altitude 
                         + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r  # distances to large bodies of water 
                         # the language family origins:
                         + UVB_family_r                                                                                   # UV incidence
                         + longitude_family_r + latitude_family_r                                                         # geographic location
                         + clim_PC1_family_r + clim_PC2_family_r + clim_PC3_family_r + hum_median_family + hum_IQR_family # climate, ecology and humidity 
                         + elevation_family_r                                                                             # altitude 
                         + dist2water_family_r + dist2ocean_family_r + dist2lakes_family_r + dist2rivers_family_r         # distances to large bodies of water 
                         + gen_D1_r + gen_D2_r + gen_D3_r + gen_D4_r + gen_D5_r + gen_D6_r + gen_D7_r + gen_D8_r + gen_D9_r + gen_D10_r
                         + (1 | glottocode_family) + (1 | macroarea),                   # random factors
                         data = d, 
                         family=bernoulli(link="logit"), 
                         prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                 prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                         save_all_pars=TRUE, # needed for Bayes factors
                         sample_prior=TRUE,  # needed for hypotheses tests
                         cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  summary(brms_blue__full); mcmc_plot(brms_blue__full, type="trace"); mcmc_plot(brms_blue__full);
  (hdi_full <- hdi(brms_blue__full, ci=0.95)); (rope_full <- rope(brms_blue__full, ci=0.95));
  brms_blue__full <- brms_fit_indices(brms_blue__full);
  (cmp_null_full <- brms_compare_models(b_0__blue, brms_blue__full, "null", "full"));
  
  # Simplified model:
  brms_blue__reduced <- brm(exists_blue ~ 1 + UVB_r + log_popSize + dist2lakes_r + gen_D7
                            + (1 | glottocode_family) + (1 | macroarea),
                            data = d, 
                         family=bernoulli(link="logit"), 
                         prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                 prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                         save_all_pars=TRUE, # needed for Bayes factors
                         sample_prior=TRUE,  # needed for hypotheses tests
                         cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  
  summary(brms_blue__reduced); mcmc_plot(brms_blue__reduced, type="trace"); mcmc_plot(brms_blue__reduced);
  (hdi_reduced <- hdi(brms_blue__reduced, ci=0.95)); (rope_reduced <- rope(brms_blue__reduced, ci=0.95)); (fixef_reduced <- fixef(brms_blue__reduced));
  brms_blue__reduced <- brms_fit_indices(brms_blue__reduced);

  # Model comparison and performance:
  (cmp_full_reduced <- brms_compare_models(brms_blue__full, brms_blue__reduced, "full", "reduced")); # reduced is better
  (cmp_null_reduced <- brms_compare_models(b_0__blue, brms_blue__reduced, "null", "reduced")); # reduced is better
  (cm_full    <- confusionMatrix(d$exists_blue, factor(ifelse(predict(brms_blue__full)[,"Estimate"] < 0.5, "no", "yes"), levels=c("no", "yes")), positive="yes"));
  (cm_reduced <- confusionMatrix(d$exists_blue, factor(ifelse(predict(brms_blue__reduced)[,"Estimate"] < 0.5, "no", "yes"), levels=c("no", "yes")), positive="yes"));
  
  # Save:
  brms_blue$both <- list("full"   =list("cmp_2_null"=cmp_null_full, 
                                                   "HDI"=hdi_full, "ROPE"=rope_full, "confusionMatrix"=cm_full), 
                                    "reduced"=list("cmp_2_null"=cmp_null_reduced, "cmp_2_full"=cmp_full_reduced, 
                                                   "fixef"=fixef_reduced, "HDI"=hdi_reduced, "ROPE"=rope_reduced, "confusionMatrix"=cm_reduced));
  
  ## On the training/testing sets:
  results <- pblapply(1:n_train, function(i) # very computationally expensive
    {
      # split the data:
      data_train <- training(train_test_splits[[i]]);
      data_test  <- testing(train_test_splits[[i]]);

      # fit the models: avoid recompiling the model by "updating" the data it is fit on:
      invisible(capture.output(brms_1   <- update(brms_blue__full, newdata=data_train, refresh=0, 
                                                      save_all_pars=TRUE, sample_prior=TRUE, cores=brms_ncores, 
                                                      iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10)),
                               type="message")); # capture the messages printed by Stan
      
      # confusion matrices:
      cm_brms_full   <- confusionMatrix(data_test$exists_blue, factor(ifelse(predict(brms_1,   newdata=data_test, allow_new_levels=TRUE)[,"Estimate"] < 0.5, 
                                                                            "no", "yes"), levels=c("no", "yes")), positive="yes");
      # predictors:
      hdi_full   <- hdi(brms_1, ci=0.95); rope_full   <- rope(brms_1,   ci=0.95, verbose=FALSE);

      # return the results:
      success_full <- data.frame("replication"  =i, # replication
                                 "accuracy"     =cm_brms_full$overall["Accuracy"],
                                 "sensitivity"  =cm_brms_full$byClass["Sensitivity"], 
                                 "specificity"  =cm_brms_full$byClass["Specificity"],
                                 "precision"    =cm_brms_full$byClass["Precision"],
                                 "recall"       =cm_brms_full$byClass["Recall"],
                                 row.names=NULL);
      preds_full <- data.frame("replication"=i, # replication,
                               "predictor"  =rownames(fixef(brms_1)), 
                               "estimate"   =fixef(brms_1)[,"Estimate"],
                               as.data.frame(hdi_full)[,c("CI_low", "CI_high")],
                               as.data.frame(rope_full)[,c("ROPE_low", "ROPE_high", "ROPE_Percentage")],
                               row.names=NULL);

      return (list("replication"       =i,
                   "data_train_indices"=train_test_splits[[i]]$in_id, 
                   "data_test_indices" =setdiff(1:nrow(train_test_splits[[i]]$data), train_test_splits[[i]]$in_id),
                   "full"         =list("success"=success_full,   "predictors"=preds_full)));
    });
  
  # Assemble the various results and save them:
  brms_blue$training_testing <- list("replications"   =n_train,
                                     "both"   =list("success"   =do.call(rbind, lapply(results, function(x) x$full$success)),   
                                                            "predictors"=do.call(rbind, lapply(results, function(x) x$full$predictors))));
  
  # Save:
  save(brms_blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
} 


```


On the full dataset, Bayesian mixed effects logistic regression with family and macroarea as random effects and using all potential predictors as fixed effects, fits the data very well: 

```{r}
tmp_allpred <- data.frame("Predictors" =c("All predictors"),
                 "Accuracy"   =100*c(brms_blue$both$full$confusionMatrix$overall["Accuracy"]),
                 "Sensitivity"=100*c(brms_blue$both$full$confusionMatrix$byClass["Sensitivity"]), 
                 "Specificity"=100*c(brms_blue$both$full$confusionMatrix$byClass["Specificity"]),
                 "Precision"  =100*c(brms_blue$both$full$confusionMatrix$byClass["Precision"]),
                 "Recall"     =100*c(brms_blue$both$full$confusionMatrix$byClass["Recall"]))

kable(tmp_allpred,
      row.names=FALSE, digits=c(NA, 1, 1, 1, 1, 1), 
      caption=capTab("Success (as %) on the full dataset using Bayesian mixed effects logistic regressions with all potential predictors."));
```

After iterative manual simplification, we still fit the data very well: 

```{r}
kable(data.frame("Predictors" =c("Set of predictors"),
                 "Accuracy"   =100*c(brms_blue$both$reduced$confusionMatrix$overall["Accuracy"]),
                 "Sensitivity"=100*c(brms_blue$both$reduced$confusionMatrix$byClass["Sensitivity"]), 
                 "Specificity"=100*c(brms_blue$both$reduced$confusionMatrix$byClass["Specificity"]),
                 "Precision"  =100*c(brms_blue$both$reduced$confusionMatrix$byClass["Precision"]),
                 "Recall"     =100*c(brms_blue$both$reduced$confusionMatrix$byClass["Recall"])),
      row.names=FALSE, digits=c(NA, 1, 1, 1, 1, 1), 
      caption=capTab("Success (as %) on the full dataset using Bayesian mixed effects logistic regressions with keeping only the predictors significantly contributing."));
```

In this case, the retained predictors are:

```{r}
kable(data.frame("predictor"  =rownames(brms_blue$both$reduced$fixef), 
                 "estimate"   =round(brms_blue$both$reduced$fixef[,"Estimate"],2),
                 "hdi"=paste0("[", round(brms_blue$both$reduced$HDI[,"CI_low"],2), ",", round(brms_blue$both$reduced$HDI[,"CI_high"],2), "]"),
                 "p.rope"=sprintf("%.2g",brms_blue$both$reduced$ROPE[,"ROPE_Percentage"]),
                 row.names=NULL), 
      row.names=FALSE, col.names=c("Predictor", "*&beta;* (estimate)", "*&beta;* (95% HDI)", "*p*~ROPE~"),
      caption=capTab(paste0("Retained predictors for Bayesian mixed effects logistic regressions following iterative maual simplification on the full dataset. The ROPE is [", round(brms_blue$both$reduced$ROPE[1,"ROPE_low"],2), ",", round(brms_blue$both$reduced$ROPE[1,"ROPE_high"],2), "].")));
```


When randomly splitting the dataset into 80% training/20% testing subsets `r brms_blue$training_testing$replications` times, using all the potential predictors, we see that:

```{r}
tmp_full   <- vapply(brms_blue$training_testing$both$success[,-1],    function(x) sprintf("%.1f%% ±%.1f%%", mean(x,na.rm=TRUE)*100, sd(x,na.rm=TRUE)*100), character(1));

tmp <- data.frame("Predictors" =c("All predictors"),
                  "accuracy"   =c(tmp_full[ grep("accuracy",    names(tmp_full), fixed=TRUE) ]),
                  "sensitivity"=c(tmp_full[ grep("sensitivity", names(tmp_full), fixed=TRUE) ]),
                  "specificity"=c(tmp_full[ grep("specificity", names(tmp_full), fixed=TRUE) ]),
                  "precision"  =c(tmp_full[ grep("precision",   names(tmp_full), fixed=TRUE) ]),
                  "recall"     =c(tmp_full[ grep("recall",      names(tmp_full), fixed=TRUE) ]));
kable(tmp, row.names=FALSE, col.names=c("Predictors", "Accuracy", "Sensitivity", "Specificity", "Precision", "Recall"),
      caption=capTab(paste0("Various measures of success at predicting *blue* using Bayesian mixed effects logistic regressions splitting the dataset randomly into 80% training/20% testing subsets, ",brms_blue$training_testing$replications," times")));

```



```{r fig.cap=capFig(paste0("Various measures of success using Bayesian mixed effects logistic regressions splitting the dataset randomly into 80% training/20% testing subsets, ",brms_blue$training_testing$replications," times. Boxplots show the spread of the training/testing values, while the solid horizontal red lines show the values when using the full dataset.")), fig.width=1*7, fig.height=5}
ggplot(brms_blue$training_testing$both$success %>% 
         reshape2::melt(id.vars=c("replication"), variable.name="measure", value.name="value") %>% 
         mutate("measure"=factor(measure, levels=c("accuracy", "sensitivity", "specificity", "precision", "recall")), "value"=value*100), 
       aes(x=measure, y=value)) + 
  ylim(c(0,100)) + xlab("Measures of success") + ylab("Value (%)") +
  theme_bw() +
  geom_boxplot(show.legend=FALSE) + 
  # draw alternating vertical bands:
  new_scale_fill() + 
  geom_rect(aes(xmin=as.numeric(measure)-0.5, xmax=as.numeric(measure)+0.5, ymin=0, ymax=100, fill=as.factor(as.numeric(measure) %% 2 == 0)), show.legend=FALSE) + 
  scale_fill_manual("", values=c("white", "gray95")) +
  # draw the actual boxplots:
  new_scale_fill() + 
  geom_boxplot(aes(x=measure, y=value)) + 
  # measures on the full dataset:
  geom_rect(data=tmp_allpred %>% 
               reshape2::melt(measure.vars=c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall"), variable.name="measure", value.name="value") %>% 
               mutate("measure"=factor(measure, levels=c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall"), labels=c("accuracy", "sensitivity", "specificity", "precision", "recall"))), 
             aes(xmin=as.numeric(measure)-0.5, xmax=as.numeric(measure)+0.5, ymin=value, ymax=value), fill=NA, color="red") + 
theme(legend.position="right");
```



#### Conditional inference trees

We used conditional inference trees (as implemented by `ctree()` in package `partykit`) to predict the existence of a specific word for 'blue' (*blue*) from all the potential predictors.

```{r include=FALSE}
file_name <- "./cached_results/ctree_blue.RData";
if( !all(file.exists(file_name)) )
{
  # Decision tree with all potential predictors:
  ctree_blue__full <- ctree(exists_blue ~ 
                              # the languages:
                              macroarea                                                   # macroarea
                            + UVB_r                                                       # UV-B incidence
                            + longitude_r + latitude_r                                    # geographic location
                            + subsistence  + log_popSize                                  # subsistence strategy and population size
                            + clim_PC1_r + clim_PC2_r + clim_PC3_r + hum_median + hum_IQR # climate, ecology and humidity
                            + elevation_r                                                 # altitude 
                            + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r  # distances to large bodies of water 
                            # the language family origins:
                            + UVB_family_r                                                                                   # UV incidence
                            + longitude_family_r + latitude_family_r                                                         # geographic location
                            + clim_PC1_family_r + clim_PC2_family_r + clim_PC3_family_r + hum_median_family + hum_IQR_family # climate, ecology and humidity 
                            + elevation_family_r                                                                             # altitude 
                            + dist2water_family_r + dist2ocean_family_r + dist2lakes_family_r + dist2rivers_family_r+     # distances to large bodies of water
                              + gen_D1_r + gen_D2_r + gen_D3_r + gen_D4_r + gen_D5_r + gen_D6_r + gen_D7_r + gen_D8_r + gen_D9_r + gen_D10_r,  # genetic distances    
                            data = d, 
                            control = ctree_control(testtype = c("MonteCarlo")));
 

##--> More examples are part of
##-->   demo(error.catching)
    
  ## On the training/testing sets:
  results <- pblapply(1:n_train, function(i) # very computationally expensive
    {
      # split the data:
      data_train <- training(train_test_splits[[i]]);
      data_test  <- testing(train_test_splits[[i]]);

      # fit the models:
       tryCatch({ctree_1   <- ctree(exists_blue ~ 
                               # the languages:
                               macroarea                                                   # macroarea
                             + UVB_r                                                       # UV-B incidence
                             + longitude_r + latitude_r                                    # geographic location
                             + subsistence  + log_popSize                                  # subsistence strategy and population size
                             + clim_PC1_r + clim_PC2_r + clim_PC3_r + hum_median + hum_IQR # climate, ecology and humidity
                             + elevation_r                                                 # altitude 
                             + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r  # distances to large bodies of water 
                             # the language family origins:
                             + UVB_family_r                                                                                   # UV incidence
                             + longitude_family_r + latitude_family_r                                                         # geographic location
                             + clim_PC1_family_r + clim_PC2_family_r + clim_PC3_family_r + hum_median_family + hum_IQR_family # climate, ecology and humidity 
                             + elevation_family_r                                                                             # altitude 
                             + dist2water_family_r + dist2ocean_family_r + dist2lakes_family_r + dist2rivers_family_r       # distances to large bodies of water  
                             + gen_D1_r + gen_D2_r + gen_D3_r + gen_D4_r + gen_D5_r + gen_D6_r + gen_D7_r + gen_D8_r + gen_D9_r + gen_D10_r,  # genetic distances    

                             data = data_train, 
                             control = ctree_control(testtype = c("MonteCarlo")));
       
     
      
      # confusion matrices:
      cm_ctree_1   <- confusionMatrix(data_test$exists_blue, predict(ctree_1,  newdata=data_test, allow_new_levels=TRUE), positive="yes");

      # return the results:
      success_1 <- data.frame("replication"  =i, # replication
                                 "accuracy"     =cm_ctree_1$overall["Accuracy"],
                                 "sensitivity"  =cm_ctree_1$byClass["Sensitivity"], 
                                 "specificity"  =cm_ctree_1$byClass["Specificity"],
                                 "precision"    =cm_ctree_1$byClass["Precision"],
                                 "recall"       =cm_ctree_1$byClass["Recall"],
                                 row.names=NULL);

      return (list("replication"       =i,
                   "data_train_indices"=train_test_splits[[i]]$in_id, 
                   "data_test_indices" =setdiff(1:nrow(train_test_splits[[i]]$data), train_test_splits[[i]]$in_id),
                   "all"         =list("success"=success_1)));
      
       }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
    });
  
  
  # Save to file:
  ctree_blue <- list("data"=d,
                     "all"=ctree_blue__full, 
                     "training_testing"=list("replications"   =n_train,
                                             "all"   =list("success"=do.call(rbind, lapply(results, function(x) x$all$success)))));
  save(ctree_blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
} 
```

When using the full dataset, the decision trees explain it quite well:

```{r fig.cap=capFig('Conditional inference trees for predicting *blue* using all the potential predictors. Please note that some predictors are transformed (the "_r" suffix), the most important here being *macroarea*.'), fig.width=12, fig.height=7, fig.show='hold', out.height="50%"}
plot(ctree_blue$all,    gp=gpar(fontsize = 11));
```


```{r}
cm1 <- confusionMatrix(ctree_blue$data$exists_blue, predict(ctree_blue$all), positive="yes");

tmp_all <- data.frame("Predictors" =c("All predictors"),
                 "Accuracy"   =100*c(cm1$overall["Accuracy"]),
                 "Sensitivity"=100*c(cm1$byClass["Sensitivity"]), 
                 "Specificity"=100*c(cm1$byClass["Specificity"]),
                 "Precision"  =100*c(cm1$byClass["Precision"]),
                 "Recall"     =100*c(cm1$byClass["Recall"]))
kable(tmp_all,
      row.names=FALSE, digits=c(NA, 1, 1, 1, 1, 1), 
      caption=capTab("Success (as %) on the full dataset using conditional inference trees with all potential predictors."));
```

It seems that the best predictor for having a word for 'blue' is the macroarea, with a distinction between Eurasian and Australian languages, on the one hand, and African, Papunesian and American languages, on the other, in both regions with small languages (≤ ≈1,000,000 speakers) having less 'blue'; while in Eurasia and Australia UV-B incidence has a negative effect, in Africa, Papunesia and America, it is the distance to large bodies of water (lakes and oceans) that does.

When randomly splitting the dataset into 80% training/20% testing subsets `r ctree_blue$training_testing$replications` times, using all the potential predictors, we see that:

```{r}
tmp_ncol   <- vapply(ctree_blue$training_testing$all$success[,-1],    function(x) sprintf("%.1f%% ±%.1f%%", mean(x,na.rm=TRUE)*100, sd(x,na.rm=TRUE)*100), character(1));

tmp <- data.frame("Predictors" =c("All predictors"),
                  "accuracy"   =c(tmp_ncol[ grep("accuracy",    names(tmp_ncol), fixed=TRUE) ]),
                  "sensitivity"=c(tmp_ncol[ grep("sensitivity", names(tmp_ncol), fixed=TRUE) ]),
                  "specificity"=c(tmp_ncol[ grep("specificity", names(tmp_ncol), fixed=TRUE) ]),
                  "precision"  =c(tmp_ncol[ grep("precision",   names(tmp_ncol), fixed=TRUE) ]),
                  "recall"     =c(tmp_ncol[ grep("recall",      names(tmp_ncol), fixed=TRUE) ]));
kable(tmp, row.names=FALSE, col.names=c("Predictors", "Accuracy", "Sensitivity", "Specificity", "Precision", "Recall"),
      caption=capTab(paste0("Various measures of success at predicting *blue* using conditional inference trees splitting the dataset randomly into 80% training/20% testing subsets, ",ctree_blue$training_testing$replications," times.")));
```

```{r fig.cap=capFig(paste0("Various measures of success using conditional inference trees splitting the dataset randomly into 80% training/20% testing subsets, ",ctree_blue$training_testing$replications," times. Boxplots show the spread of the training/testing values, while the solid horizontal red lines show the values when using the full dataset.")), fig.width=1*7, fig.height=5}
ggplot(ctree_blue$training_testing$all$success %>% 
        reshape2::melt(id.vars=c("replication"), variable.name="measure", value.name="value") %>% 
         mutate("measure"=factor(measure, levels=c("accuracy", "sensitivity", "specificity", "precision", "recall")), "value"=value*100), 
       aes(x=measure, y=value)) +
  ylim(c(0,100)) + xlab("Measures of success") + ylab("Value (%)") +
  theme_bw() +
  geom_boxplot(show.legend=FALSE) + 
  # draw alternating vertical bands:
  new_scale_fill() + 
  scale_fill_manual("", values=c("white", "gray95")) +
  geom_rect(aes(xmin=as.numeric(measure)-0.5, xmax=as.numeric(measure)+0.5, ymin=0, ymax=100, fill=as.factor(as.numeric(measure) %% 2 == 0)), show.legend=FALSE) + 
  # draw the actual boxplots:
  new_scale_fill() + 
  geom_boxplot(aes(x=measure, y=value)) + 
  # measures on the full dataset:
  geom_rect(data=tmp_all %>% 
               reshape2::melt(measure.vars=c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall"), 
                              variable.name="measure", value.name="value") %>% 
               mutate("measure"=factor(measure, 
                                       levels=c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall"), 
                                       labels=c("accuracy", "sensitivity", "specificity", "precision", "recall"))), 
             aes(xmin=as.numeric(measure)-0.5, xmax=as.numeric(measure)+0.5, ymin=value, ymax=value), color="red", fill=NA) +
  theme(legend.position="right");
```


#### Random forests

We used random forests (as implemented by `randomForest()` in package `randomForest`) and conditional random forests (as implemented by `cforest()` in package `partykit`) to predict the existence of a specific word for 'blue' (*blue*) from all the potential predictors.

```{r include=FALSE}
file_name <- "./cached_results/rforest_blue.RData";
if( !all(file.exists(file_name)) )
{
  # The potential predictors:
  list_predictors <- c(# the languages:
                              "macroarea",                                                       # macroarea
                              "UVB_r",                                                           # UV-B incidence
                              "longitude_r", "latitude_r",                                       # geographic location
                              "subsistence", "log_popSize",                                      # subsistence strategy and population size
                              "clim_PC1_r", "clim_PC2_r", "clim_PC3_r", "hum_median", "hum_IQR", # climate, ecology and humidity
                              "elevation_r",                                                     # altitude 
                              "dist2water_r", "dist2ocean_r", "dist2lakes_r", "dist2rivers_r",   # distances to large bodies of water 
                              # the language family origins:
                              "UVB_family_r", # UV incidence
                              "longitude_family_r", "latitude_family_r",                                                            # geographic location
                              "clim_PC1_family_r", "clim_PC2_family_r", "clim_PC3_family_r", "hum_median_family", "hum_IQR_family", # climate, ecology and humidity 
                              "elevation_family_r",                                                                                 # altitude 
                              "dist2water_family_r", "dist2ocean_family_r", "dist2lakes_family_r",       # distances to large bodies of water 
                              "dist2rivers_family_r" , "gen_D1_r" , "gen_D2_r" , "gen_D3_r" , "gen_D4_r" , "gen_D5_r" , "gen_D6_r" , "gen_D7_r" , "gen_D8_r" , "gen_D9_r" , "gen_D10_r");  # genetic distances    

  # Collect the success measures and the predictor importance based on the gini index (randomForest) and the accuracy indices (randomForest and cForest):
  n_replications <- n_train; # the number of replications
  cf_ntree       <- 500; # the number of trees for cforest
  cf_nperm       <- 10;  # number of permutations for cforest:varimp
  f_full       <- formula(paste0("exists_blue ~ ", paste0(list_predictors,collapse=" + "))); 
  results <- pblapply(1:n_replications, function(i) # very computationally expensive
    {
      # fit the random forests:
      rf_full   <- randomForest(f_full, data=d, importance=TRUE);
      cf_full   <- cforest(f_full, data=d, ntree=cf_ntree);
      
      # confusion matrices:
      cm_rf_full   <- confusionMatrix(d$exists_blue, rf_full$predicted,   positive="yes"); 
      cm_cf_full   <- confusionMatrix(d$exists_blue, predict(cf_full),    positive="yes");
      
      # predictor importance:
      pia_rf_full   <- importance(rf_full,   type=1); # accuracy-based
      pig_rf_full   <- importance(rf_full,   type=2); # gini-based
      piu_cf_full   <- varimp(cf_full, conditional=FALSE, nperm=cf_nperm); # unconditional

      # return the results:
      success_full <- data.frame("replication"     =i,                               # replication
                                 "rf_accuracy"     =cm_rf_full$overall["Accuracy"], # randomForest
                                 "rf_sensitivity"  =cm_rf_full$byClass["Sensitivity"], 
                                 "rf_specificity"  =cm_rf_full$byClass["Specificity"],
                                 "rf_precision"    =cm_rf_full$byClass["Precision"],
                                 "rf_recall"       =cm_rf_full$byClass["Recall"],
                                 "cf_accuracy"     =cm_cf_full$overall["Accuracy"], # cforest
                                 "cf_sensitivity"  =cm_cf_full$byClass["Sensitivity"], 
                                 "cf_specificity"  =cm_cf_full$byClass["Specificity"],
                                 "cf_precision"    =cm_cf_full$byClass["Precision"],
                                 "cf_recall"       =cm_cf_full$byClass["Recall"], 
                                 row.names=NULL);
      pi_df_full  <- data.frame("replication"      =i,                    # replication
                                "predictor"        =list_predictors, # predictors
                                "rf_accuracy_based"=pia_rf_full,         # randomForest accuracy-based
                                "rf_gini_based"    =pig_rf_full,         # randomForest gini-based
                                "cf_unconditional" =piu_cf_full,         # cforest unconditional
                                row.names=NULL);
      
      return (list("replication" =i,
                   "full"   =list("success"=success_full,   "pred_importance"=pi_df_full)));
    }); #, mc.cores=max(detectCores(all.tests=TRUE, logical=FALSE), 1, na.rm=TRUE)); # try to use multiple cores, if present (limited to a maximum of 8)
  
  # Assemble the various results and save them:
  rforest_blue <- list("data"           =d,
                       "replications"   =n_replications,
                       "full"   =list("success"=do.call(rbind, lapply(results, function(x) x$full$success)),  "pred_importance"=do.call(rbind, lapply(results, function(x) x$full$pred_importance))));
  save(rforest_blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
} 
```

The (conditional) random forests are quite successful at predicting *blue*:

```{r}
tmp_full   <- vapply(rforest_blue$full$success[,-1],    function(x) sprintf("%.1f%% ±%.1f%%", mean(x,na.rm=TRUE)*100, sd(x,na.rm=TRUE)*100), character(1));

tmp <- data.frame("method"     =rep(c("random forest", "conditional random forest"), times=1),
                  "accuracy"   =c(tmp_full[ grep("accuracy",    names(tmp_full), fixed=TRUE) ]),
                  "sensitivity"=c(tmp_full[ grep("sensitivity", names(tmp_full), fixed=TRUE) ]),
                  "specificity"=c(tmp_full[ grep("specificity", names(tmp_full), fixed=TRUE) ]),
                  "precision"  =c(tmp_full[ grep("precision",   names(tmp_full), fixed=TRUE) ]),
                  "recall"     =c(tmp_full[ grep("recall",      names(tmp_full), fixed=TRUE) ]));
kable(tmp, row.names=FALSE, col.names=c("Method", "Accuracy", "Sensitivity", "Specificity", "Precision", "Recall"),
      caption=capTab(paste0("Various measures of success at predicting *blue* using two random forest methods, ",rforest_blue$replications," replications.")));
```

```{r fig.cap=capFig(paste0("Various measures of success using (conditional) random forests with ",rforest_blue$replications," replications.")), fig.width=2*4, fig.height=5}
ggplot(rbind(cbind(rforest_blue$full$success, "full"="yes")) %>% 
         reshape2::melt(id.vars=c("replication", "full"), variable.name="measure", value.name="value") %>% 
         mutate("method"=factor(if_else(substring(measure,1,2) == "rf", "random forest", "conditional random forest"), levels=c("random forest", "conditional random forest")), 
                "measure"=factor(substring(measure,4), levels=c("accuracy", "sensitivity", "specificity", "precision", "recall")), 
                "value"=value*100), 
       aes(x=measure, y=value)) + 
  ylim(c(0,100)) + xlab("Measures of success") + ylab("Value (%)") +
  theme_bw() +
  geom_boxplot(show.legend=FALSE) + 
  # draw alternating vertical bands:
  new_scale_fill() + 
  geom_rect(aes(xmin=as.numeric(measure)-0.5, xmax=as.numeric(measure)+0.5, ymin=0, ymax=100, fill=as.factor(as.numeric(measure) %% 2 == 0)), show.legend=FALSE) + 
  scale_fill_manual("", values=c("white", "gray95")) +
  # draw the actual boxplots:
  new_scale_fill() + 
  geom_boxplot(aes(x=measure, y=value)) + 
  theme(legend.position="bottom") +
  facet_grid(. ~ method);
```

The importance of the predictors is:

```{r fig.cap=capFig(paste0("Different measures of predictor importance using ",rforest_blue$replications," replications. Left column: Accuracy-based predictor importance from random forests; this is a measure of the amount by which removing a variable decreases the accuracy thus higher values point to more relevant predictors. Column in the center: Gini-index-based predictor importance from random forests; this measures by how much the Gini impurity decrease when a variable is chosen to split a node (please note that only the relative values matter, and that there is a bias towards using numeric variables to split nodes). Right column: Unconditional predictor importance from conditional random forests; this is similar to the accuracy-based importance from random forests. ")), fig.width=4*3, fig.height=7*1}
grid.arrange(ggplot(rforest_blue$full$pred_importance, 
                    aes(x=reorder(predictor, MeanDecreaseAccuracy, median), y=MeanDecreaseAccuracy)) +
               theme_bw() +
               geom_boxplot(fill="gold", color="black") +
               labs(x="Predictors (sorted by decreasing importance)", y="Mean decrease accuracy") +
               theme(axis.text.y = element_text(angle=30, hjust=1, size=10), 
                     axis.text.x = element_text(hjust=1, size=10),
                     axis.title=element_text(size=16),
                     legend.position="none") +
               ggtitle("Accuracy-based") + 
               coord_flip() + 
               NULL,
             ggplot(rforest_blue$full$pred_importance, 
                    aes(x=reorder(predictor, MeanDecreaseGini, median), y=MeanDecreaseGini)) +
               theme_bw() +
               geom_boxplot(fill="gold", color="black") +
               labs(x="Predictors (sorted by decreasing importance)", y="Mean decrease Gini index") +
               theme(axis.text.y = element_text(angle=30, hjust=1, size=10), 
                     axis.text.x = element_text(hjust=1, size=10),
                     axis.title=element_text(size=16),
                     legend.position="none") +
               ggtitle("Gini-based") + 
               coord_flip() + 
               NULL,
             ggplot(rforest_blue$full$pred_importance, 
                    aes(x=reorder(predictor, cf_unconditional, median), y=cf_unconditional)) +
               theme_bw() +
               geom_boxplot(fill="gold", color="black") +
               labs(x="Predictors (sorted by decreasing importance)", y="Unconditional importance") +
               theme(axis.text.y = element_text(angle=30, hjust=1, size=10), 
                     axis.text.x = element_text(hjust=1, size=10),
                     axis.title=element_text(size=16),
                     legend.position="none") +
               ggtitle("Unconditional importance") + 
               coord_flip() + 
               NULL,
             ncol=3);
```

It can be seen that:

a) *blue* is relatively well predicted, and that conditional random forests perform slightly better than "classic" random forests;
b) the various methods for estimating the predictor importance produce very similar results;
c) *population size* is the most important predictor
d) the importance of the other predictors is relatively stable, and suggests that *latitude*, *UV-B incidence*, *humidity*, *climate PC1* and *distance to lakes* tend to be the (next) most important.


#### Support Vector Machines (SVM)

We used Support Vector Machines (SVMs, as implemented by `fit(...,model="svm")` in the `rminer` package) to predict the existence of a specific word for 'blue' (*blue*) from all the potential predictors.
Here, we randomly split the dataset into a training (80% of the data points) and a testing (the remaining 20%) set (stratified on *blue*), repeated multiple times.

```{r include=FALSE}
file_name <- "./cached_results/svm_blue.RData";
if( !all(file.exists(file_name)) )
{
  # The potential predictors:
  list_predictors <- c(# the languages:
                              "macroarea",                                                       # macroarea
                              "UVB_r",                                                           # UV-B incidence
                              "longitude_r", "latitude_r",                                       # geographic location
                              "subsistence", "log_popSize",                                      # subsistence strategy and population size
                              "clim_PC1_r", "clim_PC2_r", "clim_PC3_r", "hum_median", "hum_IQR", # climate, ecology and humidity
                              "elevation_r",                                                     # altitude 
                              "dist2water_r", "dist2ocean_r", "dist2lakes_r", "dist2rivers_r",   # distances to large bodies of water 
                              # the language family origins:
                              "UVB_family_r", # UV incidence
                              "longitude_family_r", "latitude_family_r",                                                            # geographic location
                              "clim_PC1_family_r", "clim_PC2_family_r", "clim_PC3_family_r", "hum_median_family", "hum_IQR_family", # climate, ecology and humidity 
                              "elevation_family_r",                                                                                 # altitude 
                              "dist2water_family_r", "dist2ocean_family_r", "dist2lakes_family_r", "dist2rivers_family_r");  # genetic distances    
         # distances to large bodies of water 
  # remove the NAs in the relevant variables as SVM complains...
  #d <- d[ complete.cases(d[,c("exists_blue", list_predictors)])];
  d <- d[ complete.cases(d[,c("exists_blue", list_predictors)]), c("exists_blue", list_predictors)];

  # Save:
  svm_blue <- list("data"=d);
  
  
  # On all the data:
  f_full         <- formula(paste0("exists_blue ~ ", paste0(list_predictors,collapse=" + "))); 
  
  # fit the SVMs:
  svm_fit_full   <- rminer::fit(f_full, d, model="svm", task="class");

  # confusion matrices on the test data:
  cm_svm_full   <- confusionMatrix(d$exists_blue, predict(svm_fit_full,   d, type='class'), positive="yes");

    # predictor importance (based on sensitivity analysis):
  tmp             <- rminer::Importance(svm_fit_full, d, method="sensv"); 
  pia_svm_full   <- tmp$imp; names(pia_svm_full)   <- names(d); pia_svm_full   <- pia_svm_full[names(pia_svm_full) %in% list_predictors];

  # Save:
  svm_blue$full <- list("success"=data.frame("accuracy"   =cm_svm_full$overall["Accuracy"],
                                                     "sensitivity"=cm_svm_full$byClass["Sensitivity"], 
                                                     "specificity"=cm_svm_full$byClass["Specificity"],
                                                     "precision"  =cm_svm_full$byClass["Precision"],
                                                     "recall"     =cm_svm_full$byClass["Recall"],
                                                     row.names=NULL),   
                                "pred_importance"=data.frame("predictor"  =list_predictors,                # predictors
                                                             "importance" =pia_svm_full[list_predictors], # sensitivity-based
                                                             row.names=NULL));
      
  # Using traning/testing subsets:
  results <- pblapply(1:n_train, function(i)
    {
      # split the dataset into a training (80%) and a testing (20%) set stratified by exists_blue:
      data_train <- training(train_test_splits[[i]]);
      data_test  <- testing(train_test_splits[[i]]);
      # keep only the relevant columns and no NAs:
      data_train <- data_train[ complete.cases(data_train[,c("exists_blue", list_predictors)]), c("exists_blue", list_predictors)];
      data_test  <- data_test [ complete.cases(data_test [,c("exists_blue", list_predictors)]), c("exists_blue", list_predictors)];
      
      # fit the SVMs:
      svm_fit_full   <- rminer::fit(f_full, data_train, model="svm", task="class");

      # confusion matrices on the test data:
      cm_svm_full   <- confusionMatrix(data_test$exists_blue, predict(svm_fit_full,   data_test, type='class'), positive="yes");

      # predictor importance (based on sensitivity analysis):
      tmp             <- rminer::Importance(svm_fit_full, data_train, method="sensv"); 
      pia_svm_full   <- tmp$imp; names(pia_svm_full)   <- names(data_train); pia_svm_full   <- pia_svm_full[names(pia_svm_full) %in% list_predictors];

      # return the results:
      success_full <- data.frame("replication"    =i, # replication
                                 "accuracy"   =cm_svm_full$overall["Accuracy"],
                                 "sensitivity"=cm_svm_full$byClass["Sensitivity"], 
                                 "specificity"=cm_svm_full$byClass["Specificity"],
                                 "precision"  =cm_svm_full$byClass["Precision"],
                                 "recall"     =cm_svm_full$byClass["Recall"],
                                 row.names=NULL);
      pi_df_full   <- data.frame("replication"=i,                                   # replication
                                 "predictor"  =list_predictors,                # predictors
                                 "importance" =pia_svm_full[list_predictors], # sensitivity-based
                                 row.names=NULL);

      return (list("replication"       =i,
                   "data_train_indices"=train_test_splits[[i]]$in_id, 
                   "data_test_indices" =setdiff(1:nrow(train_test_splits[[i]]$data), train_test_splits[[i]]$in_id),
                   "full"         =list("success"=success_full,   "pred_importance"=pi_df_full)));
    });
  
  # Assemble the various results and save them:
  svm_blue$training_testing <- list("replications"   =n_train,
                                    "full"   =list("success"=do.call(rbind, lapply(results, function(x) x$full$success)), "pred_importance"=do.call(rbind, lapply(results, function(x) x$full$pred_importance))));
  save(svm_blue, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
} 
```

On the full data, SVMs are quite successful at predicting *blue*:

```{r}
kable((tmp_all <- data.frame("Predictors" =c("All predictors"),
                 rbind(100*svm_blue$full$success))),
      row.names=FALSE, digits=c(NA, 1, 1, 1, 1, 1), 
      caption=capTab("Success (as %) on the full dataset using SVMs with all potential predictors."));
```

The predictors ordered by their importance (only those with importance > 0):

```{r results='asis'}
tmp <- svm_blue$full$pred_importance[ order(svm_blue$full$pred_importance$importance, decreasing=TRUE), ];
tmp <- tmp[ tmp$importance > 0, ];
cat(paste0("- with all predictors: ", paste0("*",tmp$predictor,"*", " (", round(tmp$importance,2), ")", collapse=", ")));

```


When using `r svm_blue$training_testing$replications` random training/testing subsets:

```{r}
tmp_full   <- vapply(svm_blue$training_testing$full$success[,-1],    function(x) sprintf("%.1f%% ±%.1f%%", mean(x,na.rm=TRUE)*100, sd(x,na.rm=TRUE)*100), character(1));
tmp <- data.frame("Predictors" =rep(c("All predictors")),
                  "accuracy"   =c(tmp_full[ grep("accuracy",    names(tmp_full), fixed=TRUE) ]),
                  "sensitivity"=c(tmp_full[ grep("sensitivity", names(tmp_full), fixed=TRUE) ]),
                  "specificity"=c(tmp_full[ grep("specificity", names(tmp_full), fixed=TRUE) ]),
                  "precision"  =c(tmp_full[ grep("precision",   names(tmp_full), fixed=TRUE) ]),
                  "recall"     =c(tmp_full[ grep("recall",      names(tmp_full), fixed=TRUE) ]));
kable(tmp, row.names=FALSE, col.names=c("Predictors", "Accuracy", "Sensitivity", "Specificity", "Precision", "Recall"),
      caption=capTab(paste0("Various measures of success at predicting *blue* using SVMs with ",svm_blue$training_testing$replications," training/testing sets.")));
```

```{r fig.cap=capFig(paste0("Various measures of success using SVMs with ",svm_blue$replications," training/testing sets. Boxplots show the spread of the training/testing values, while the solid horizontal red lines show the values when using the full dataset.")), fig.width=1*6, fig.height=5}
ggplot(svm_blue$training_testing$full$success %>% 
         reshape2::melt(id.vars=c("replication"), variable.name="measure", value.name="value") %>% 
         mutate("measure"=factor(measure, levels=c("accuracy", "sensitivity", "specificity", "precision", "recall")), "value"=value*100), 
       aes(x=measure, y=value)) + 
  ylim(c(0,100)) + xlab("Measures of success") + ylab("Value (%)") +
  theme_bw() +
  geom_boxplot(show.legend=FALSE) + 
  # draw alternating vertical bands:
  new_scale_fill() + 
  scale_fill_manual("", values=c("white", "gray95")) +
  geom_rect(aes(xmin=as.numeric(measure)-0.5, xmax=as.numeric(measure)+0.5, ymin=0, ymax=100, fill=as.factor(as.numeric(measure) %% 2 == 0)), show.legend=FALSE) + 
  # draw the actual boxplots:
  new_scale_fill() + 
  geom_boxplot(aes(x=measure, y=value)) + 
  # measures on the full dataset:
  geom_rect(data=tmp_all %>%
               reshape2::melt(measure.vars=c("accuracy", "sensitivity", "specificity", "precision", "recall"), variable.name="measure", value.name="value") %>% 
               mutate("measure"=factor(measure, 
                                       levels=c("accuracy", "sensitivity", "specificity", "precision", "recall"))), 
             aes(xmin=as.numeric(measure)-0.5, xmax=as.numeric(measure)+0.5, ymin=value, ymax=value), color="red", fill=NA) +  theme(legend.position="right");
```

The importance of the predictors is:

```{r fig.cap=capFig(paste0("Specificity-based predictor importance from SVMs using ",svm_blue$training_testing$replications," training/testing sets.")), fig.width=4*1, fig.height=7*1}
grid.arrange(ggplot(svm_blue$training_testing$full$pred_importance, 
                    aes(x=reorder(predictor, importance, median), y=importance)) +
               theme_bw() +
               geom_boxplot(fill="gold", color="black") +
               labs(x="Predictors (sorted by decreasing importance)", y="Importance") +
               theme(axis.text.y = element_text(angle=30, hjust=1, size=10), 
                     axis.text.x = element_text(hjust=1, size=10),
                     axis.title=element_text(size=16),
                     legend.position="none") +
               ggtitle("Sorted by importance") + 
               coord_flip() + 
               NULL,
             ncol=1);
```

It can be seen that:

a) *blue* is relatively well predicted;
b) here too, *population size* is the most important predictor, but not exceedingly better than the next ones;
c) the next best predictors are *humidity*,  *distance to oceans for the families*, *climate PC1*, *PC2* and *PC3*, *distance to lakes* and *UV-B incidence*.




#### Interpretation

On the full dataset:

- all techniques predict *blue* very well from the available information, with very little difference between them
- consistently, the most relevant predictors are: *population size*, *UV-B*, *climate*/*humidity*, *latitude* and *distance to large bodies of standing water* (mostly lakes but also sometimes seas/oceans).

In what concerns generalizability (either when explicitly repeatedly splitting the data into training/testing subsets or internally using random forests):

- all techniques do pretty well and with comparable performance (as expected, lower than when using the full dataset for training and testing),
- the best are the conditional random forests and conditional trees, followed by Bayesian regression and SVMs.

Thus, the presence of a dedicated word for 'blue' is partly explained by population size and a few environmental variables concerning UV-B radiation, climate and distance to large bodies of standing water.


### Conclusions: UV-B does influence the color vocabulary

Using various types of methods, we do find, as predicted by  @lindsey_color_2002 and @brown_color_2004 that the presence of a dedicated word for 'blue' is influenced negatively by the amount of UV-B incident radiation (i.e., that higher levels of UV-B radiation reduce the probability that a language has such a word).
Interestingly, different techniques vary in their support for an influence of measures of "cultural complexity" (such as subsistence strategy), but there is at least a hint of a positive relationship. The mediation analysis and path analysis suggest that the effect of subsistence is mediated by population size.
Likewise, climate, ecology and humidity might affect the presence of a dedicated word for 'blue', with drier, higher seasonality increasing the probability of 'blue'.
Finally, there is an a priori surprising negative effect of distance to large bodies of standing water (lakes, in particular).


## Hypothesis 2: UV incidence and abnormal color perception

Here we test the second hypothesis [@brown_color_2004], linking UV incidence and the population frequency of abnormal color perception:

```{r fig.cap=capFig("Graphical representation of Hypothesis 2.")}
DiagrammeR::grViz(paste0('
  digraph hypothesis_2 {

  # the graph:
  graph [overlap = true]
  rankdir="LR";

  # the nodes:
  node [shape = box, style = "filled", fillcolor = "gray90"];
  UV   [label = "UV incidence"]; 
  Gen  [label = "genetics of color perception"];
  Perc [label = "physiology of color perception"]; 
  Blue [label = "word for blue", fillcolor = "white", color="gray50", fontcolor = "gray50"]; 

  # the edges:
  edge       [style = "solid", color = "black"];
  UV   -> Perc ;
  Gen  -> Perc [label = "development", color = "red", fontcolor = "red"] ;
  Perc -> Gen  [style = "dashed", label = "evolution", color = "blue", fontcolor = "blue"];
  Perc -> Blue [color = "gray50", fontcolor = "gray50", label = "(H1)"] ;
}
'))
```

With our variables (and strictly following the hypothesis), this becomes:

```{r fig.cap=capFig("Graphical representation of Hypothesis 2 with our measures as a SEM diagram.")}
DiagrammeR::grViz(paste0('
  digraph hypothesis_2_sem {

  # the graph:
  graph [overlap = true]
  rankdir="LR";

  # the nodes:
  node [shape = box, style = "filled", fillcolor = "lightyellow"];
  dalt [label = "daltonism", tootlip = "daltonism" ];
  rgap [label = "red/green abn. perc.", shape = "ellipse", fillcolor = "gray95", fontcolor="gray50", color="gray50" ];
  blap [label = "blue abn. perc.",      shape = "ellipse", fillcolor = "gray95", fontcolor="gray50", color="gray50" ];
  UVB  [label = "UV-B", tooltip = "UV-B"]; 
  blue [label = "blue", tooltip = "blue"]; 
  lat  [label = "latitude", tooltip = "latitude"]; 

  # the edges:
  edge [style = "solid", color = "black"];
  dalt -> rgap ;
  UVB  -> blap ;
  blap -> blue ;
  lat  -> UVB  ;
  blap -> dalt ;
}
'))
```

However, we do not have enough data to model the two latent variables, so our graph reduces to the following path analysis:

```{r fig.cap=capFig("Graphical representation of Hypothesis 2 with our measures as a path analysis diagram.")}
DiagrammeR::grViz(paste0('
  digraph hypothesis_2_pa {

  # the graph:
  graph [overlap = true]
  rankdir="LR";

  # the nodes:
  node [shape = box, style = "filled", fillcolor = "lightyellow"];
  dalt [label = "daltonism", tootlip = "daltonism" ];
  UVB  [label = "UV-B", tooltip = "UV-B"]; 
  blue [label = "blue", tooltip = "blue"]; 
  lat  [label = "latitude", tooltip = "latitude"]; 

  # the edges:
  edge [style = "solid", color = "black"];
  UVB  -> dalt ;
  UVB  -> blue ;
  lat  -> UVB  ;
  blue -> dalt ;
}
'))
```



### Variation between families and macroareas

First, we need to investigate the variation between families and macroareas, as we want to model them as random effects in our models.

```{r include=FALSE}
# Null model (intercept) -> daltonism:
if( !all(file.exists("./cached_results/b_0__daltonism.RData")) )
{
  # check the random effects:
  # ICCs:
  m_0__daltonism <- lmer(daltonism_r ~ 1 + (1 | glottocode_family) + (1 | macroarea), data=d_colors); # model as normal for now
  summary(m_0__daltonism);
  icc_fm <- performance::icc(m_0__daltonism);
  # - family:
  m_0__daltonism_family <- update(m_0__daltonism, . ~ . - (1 | glottocode_family)); 
  summary(m_0__daltonism_family);
  icc_m <- performance::icc(m_0__daltonism_family);
  anova(m_0__daltonism, m_0__daltonism_family);
  # - macroarea:
  m_0__daltonism_macroarea <- update(m_0__daltonism, . ~ . - (1 | macroarea)); 
  summary(m_0__daltonism_macroarea);
  icc_f <- performance::icc(m_0__daltonism_macroarea);
  anova(m_0__daltonism, m_0__daltonism_macroarea);
  
  # brms:
  # 1. check beta vs gaussian distributions:
  b_0__daltonism_gaussian <- brm(daltonism_r ~ 1 + (1 | glottocode_family) + (1 | macroarea), # null model for daltonism_r
                                 family=gaussian(), data=d_colors, 
                                 prior=c(prior(normal(0, 5), class="Intercept")), # pretty wide priors centered on 0
                                 save_all_pars=TRUE, # needed for Bayes factors
                                 sample_prior=TRUE,  # needed for hypotheses tests
                                 cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10));
  b_0__daltonism_gaussian <- brms_fit_indices(b_0__daltonism_gaussian);
  summary(b_0__daltonism_gaussian); mcmc_plot(b_0__daltonism_gaussian, type="trace"); mcmc_plot(b_0__daltonism_gaussian);
  
  b_0__daltonism_beta <- brm(daltonism_r ~ 1 + (1 | glottocode_family) + (1 | macroarea), # null model for daltonism_r
                             family=Beta(), data=d_colors, 
                             prior=c(prior(normal(0, 5), class="Intercept")), # pretty wide priors centered on 0
                             save_all_pars=TRUE, # needed for Bayes factors
                             sample_prior=TRUE,  # needed for hypotheses tests
                             cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10));
  b_0__daltonism_beta <- brms_fit_indices(b_0__daltonism_beta);
  summary(b_0__daltonism_beta); mcmc_plot(b_0__daltonism_beta, type="trace"); mcmc_plot(b_0__daltonism_beta);
  
  compare_0__daltonism__gaussian_beta <- brms_compare_models(b_0__daltonism_gaussian, b_0__daltonism_beta, "gaussian", "beta"); # beta fits better
  b_0__daltonism <- b_0__daltonism_beta;
  
  # 2. check random effects:
  # - family:
  b_0__daltonism_family <- update(b_0__daltonism, . ~ . - (1 | glottocode_family), 
                                  cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10)); 
  b_0__daltonism_family <- brms_fit_indices(b_0__daltonism_family);
  summary(b_0__daltonism_family); mcmc_plot(b_0__daltonism_family, type="trace"); mcmc_plot(b_0__daltonism_family);
  compare_0_family__daltonism <- brms_compare_models(b_0__daltonism, b_0__daltonism_family, "both raneff", "macroarea only"); # the are equivalent
  # - macroarea:
  b_0__daltonism_macroarea <- update(b_0__daltonism, . ~ . - (1 | macroarea), 
                                     cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10)); 
  b_0__daltonism_macroarea <- brms_fit_indices(b_0__daltonism_macroarea);
  summary(b_0__daltonism_macroarea); mcmc_plot(b_0__daltonism_macroarea, type="trace"); mcmc_plot(b_0__daltonism_macroarea);
  compare_0_macroarea__daltonism <- brms_compare_models(b_0__daltonism, b_0__daltonism_macroarea, "both raneff", "family only"); # family one might be marginally better?
  # -> let's keep both...
  
  # Save results:
  save(b_0__daltonism, 
       icc_fm, icc_f, icc_m,
       file="./cached_results/b_0__daltonism.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_0__daltonism.RData");
}
```

The ICC of including separately family is `r round(100*icc_f$ICC_adjusted,1)`%, and of macroarea is `r round(100*icc_m$ICC_adjusted,1)`%; thus, we will include both in our models.





### Potential predictors

Here we look at the potential predictors individually.


#### 'blue' &rarr; daltonism

```{r fig.cap=capFig("Population frequency of daltonism (in %) function of having a specific word for 'blue', showing the jittered data points, the densities (colored violins) and boxplots (black)."), fig.height=1*5, fig.width=1*4}
ggplot(d_colors, aes(y=daltonism, x=exists_blue, fill=exists_blue)) + 
  geom_jitter(alpha=0.50, aes(color=exists_blue)) + geom_violin(alpha=0.25, aes(color=exists_blue)) + geom_boxplot(fill=alpha("gray80",0.30)) + 
  theme_bw() +
  labs(x='Word for blue?', y="Population frequency of datonism (%)") +
  theme(axis.text.x = element_text(size =14, face = "bold"), 
        axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
        axis.title=element_text(size=14),
        legend.title=element_text(size =16),
        legend.text=element_text(size =16),
        legend.position="bottom") +
  scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + scale_color_manual('Word for blue?', values=c("yes"="blue", "no"="green")) + 
  NULL;
```

```{r include=FALSE}
# word for blue -> daltonism:
if( !all(file.exists("./cached_results/b_blue__daltonism.RData")) )
{
  # Fit the model:
  b_blue__daltonism <- brm(daltonism_r ~ 1 + exists_blue  + (1 | glottocode_family) + (1 | macroarea), # exists_blue -> daltonism_r
                             family=Beta(), data=d_colors, 
                             prior=c(prior(normal(0, 5), class="Intercept"),
                                     prior(normal(0, 5), class="b")), # pretty wide priors centered on 0
                             save_all_pars=TRUE, # needed for Bayes factors
                             sample_prior=TRUE,  # needed for hypotheses tests
                             cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10));
  b_blue__daltonism <- brms_fit_indices(b_blue__daltonism);
  summary(b_blue__daltonism); mcmc_plot(b_blue__daltonism, type="trace"); mcmc_plot(b_blue__daltonism);
  (h_blue__daltonism <- brms::hypothesis(b_blue__daltonism, c("exists_blueyes = 0", "exists_blueyes > 0")));
  (hdi_blue__daltonism <- hdi(b_blue__daltonism, ci=0.95));
  compare_0_blue__daltonism <- brms_compare_models(b_0__daltonism, b_blue__daltonism, "null", "+ blue"); # clear positive effect of exists_blue!
  
  # Save results:
  save(compare_0_blue__daltonism, h_blue__daltonism, hdi_blue__daltonism, #b_blue__daltonism, # no need to save the actual model
       file="./cached_results/b_blue__daltonism.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_blue__daltonism.RData");
}
```

The existence of a dedicated word for 'blue' (*exists_blue*) has a significant positive influence on *daltonism*: *&beta;~blue:yes-no~* = `r round(h_blue__daltonism$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_blue__daltonism$CI_low[ hdi_blue__daltonism$Parameter == "b_exists_blueyes" ], hdi_blue__daltonism$CI_high[ hdi_blue__daltonism$Parameter == "b_exists_blueyes" ])`), *p*(*&beta;*=0) = `r sprintf("%.2g",h_blue__daltonism$hypothesis$Post.Prob[1])`, *p*(*&beta;*>0) = `r sprintf("%.2g",h_blue__daltonism$hypothesis$Post.Prob[2])`); and fits the data much better than the null model (Bayes factor = `r sprintf("%.2g",compare_0_blue__daltonism$BF)`, LOO = `r sprintf("%.2f [SE=%.2f]", compare_0_blue__daltonism$LOO["null", "elpd_diff"], compare_0_blue__daltonism$LOO["null", "se_diff"])`, WAIC = `r sprintf("%.2f [SE=%.2f]", compare_0_blue__daltonism$WAIC["null", "elpd_diff"], compare_0_blue__daltonism$WAIC["null", "se_diff"])`, K-fold = `r sprintf("%.2f [SE=%.2f]", compare_0_blue__daltonism$KFOLD["null", "elpd_diff"], compare_0_blue__daltonism$KFOLD["null", "se_diff"])`).


#### daltonism &rarr; 'blue'

```{r include=FALSE}
# daltonism -> word for blue:
if( !all(file.exists("./cached_results/b_daltonism__blue.RData")) )
{
  # Fit the model:
  b_daltonism__blue <- brm(exists_blue ~ 1 + daltonism_r + I(daltonism_r^2)  + (1 | glottocode_family) + (1 | macroarea), # daltonism_r -> exists_blue
                           family=bernoulli(link="logit"), data=d_colors, 
                           prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                   prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                           save_all_pars=TRUE, # needed for Bayes factors
                           sample_prior=TRUE,  # needed for hypotheses tests
                           cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10));
  b_daltonism__blue <- brms_fit_indices(b_daltonism__blue);
  summary(b_daltonism__blue); mcmc_plot(b_daltonism__blue, type="trace"); mcmc_plot(b_daltonism__blue);
  (h_daltonism__blue <- brms::hypothesis(b_daltonism__blue, c("daltonism_r = 0", "Idaltonism_rE2 = 0")));
  (hdi_daltonism__blue <- hdi(b_daltonism__blue, ci=0.95));
  compare_0_daltonism__blue <- brms_compare_models(b_0__blue, b_daltonism__blue, "null", "+ daltonism"); # daltonism has an effect
  
  # Try to remove the quadratic effect:
  b_daltonism__blue_1 <- update(b_daltonism__blue, . ~ . - I(daltonism_r^2), 
                                   save_all_pars=TRUE, sample_prior=TRUE, cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10));
  b_daltonism__blue_1 <- brms_fit_indices(b_daltonism__blue_1);
  summary(b_daltonism__blue_1); mcmc_plot(b_daltonism__blue_1, type="trace"); mcmc_plot(b_daltonism__blue_1);
  (h_daltonism__blue_1 <- brms::hypothesis(b_daltonism__blue_1, c("daltonism_r = 0", "daltonism_r > 0")));
  (hdi_daltonism__blue_1 <- hdi(b_daltonism__blue_1, ci=0.95));
  compare_daltonism__blue_1 <- brms_compare_models(b_daltonism__blue, b_daltonism__blue_1, "full", "- qudratic"); # quadratic and linear seem equivalent
  b_daltonism__blue <- b_daltonism__blue_1; h_daltonism__blue <- h_daltonism__blue_1; hdi_daltonism__blue <- hdi_daltonism__blue_1; # keep the linear only
  
  # Compare with null:
  compare_0_daltonism__blue <- brms_compare_models(b_0__blue, b_daltonism__blue, "null", "+ daltonism"); # daltonism has a positive effect...
  
  # Save results:
  save(compare_0_daltonism__blue, h_daltonism__blue, hdi_daltonism__blue, #b_daltonism__blue, # no need to save the actual model
       file="./cached_results/b_daltonism__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_daltonism__blue.RData");
}
```

Conversely, *daltonism* has a positive effect on the existence of a dedicated word for 'blue' (*exists_blue*): *&beta;~daltonism~* = `r round(h_daltonism__blue$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_daltonism__blue$CI_low[ hdi_daltonism__blue$Parameter == "b_daltonism_r" ], hdi_daltonism__blue$CI_high[ hdi_daltonism__blue$Parameter == "b_daltonism_r" ])`), *p*(*&beta;*=0) = `r sprintf("%.2g",h_daltonism__blue$hypothesis$Post.Prob[1])`, *p*(*&beta;*>0) = `r sprintf("%.2g",h_daltonism__blue$hypothesis$Post.Prob[2])`); and fits the data better than the null model (Bayes factor = `r sprintf("%.2g",compare_0_daltonism__blue$BF)`, LOO = `r sprintf("%.2f [SE=%.2f]", compare_0_daltonism__blue$LOO["null", "elpd_diff"], compare_0_daltonism__blue$LOO["null", "se_diff"])`, WAIC = `r sprintf("%.2f [SE=%.2f]", compare_0_daltonism__blue$WAIC["null", "elpd_diff"], compare_0_daltonism__blue$WAIC["null", "se_diff"])`, K-fold = `r sprintf("%.2f [SE=%.2f]", compare_0_daltonism__blue$KFOLD["null", "elpd_diff"], compare_0_daltonism__blue$KFOLD["null", "se_diff"])`).


#### UV-B &rarr; daltonism

```{r fig.cap=capFig('Population frequency of daltonism (in %) function of UV-B incidence, showing the data points and the linear and LOESS trends (with 95% CIs).'), fig.height=1*5, fig.width=1*5}
ggplot(d_colors, aes(y=daltonism, x=UVB)) + 
  geom_point(alpha=0.35) + 
  geom_smooth(method="lm", color="blue") + geom_smooth(method="loess", color="red") + 
  theme_bw() +
  labs(x=expression("UV-B incidence (J/m"^"2"~")"), y="Population frequency of datonism (%)") +
  theme(axis.text.x = element_text(size =14, face = "bold"), 
        axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
        axis.title=element_text(size=14),
        legend.title=element_text(size =16),
        legend.text=element_text(size =16),
        legend.position="bottom") + 
  NULL;
```

```{r include=FALSE}
# UVB -> daltonism:
if( !all(file.exists("./cached_results/b_uvb__daltonism.RData")) )
{
  # Fit the model:
  b_uvb__daltonism <- brm(daltonism_r ~ 1 + UVB_r + I(UVB_r^2)  + (1 | glottocode_family) + (1 | macroarea), # UVB -> daltonism_r
                          family=Beta(), data=d_colors, 
                          prior=c(prior(normal(0, 5), class="Intercept"),
                                  prior(normal(0, 5), class="b")), # pretty wide priors centered on 0
                          save_all_pars=TRUE, # needed for Bayes factors
                          sample_prior=TRUE,  # needed for hypotheses tests
                          cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10));
  b_uvb__daltonism <- brms_fit_indices(b_uvb__daltonism);
  summary(b_uvb__daltonism); mcmc_plot(b_uvb__daltonism, type="trace"); mcmc_plot(b_uvb__daltonism);
  (h_uvb__daltonism <- brms::hypothesis(b_uvb__daltonism, c("UVB_r = 0", "UVB_r < 0", "IUVB_rE2 = 0")));
  (hdi_uvb__daltonism <- hdi(b_uvb__daltonism, ci=0.95));
  
  # Try to remove the quadratic effect:
  b_uvb__daltonism_1 <- update(b_uvb__daltonism, . ~ . - I(UVB_r^2), 
                               save_all_pars=TRUE, sample_prior=TRUE, cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10));
  b_uvb__daltonism_1 <- brms_fit_indices(b_uvb__daltonism_1);
  summary(b_uvb__daltonism_1); mcmc_plot(b_uvb__daltonism_1, type="trace"); mcmc_plot(b_uvb__daltonism_1);
  (h_uvb__daltonism_1 <- brms::hypothesis(b_uvb__daltonism_1, c("UVB_r = 0", "UVB_r < 0")));
  (hdi_uvb__daltonism_1 <- hdi(b_uvb__daltonism_1, ci=0.95));
  compare_uvb__daltonism_1 <- brms_compare_models(b_uvb__daltonism, b_uvb__daltonism_1, "full", "- qudratic"); # quadratic effect does not matter
  b_uvb__daltonism <- b_uvb__daltonism_1; h_uvb__daltonism <- h_uvb__daltonism_1; hdi_uvb__daltonism <- hdi_uvb__daltonism_1; # keep the linear only
  
  # Compare with null:
  compare_0_uvb__daltonism <- brms_compare_models(b_0__daltonism, b_uvb__daltonism, "null", "+ UVB"); # clear negative effect of UVB!
  
  # Save results:
  save(compare_0_uvb__daltonism, h_uvb__daltonism, hdi_uvb__daltonism, #b_uvb__daltonism, # no need to save the actual model
       file="./cached_results/b_uvb__daltonism.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_uvb__daltonism.RData");
}
```

UV-B incidence has a significant negative linear effect on *daltonism* (the quadratic effect is clearly not important): *&beta;~UVB~* = `r round(h_uvb__daltonism$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_uvb__daltonism$CI_low[ hdi_uvb__daltonism$Parameter == "b_UVB_r" ], hdi_uvb__daltonism$CI_high[ hdi_uvb__daltonism$Parameter == "b_UVB_r" ])`), *p*(*&beta;*=0) = `r sprintf("%.2g",h_uvb__daltonism$hypothesis$Post.Prob[1])`, *p*(*&beta;*<0) = `r sprintf("%.2g",h_uvb__daltonism$hypothesis$Post.Prob[2])`); and fits the data much better than the null model (Bayes factor = `r sprintf("%.2g",compare_0_uvb__daltonism$BF)`, LOO = `r sprintf("%.2f [SE=%.2f]", compare_0_uvb__daltonism$LOO["null", "elpd_diff"], compare_0_uvb__daltonism$LOO["null", "se_diff"])`, WAIC = `r sprintf("%.2f [SE=%.2f]", compare_0_uvb__daltonism$WAIC["null", "elpd_diff"], compare_0_uvb__daltonism$WAIC["null", "se_diff"])`, K-fold = `r sprintf("%.2f [SE=%.2f]", compare_0_uvb__daltonism$KFOLD["null", "elpd_diff"], compare_0_uvb__daltonism$KFOLD["null", "se_diff"])`).


#### Latitude &rarr; daltonism

```{r fig.cap=capFig('Population frequency of daltonism (in %) function of UV-B incidence, showing the data points and the linear and LOESS trends (with 95% CIs).'), fig.height=1*5, fig.width=1*5}
ggplot(d_colors, aes(y=daltonism, x=latitude)) + 
  geom_point(alpha=0.35) + 
  geom_smooth(method="lm", color="blue") + geom_smooth(method="loess", color="red") + 
  theme_bw() +
  labs(x=expression("Latitude"), y="Population frequency of datonism (%)") +
  theme(axis.text.x = element_text(size =14, face = "bold"), 
        axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
        axis.title=element_text(size=14),
        legend.title=element_text(size =16),
        legend.text=element_text(size =16),
        legend.position="bottom") + 
  NULL;
```

```{r include=FALSE}
# UVB -> daltonism:
if( !all(file.exists("./cached_results/b_lat__daltonism.RData")) )
{
  # Fit the model:
  b_lat__daltonism <- brm(daltonism_r ~ 1 + latitude_r + I(latitude_r^2)  + (1 | glottocode_family) + (1 | macroarea), # latitude -> daltonism_r
                          family=Beta(), data=d_colors, 
                          prior=c(prior(normal(0, 5), class="Intercept"),
                                  prior(normal(0, 5), class="b")), # pretty wide priors centered on 0
                          save_all_pars=TRUE, # needed for Bayes factors
                          sample_prior=TRUE,  # needed for hypotheses tests
                          cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10));
  b_lat__daltonism <- brms_fit_indices(b_lat__daltonism);
  summary(b_lat__daltonism); mcmc_plot(b_lat__daltonism, type="trace"); mcmc_plot(b_lat__daltonism);
  (h_lat__daltonism <- brms::hypothesis(b_lat__daltonism, c("latitude_r = 0", "latitude_r < 0", "Ilatitude_rE2 = 0")));
  (hdi_lat__daltonism <- hdi(b_lat__daltonism, ci=0.95));
  
  # Try to remove the quadratic effect:
  b_lat__daltonism_1 <- update(b_lat__daltonism, . ~ . - I(latitude_r^2), 
                               save_all_pars=TRUE, sample_prior=TRUE, cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10));
  b_lat__daltonism_1 <- brms_fit_indices(b_lat__daltonism_1);
  summary(b_lat__daltonism_1); mcmc_plot(b_lat__daltonism_1, type="trace"); mcmc_plot(b_lat__daltonism_1);
  (h_lat__daltonism_1 <- brms::hypothesis(b_lat__daltonism_1, c("latitude_r = 0", "latitude_r < 0")));
  (hdi_lat__daltonism_1 <- hdi(b_lat__daltonism_1, ci=0.95));
  compare_lat__daltonism_1 <- brms_compare_models(b_lat__daltonism, b_lat__daltonism_1, "full", "- quadratic"); # quadratic effect does not matter
  b_lat__daltonism <- b_lat__daltonism_1; h_lat__daltonism <- h_lat__daltonism_1; hdi_lat__daltonism <- hdi_lat__daltonism_1; # keep the linear only

  
  # Compare with null:
  compare_0_lat__daltonism <- brms_compare_models(b_0__daltonism, b_lat__daltonism, "null", "+ latitude"); # clear negative effect of latitude!
  
  # Save results:
  save(compare_0_lat__daltonism, h_lat__daltonism, hdi_lat__daltonism, #b_lat__daltonism, # no need to save the actual model
       file="./cached_results/b_lat__daltonism.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_lat__daltonism.RData");
}
```

*Latitude*  has a significant positive linear effect on *daltonism* (the quadratic effect is clearly not important): *&beta;~latitude~* = `r round(h_lat__daltonism$hypothesis$Estimate[1],2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_lat__daltonism$CI_low[ hdi_lat__daltonism$Parameter == "b_latitude_r" ], hdi_lat__daltonism$CI_high[ hdi_lat__daltonism$Parameter == "b_latitude_r" ])`), *p*(*&beta;*=0) = `r sprintf("%.2g",h_lat__daltonism$hypothesis$Post.Prob[1])`, *p*(*&beta;*<0) = `r sprintf("%.2g",h_lat__daltonism$hypothesis$Post.Prob[2])`); and fits the data much better than the null model (Bayes factor = `r sprintf("%.2g",compare_0_lat__daltonism$BF)`, LOO = `r sprintf("%.2f [SE=%.2f]", compare_0_lat__daltonism$LOO["null", "elpd_diff"], compare_0_lat__daltonism$LOO["null", "se_diff"])`, WAIC = `r sprintf("%.2f [SE=%.2f]", compare_0_lat__daltonism$WAIC["null", "elpd_diff"], compare_0_lat__daltonism$WAIC["null", "se_diff"])`, K-fold = `r sprintf("%.2f [SE=%.2f]", compare_0_lat__daltonism$KFOLD["null", "elpd_diff"], compare_0_lat__daltonism$KFOLD["null", "se_diff"])`).


#### daltonism and genetic distances

```{r fig.cap=capFig('Population frequency of daltonism (in %) function of genetic distance between populations.'), fig.height=3*4, fig.width=3*5}
grid.arrange(grobs=lapply(grep("gen_D", names(d_colors), fixed=TRUE), function(i)
              {
                d_1 <- d_colors[,c("daltonism", names(d_colors)[i])]; names(d_1) <- c("daltonism", "gen_D");
                ggplot(d_1, aes(x=gen_D, y=daltonism)) + 
                  geom_point(alpha=0.35) + 
                  geom_smooth(method="lm", color="blue") + geom_smooth(method="loess", color="red") + 
                  theme_bw() +
                  labs(y="Datonism (%)", x=names(d_colors)[i]) +
                  theme(axis.text.x = element_text(size =14, face = "bold"), 
                        axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                        axis.title=element_text(size=14),
                        legend.title=element_text(size =16),
                        legend.text=element_text(size =16),
                        legend.position="bottom") + 
                  NULL;
              }),
              ncol=4);
```

```{r include=FALSE}
# genetic distances -> daltonism:
if( !all(file.exists("./cached_results/b_genetics__daltonism.RData")) )
{
  # Fit the model:
  b_genetics__daltonism <- brm(daltonism_r ~ 1 + 
                                 gen_D1 + gen_D2 + gen_D3 + gen_D4 + gen_D5 + gen_D6 + gen_D7 + gen_D8 + gen_D9 + gen_D10 + 
                                 I(gen_D1^2) + I(gen_D2^2) + I(gen_D3^2) + I(gen_D4^2) + I(gen_D5^2) + 
                                 I(gen_D6^2) + I(gen_D7^2) + I(gen_D8^2) + I(gen_D9^2) + I(gen_D10^2) +  # genetics -> daltonism_r
                                 (1 | glottocode_family) + (1 | macroarea),
                               family=Beta(), data=d_colors, 
                               prior=c(prior(normal(0, 5), class="Intercept"),
                                       prior(normal(0, 5), class="b")), # pretty wide priors centered on 0
                               save_all_pars=TRUE, # needed for Bayes factors
                               sample_prior=TRUE,  # needed for hypotheses tests
                               cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10));
  b_genetics__daltonism <- brms_fit_indices(b_genetics__daltonism);
  summary(b_genetics__daltonism); mcmc_plot(b_genetics__daltonism, type="trace"); mcmc_plot(b_genetics__daltonism);
  (hdi_genetics__daltonism <- hdi(b_genetics__daltonism, ci=0.95));
  compare_0_genetics__daltonism <- brms_compare_models(b_0__daltonism, b_genetics__daltonism, "null", "+ genetics"); # not clear if genetics matters...
  # Iterative manual simplification:
  b_genetics__daltonism_1 <- update(b_genetics__daltonism, . ~ . - I(gen_D7^2) - I(gen_D8^2) - I(gen_D9^2) - I(gen_D10^2) - gen_D6 - gen_D9 - gen_D4 - gen_D5 - 
                                      I(gen_D1^2) - I(gen_D3^2) - gen_D7 - I(gen_D4^2) - I(gen_D6^2) - gen_D10 - gen_D3 - gen_D2,
                         save_all_pars=TRUE, sample_prior=TRUE, cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10)); 
  summary(b_genetics__daltonism_1); mcmc_plot(b_genetics__daltonism_1, type="trace"); mcmc_plot(b_genetics__daltonism_1) + xlim(c(-0.5, 0.5)); # phi and intercept are too big
  (hdi_reduced <- hdi(b_genetics__daltonism_1, ci=0.95)); (rope_reduced <- rope(b_genetics__daltonism_1, ci=0.95)); fixef_reduced <- fixef(b_genetics__daltonism_1);
  b_genetics__daltonism_1 <- brms_fit_indices(b_genetics__daltonism_1);
  # Model comparison:
  (cmp_full_reduced <- brms_compare_models(b_genetics__daltonism, b_genetics__daltonism_1, "full", "reduced")); # reduced is better
  (cmp_null_reduced <- brms_compare_models(b_0__daltonism, b_genetics__daltonism_1, "null", "reduced")); # reduced is better
  # Keep the reduced model:
  b_genetics__daltonism <- b_genetics__daltonism_1; compare_0_genetics__daltonism <- cmp_null_reduced; 
  hdi_genetics__daltonism <- hdi_reduced; rope_genetics__daltonism <- rope_reduced; fixef_genetics__daltonism <- fixef_reduced;
  r2_genetics__daltonism <- bayes_R2(b_genetics__daltonism_1);
  
  
  # Mantel correlations:
  library(vegan); # mantel correlations
  # Load the genetic distances with ultrametric imputation:
  d_gen_ult <- read.table("./input_files/distmat_gen_ult.csv", header=TRUE, sep=","); rownames(d_gen_ult) <- d_gen_ult[,1]; d_gen_ult <- d_gen_ult[,-1]; d_gen_ult <- as.matrix(d_gen_ult);
  # Compute the daltonism distances:
  tmp <- matrix(d_colors$daltonism/100, ncol=1); rownames(tmp) <- d_colors$glottocode; d_dalto <- as.matrix(dist(tmp));
  # Make sure the two matrices correspond:
  s <- sort(intersect(rownames(d_gen_ult), rownames(d_dalto)));
  # Compute the correlation:
  (mantel_r_genetics__daltonism   <- vegan::mantel(d_gen_ult[s,s], d_dalto[s,s], method="pearson",  permutations=999)); # r=0.07798, p=0.001
  (mantel_rho_genetics__daltonism <- vegan::mantel(d_gen_ult[s,s], d_dalto[s,s], method="spearman", permutations=999)); # r=0.07694, p=0.001
  
  
  # Save results:
  save(compare_0_genetics__daltonism, hdi_genetics__daltonism, rope_genetics__daltonism, fixef_genetics__daltonism, #b_genetics__daltonism, # no need to save the actual model
       r2_genetics__daltonism,
       mantel_r_genetics__daltonism, mantel_rho_genetics__daltonism,
       file="./cached_results/b_genetics__daltonism.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_genetics__daltonism.RData");
}
```

Interestingly, the population frequency of daltonism is only weakly correlated with the inter-population genetic distances: a Bayesian mixed-effects quadratic Beta regression of daltonism with family and macroarea as random effects and the first 10 MDS dimensions of the ultrametrically-imputed genetic distances matrix with manual simplification is better than the null model (Bayes factor = `r sprintf("%.2g",compare_0_genetics__daltonism$BF)`, LOO = `r sprintf("%.2f [SE=%.2f]", compare_0_genetics__daltonism$LOO["null", "elpd_diff"], compare_0_genetics__daltonism$LOO["null", "se_diff"])`, WAIC = `r sprintf("%.2f [SE=%.2f]", compare_0_genetics__daltonism$WAIC["null", "elpd_diff"], compare_0_genetics__daltonism$WAIC["null", "se_diff"])`, K-fold = `r sprintf("%.2f [SE=%.2f]", compare_0_genetics__daltonism$KFOLD["null", "elpd_diff"], compare_0_genetics__daltonism$KFOLD["null", "se_diff"])`), has `r sprintf("R^2^= %.1f%% ±%.1f%%", r2_genetics__daltonism["R2", "Estimate"]*100, r2_genetics__daltonism["R2", "Est.Error"]*100)`, and the retained predictors are:

```{r}
kable(data.frame("predictor"  =rownames(fixef_genetics__daltonism), 
                 "estimate"   =round(fixef_genetics__daltonism[,"Estimate"],2),
                 "hdi"=paste0("[", round(hdi_genetics__daltonism[,"CI_low"],2), ",", round(hdi_genetics__daltonism[,"CI_high"],2), "]"),
                 "p.rope"=sprintf("%.2g",rope_genetics__daltonism[,"ROPE_Percentage"]),
                 row.names=NULL), 
      row.names=FALSE, col.names=c("Predictor", "*&beta;* (estimate)", "*&beta;* (95% HDI)", "*p*~ROPE~"),
      caption=capTab(paste0("Retained predictors ; the ROPE is [", round(rope_genetics__daltonism[1,"ROPE_low"],2), ",", round(rope_genetics__daltonism[1,"ROPE_high"],2), "].")));
```

Likewise, the Mantel correlations between the Euclidean distances between population frequencies in daltonism and ultrametrically-imputed genetic distances matrix are weak but significant (using 1000 permutations): with Pearson `r sprintf("*r*=%.3f, *p*=%.4g", mantel_r_genetics__daltonism$statistic, mantel_r_genetics__daltonism$signif)`, and with Spearman `r sprintf("*&rho;*=%.3f, *p*=%.4g", mantel_rho_genetics__daltonism$statistic, mantel_rho_genetics__daltonism$signif)`.


### Mediation analyses

#### UV-B &rarr; 'blue' &rarr; daltonism

```{r include=FALSE}
file_name <- "./cached_results/bmed_uvb_blue__daltonism.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_uvb_blue__daltonism <- .fit_mediation_model(d=d_colors, 
                                                   outcome="daltonism_r", outcome_name="daltonism", 
                                                   treatment="UVB_r", treatment_name="UV-B", 
                                                   mediator="exists_blue", mediator_name="blue", 
                                                   family_mediator=bernoulli("logit"), family_outcome=Beta(), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                                   cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                                   save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_uvb_blue__daltonism, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the mediator is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_uvb_blue__daltonism; .plot_mediation_model(m$summary, m$mediation, 
                                                     outcome=m$outcome, outcome_name=m$outcome_name, 
                                                     treatment=m$treatment, treatment_name=m$treatment_name, 
                                                     mediator=m$mediator, mediator_name=m$mediator_name);
```

Thus, as suggested, *daltonism* is negatively affected by *UV-B* incidence (total effect) both directly and indirectly, mediated by *blue*; in particular, the frequency of daltonism is higher is populations whose language has a dedicated word for 'blue'.

#### latitude &rarr; UV-B &rarr; daltonism

```{r include=FALSE}
file_name <- "./cached_results/bmed_lat_uvb__daltonism.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_lat_uvb__daltonism <- .fit_mediation_model(d=d_colors, 
                                                   outcome="daltonism_r", outcome_name="daltonism", 
                                                   treatment="latitude_r", treatment_name="Latitude", 
                                                   mediator="UVB_r", mediator_name="UV-B", 
                                                   family_mediator=gaussian(), family_outcome=Beta(), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                                   cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                                   save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_lat_uvb__daltonism, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the mediator is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_lat_uvb__daltonism; .plot_mediation_model(m$summary, m$mediation, 
                                                     outcome=m$outcome, outcome_name=m$outcome_name, 
                                                     treatment=m$treatment, treatment_name=m$treatment_name, 
                                                     mediator=m$mediator, mediator_name=m$mediator_name);
```

#### latitude &rarr; 'blue' &rarr; daltonism

```{r include=FALSE}
file_name <- "./cached_results/bmed_lat_blue__daltonism.RData";
if( !all(file.exists(file_name)) )
{
  # Fit the mediation model:
  bmed_lat_blue__daltonism <- .fit_mediation_model(d=d_colors, 
                                                   outcome="daltonism_r", outcome_name="daltonism", 
                                                   treatment="latitude_r", treatment_name="Latitude", 
                                                   mediator="exists_blue", mediator_name="blue", 
                                                   family_mediator=bernoulli(link="logit"), family_outcome=Beta(), ranefs="(1 | glottocode_family) + (1 | macroarea)",
                                                   cores=brms_ncores, iter=6000, warmup=2000, thin=2, control=list(adapt_delta=0.999, max_treedepth=10),
                                                   save_model=FALSE, show_results=TRUE);
  # Save results:
  save(bmed_lat_blue__daltonism, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
}
```

```{r fig.cap=capFig('Mediation analysis showing the total, direct and indirect effects, as well as the actual regression coefficients. Please note that because the mediator is binary, the direct and indirect effects may be on different scales.'), fig.width=10, fig.height=5}
m <- bmed_lat_blue__daltonism; .plot_mediation_model(m$summary, m$mediation, 
                                                     outcome=m$outcome, outcome_name=m$outcome_name, 
                                                     treatment=m$treatment, treatment_name=m$treatment_name, 
                                                     mediator=m$mediator, mediator_name=m$mediator_name);
```

#### Interpretation 

```{r fig.cap=capFig("Graphical representation of the links supported by the mediation and regression analyses. Edges: solid blue = negative effects, solid red = positive effects, and dashed gray = non-significant effects; the dashed red edge is borderline significant. Nodes: 'blue' in blue is the outcome, 'UV-B' in green is of particular interest here. This should *not* be interpreted as a path diagram!"), fig.width=7, fig.height=4}

DiagrammeR::grViz(paste0('
  digraph hypothesis_2_full_sem {

  # the graph:
  graph [overlap = true]
  rankdir="LR";

  # the nodes:
  node [shape = box, style = "filled", fillcolor = "lightyellow"];
  dalt [label = "daltonism", tootlip = "daltonism" , fillcolor="deepskyblue"];
  UVB  [label = "UV-B", tooltip = "UV-B", fillcolor="seagreen1"]; 
  blue [label = "blue", tooltip = "blue"]; 
  lat  [label = "latitude", tooltip = "latitude"]; 

  # the edges:
  edge [style = "solid", color = "black"];
  #lat -> dalt [label="-", color="blue", fontcolor="blue"] ;
  lat -> dalt [label="+", color="red", fontcolor="red"];
  UVB  -> dalt [label="-", color="blue", fontcolor="blue"];
  blue -> dalt [label="+", color="red", fontcolor="red"];
  UVB  -> blue [label="-", color="blue", fontcolor="blue"];
  lat -> blue [label="+", color="red", fontcolor="red"];
  #lat -> UVB [label="+", color="red", fontcolor="red"];
  lat -> UVB [label="-", color="blue", fontcolor="blue"];
}
'))
```

The mediation analysis for the hypothesis 1 also showed that distance to lakes, population size, climate and subsistence could potentially be included in this diagram for their effects on blue. However, as they do not affect *daltonism* here, we did not include them in the diagram. 

### Path analysis

```{r include=FALSE}
file_name <- "./cached_results/sem_daltonism.RData";
if( !all(file.exists(file_name)) )
{
  # The data:
  d_colors_lavaan <- d_colors[,c("glottocode", "glottocode_family", "macroarea", 
                                 "exists_blue", "UVB_r", "latitude_r", "dist2lakes_r", "clim_PC1_r", "subsistence", "log_popSize", "daltonism_r")];
  names(d_colors_lavaan) <- c("glottocode", "glottocode_family", "macroarea", 
                                 "blue", "uvb", "lat", "d2l", "clim", "subs", "psiz", "dalt" );
  # Binary variables recoding (as per https://lavaan.ugent.be/tutorial/cat.html):
  d_colors_lavaan$blue <- ordered(d_colors_lavaan$blue, levels=c("no","yes")); # this is an endogenous (dependent) variable -> make it ordered
  d_colors_lavaan$subs <- ordered(d_colors_lavaan$subs, levels=c("HG","AGR")); # this is an endogenous (dependent) variable -> make it ordered
  
  # In order to avoid the warning...
  d_colors_lavaan$dalt <- d_colors_lavaan$dalt*100
  d_colors_lavaan$lat <- d_colors_lavaan$lat*100
  
  # Strictly the hypothesis and adding subsistence:
  sem_daltonism_essential_subs <- '
    # blue:
    blue ~ b_bu * uvb  # uvb  ---> blue 
         + b_bl * lat  # lat  ---> blue
         + b_bs * subs  # subs  ---> blue

    # UV-B incidence:   
    uvb  ~ b_ul * lat  # lat  ---> uvb  
    uvb ~~ subs
    
    # Daltonism:
    dalt ~ b_du * uvb  # uvb  ---> dalt
         + b_db * blue # blue ---> dalt
         + b_ds * subs # subs ---> dalt
         #+ b_dl * lat # lat ---> dalt
         
    # Subsistence
    subs ~ b_sl * lat # lat ---> subs
  ';
  semfit_daltonism_essential_subs <- sem(sem_daltonism_essential_subs, data=d_colors_lavaan, se="robust.sem");
  summary(semfit_daltonism_essential_subs, fit.measures=TRUE, standardize=TRUE, rsquare=TRUE, estimates=TRUE, ci=TRUE);
  lavaanPlot(model=semfit_daltonism_essential_subs, coefs=TRUE, sig=1.00, stand=FALSE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
  lavaanPlot(model=semfit_daltonism_essential_subs, coefs=TRUE, sig=0.05, stand=TRUE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
  semPaths(semfit_daltonism_essential_subs, what="est", edge.label.cex = 1.5, fade = FALSE)
  fitMeasures(semfit_daltonism_essential_subs, c("chisq", "df", "pvalue", "cfi", "tli", "nnfi", "rfi"));
  mi <- modindices(semfit_daltonism_essential_subs);

  
  # Strictly the hypothesis:
  sem_daltonism_essential <- '
    # blue:
    blue ~ b_bu * uvb  # uvb  ---> blue 
         + b_bl * lat  # lat  ---> blue

    # UV-B incidence:   
    uvb  ~ b_ul * lat  # lat  ---> uvb  

    # Daltonism:
    dalt ~ b_du * uvb  # uvb  ---> dalt
         + b_db * blue # blue ---> dalt
 ';
  semfit_daltonism_essential <- sem(sem_daltonism_essential, data=d_colors_lavaan, se="robust.sem");
  summary(semfit_daltonism_essential, fit.measures=TRUE, standardize=TRUE, rsquare=TRUE, estimates=TRUE, ci=TRUE);
  lavaanPlot(model=semfit_daltonism_essential, coefs=TRUE, sig=1.00, stand=FALSE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
  lavaanPlot(model=semfit_daltonism_essential, coefs=TRUE, sig=0.05, stand=TRUE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
  semPaths(semfit_daltonism_essential, what="est", edge.label.cex = 1.5, fade = FALSE)
  fitMeasures(semfit_daltonism_essential, c("chisq", "df", "pvalue", "cfi", "tli", "nnfi", "rfi"));
  mi <- modindices(semfit_daltonism_essential);

  # Save to file:
  sem_daltonism <- list("data"=d_colors_lavaan,
                        "model_essential"=semfit_daltonism_essential,
                        "model_essential_subs"=semfit_daltonism_essential_subs);
  save(sem_daltonism, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
} 
```

Here we fit a path analysis model that is strictly following hypothesis 2 (i.e., we exclude other potential factors):

```{r fig.cap=capFig('The path model with non-standardized coefficients, showing all estimated path estimates. Single-headed arrows represent regressions, double-headed arrows represent covariance and variance (when refering to the same node), and the arrows emerging from triangles are the intercepts; blue edges have negative estimates, red ones positive estimates.'), fig.width=15, fig.height=10}
semPaths(sem_daltonism$model_essential, what="est", edge.label.cex=0.75, fade=FALSE, nCharNodes=0, posCol="red", negCol="blue", layout="tree2");
```

```{r fig.cap=capFig('The path model with standardized coefficients; conventions as above.'), fig.width=15, fig.height=10}
semPaths(sem_daltonism$model_essential, what="std", edge.label.cex=0.75, fade=FALSE, nCharNodes=0, posCol="red", negCol="blue", layout="tree2");
```

```{r fig.cap=capFig('The path model with non-standardized coefficients, showing all path estimates with significance. Single-headed arrows represent regressions, while double-headed arrows represent covariance. Please note that this is not a "standard" SEM/path analysis diagram (see below for such a representation).')}
lavaanPlot(model=sem_daltonism$model_essential, coefs=TRUE, sig=1.00, stand=FALSE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
```

```{r fig.cap=capFig('The path model with standardized coefficients (see @grace_interpreting_2005 about pitfalls in interpreting such coefficients) showing only the significant (at the 0.05 level) path estimates with significance.')}
lavaanPlot(model=sem_daltonism$model_essential, coefs=TRUE, sig=0.05, stand=TRUE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
```

This model fits the data well: `r sprintf("*&chi;*^2^(%d) = %.1f, *p* = %.3g; CFI = %.3f, TLI = %.3f, NNFI = %.3f, RMSEA = %.3f 90%%CI [%.3f, %.3f]", fitMeasures(sem_daltonism$model_essential, c("df")), fitMeasures(sem_daltonism$model_essential, c("chisq")), fitMeasures(sem_daltonism$model_essential, c("pvalue")), fitMeasures(sem_daltonism$model_essential, c("cfi")), fitMeasures(sem_daltonism$model_essential, c("tli")), fitMeasures(sem_daltonism$model_essential, c("nnfi")), fitMeasures(sem_daltonism$model_essential, c("rmsea")), fitMeasures(sem_daltonism$model_essential, c("rmsea.ci.lower")), fitMeasures(sem_daltonism$model_essential, c("rmsea.ci.upper")))`.


As *subsistence* could potentially be linked to *colour blindness* rate, as observed by (@salzano_color_1964; @salzano_rare_1961; @post_colorblindness_1963; @post_population_1962  @pickford_natural_1963 ; @kalmus_frequency_1964; @kalmus_frequency_1961; @kalmus_defective_1957; @junqueira_ptc_1957 ; @garth_incidence_1933; @adam_further_1969), we also created a path analysis model including subsistence:

```{r fig.cap=capFig('The path model with standardized coefficients (see @grace_interpreting_2005 about pitfalls in interpreting such coefficients) showing only the significant (at the 0.05 level) path estimates with significance.')}
lavaanPlot(model=sem_daltonism$model_essential_subs, coefs=TRUE, sig=0.05, stand=TRUE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
```

This model also fits the data well: `r sprintf("*&chi;*^2^(%d) = %.1f, *p* = %.3g; CFI = %.3f, TLI = %.3f, NNFI = %.3f, RMSEA = %.3f 90%%CI [%.3f, %.3f]", fitMeasures(sem_daltonism$model_essential_subs, c("df")), fitMeasures(sem_daltonism$model_essential_subs, c("chisq")), fitMeasures(sem_daltonism$model_essential_subs, c("pvalue")), fitMeasures(sem_daltonism$model_essential_subs, c("cfi")), fitMeasures(sem_daltonism$model_essential_subs, c("tli")), fitMeasures(sem_daltonism$model_essential_subs, c("nnfi")), fitMeasures(sem_daltonism$model_essential_subs, c("rmsea")), fitMeasures(sem_daltonism$model_essential_subs, c("rmsea.ci.lower")), fitMeasures(sem_daltonism$model_essential_subs, c("rmsea.ci.upper")))`.


The full model outputs are in [Appendix III. Path analysis for "daltonism"]), but we can see that:

- as above, *latitude* affects *UV-B* (-) and *blue* (-),
- and *daltonism* is affected by:
    + in the absence of *subsistence*, by *blue* (+) and by *UV-B* (-), 
    + but, when *subsistence* is added, by *subsistence* (+) and *UV-B* (-).

<!-- ### Path analysis - paper format -->

<!-- #### Without subsistence (strictly the hypothesis) -->

<!-- ```{r} -->

<!-- standardizedsolution(sem_daltonism$model_essential) -->

<!-- DiagrammeR::grViz(paste0(' -->
<!--   digraph hypothesis_2_pa { -->

<!--   # the graph: -->
<!--   graph [overlap = true] -->
<!--   rankdir="LR"; -->

<!--   # the nodes: -->
<!--   node [shape = box, style = "filled", fillcolor = "lightyellow"]; -->
<!--   dalt [label = "daltonism", tootlip = "daltonism"]; -->
<!--   UVB  [label = "UV-B", tooltip = "UV-B"];  -->
<!--   blue [label = "blue", tooltip = "blue"];  -->
<!--   lat  [label = "latitude", tooltip = "latitude"];  -->

<!--   # the edges: -->
<!--   edge [style = "solid", color = "black"]; -->
<!--   UVB  -> dalt [label="-0.31 (0.000)", color="blue", fontcolor="blue"]; -->
<!--   UVB  -> blue [label="-0.55 (0.000)", color="blue", fontcolor="blue"]; -->
<!--   lat  -> UVB  [label="-0.95 (0.000)", color="blue", fontcolor="blue"]; -->
<!--   blue -> dalt [label="0.4 (0.000)", color="red", fontcolor="red"]; -->
<!-- } -->
<!-- ')) -->

<!-- standardizedsolution(sem_daltonism$model_essential_subs) -->

<!-- DiagrammeR::grViz(paste0(' -->
<!--   digraph hypothesis_2_pa { -->

<!--   # the graph: -->
<!--   graph [overlap = true] -->
<!--   rankdir="LR"; -->

<!--   # the nodes: -->
<!--   node [shape = box, style = "filled", fillcolor = "lightyellow"]; -->
<!--   dalt [label = "daltonism", tootlip = "daltonism"]; -->
<!--   UVB  [label = "UV-B", tooltip = "UV-B"];  -->
<!--   blue [label = "blue", tooltip = "blue"];  -->
<!--   lat  [label = "latitude", tooltip = "latitude"];  -->
<!--   subs  [label = "subsistence", tooltip = "subsistence"];  -->

<!--   # the edges: -->
<!--   edge [style = "solid", color = "black"]; -->
<!--   UVB  -> dalt [label="-0.53 (0.000)", color="blue", fontcolor="blue"]; -->
<!--   UVB  -> blue [label="-0.56 (0.000)", color="blue", fontcolor="blue"]; -->
<!--   lat  -> UVB  [label="-0.96 (0.000)", color="blue", fontcolor="blue"]; -->
<!--   blue -> dalt [label="0.11 (0.324)", color="gray", fontcolor="gray50", style="dashed"]; -->
<!--   lat  -> subs  [label="-0.18 (0.111)", color="gray", fontcolor="gray50", style="dashed"]; -->
<!--   subs  -> blue  [label="0.41 (0.000)", color="red", fontcolor="red"]; -->
<!--   subs  -> dalt  [label="0.38 (0.000)", color="red", fontcolor="red"]; -->
<!--   subs  -> UVB  [dir=both, label="-0.26 (0.016)", color="blue", fontcolor="blue"]; -->
<!-- } -->
<!-- ')) -->

<!-- ``` -->



### Predicting daltonism

We check how good are various techniques at predicting the population frequency of daltonism from a collection of potential predictors.
As for 'blue', we estimate this on the full dataset and on random training and testing subsets.

```{r include=FALSE}
load("./cached_results/traning-testing-splits.RData"); # (re)load the data...
```



#### Multiple regression

```{r include=FALSE}
file_name <- "./cached_results/brms_daltonism.RData";
if( !all(file.exists(file_name)) )
{
  # Save to file:
  brms_daltonism <- list("data"=d);

  ## On the full data:
  brms_dalto <- brm(daltonism_r ~ 1 
                    + UVB_r                                                       # UV-B incidence
                    + longitude_r + latitude_r                                    # geographic location
                    + subsistence  + log_popSize                                  # subsistence strategy and population size
                    + clim_PC1_r + clim_PC2_r + clim_PC3_r + hum_median + hum_IQR # climate, ecology and humidity
                    + elevation_r                                                 # altitude 
                    + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r  # distances to large bodies of water 
                    + exists_blue                                                 # dedicated word for blue
                    + gen_D1_r + gen_D2_r + gen_D3_r + gen_D4_r + gen_D5_r + gen_D6_r + gen_D7_r + gen_D8_r + gen_D9_r + gen_D10_r # genetic distances between populations
                    + (1 | glottocode_family) + (1 | macroarea),                  # random factors
                    data = d, 
                    family=Beta(), 
                    prior=c(prior(normal(0, 5), class="Intercept"),
                            prior(normal(0, 5), class="b")), # pretty wide priors centered on 0
                    save_all_pars=TRUE, # needed for Bayes factors
                    sample_prior=TRUE,  # needed for hypotheses tests
                    cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  summary(brms_dalto); mcmc_plot(brms_dalto, type="trace"); mcmc_plot(brms_dalto);
  (hdi_full <- hdi(brms_dalto, ci=0.95)); (rope_full <- rope(brms_dalto, ci=0.95));
  brms_dalto <- brms_fit_indices(brms_dalto);
  (cmp_null_full <- brms_compare_models(b_0__daltonism, brms_dalto, "null", "full")); # not clear if daltonism can be predicted with these data...
  # Iterative manual simplification:
  brms_dalto_1 <- update(brms_dalto, . ~ . - log_popSize - dist2rivers_r - elevation_r - clim_PC3_r - gen_D10_r - gen_D7_r - gen_D6_r - gen_D4_r - gen_D1_r - 
                           clim_PC2_r - hum_median - dist2ocean_r - hum_IQR - gen_D3_r - gen_D9_r - dist2lakes_r - gen_D5_r - latitude_r - UVB_r -
                           subsistence - gen_D2_r,
                         save_all_pars=TRUE, sample_prior=TRUE, cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10)); 
  summary(brms_dalto_1); mcmc_plot(brms_dalto_1, type="trace"); mcmc_plot(brms_dalto_1) + xlim(c(-5, 5)); # phi is too big and swamps the plot
  (hdi_reduced <- hdi(brms_dalto_1, ci=0.95)); (rope_reduced <- rope(brms_dalto_1, ci=0.95)); fixef_reduced <- fixef(brms_dalto_1);
  brms_dalto_1 <- brms_fit_indices(brms_dalto_1);
  # Model comparison and performance:
  (cmp_full_reduced <- brms_compare_models(brms_dalto, brms_dalto_1, "full", "reduced")); # reduced is better
  (cmp_null_reduced <- brms_compare_models(b_0__daltonism, brms_dalto_1, "null", "reduced")); # reduced is better
  (r2_full    <- bayes_R2(brms_dalto));
  (r2_reduced <- bayes_R2(brms_dalto_1));
  (rmse_full    <- sqrt(mean(predictive_error(brms_dalto, resp="daltonism_r")^2)));
  (rmse_reduced <- sqrt(mean(predictive_error(brms_dalto_1, resp="daltonism_r")^2)));
  # Save:
  brms_daltonism$all_data <- list("full"   =list("cmp_2_null"=cmp_null_full, 
                                                 "HDI"=hdi_full, "ROPE"=rope_full, "R2"=r2_full["R2", "Estimate"], "RMSE"=rmse_full), 
                                  "reduced"=list("cmp_2_null"=cmp_null_reduced, "cmp_2_full"=cmp_full_reduced, 
                                                 "fixef"=fixef_reduced, "HDI"=hdi_reduced, "ROPE"=rope_reduced, "R2"=r2_reduced["R2", "Estimate"], "RMSE"=rmse_reduced));
  
    
  ## On the training/testing sets:
  results <- pblapply(1:n_train, function(i) # very computationally expensive
    {
      # split the data:
      data_train <- training(train_test_splits[[i]]);
      data_test  <- testing(train_test_splits[[i]]);

      # fit the models: avoid recompiling the model by "updating" the data it is fit on:
      invisible(capture.output(brms_train <- update(brms_dalto, newdata=data_train, refresh=0, 
                                                    save_all_pars=TRUE, sample_prior=TRUE, cores=brms_ncores, 
                                                    iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=10)),
                               type="message")); # capture the messages printed by Stan
      
      # performance:
      r2_train   <- bayes_R2(brms_train, newdata=data_test, allow_new_levels=TRUE);
      rmse_train <- sqrt(mean(predictive_error(brms_train, newdata=data_test, allow_new_levels=TRUE, resp="daltonism_r")^2));

      # predictors:
      hdi_train <- hdi(brms_train, ci=0.95); rope_train <- rope(brms_train, ci=0.95, verbose=FALSE);

      # return the results:
      success_train <- data.frame("replication" =i, # replication
                                  "R2"          =r2_train["R2", "Estimate"],
                                  "RMSE"        =rmse_train,
                                  row.names=NULL);
      preds_train <- data.frame("replication"=i, # replication,
                                "predictor"  =rownames(fixef(brms_train)), 
                                "estimate"   =fixef(brms_train)[,"Estimate"],
                                as.data.frame(hdi_train)[,c("CI_low", "CI_high")],
                                as.data.frame(rope_train)[,c("ROPE_low", "ROPE_high", "ROPE_Percentage")],
                                row.names=NULL);
      
      return (list("replication"       =i,
                   "data_train_indices"=train_test_splits[[i]]$in_id, 
                   "data_test_indices" =setdiff(1:nrow(train_test_splits[[i]]$data), train_test_splits[[i]]$in_id),
                   "performance"       =list("success"=success_train, "predictors"=preds_train)));
    });
  
  # Assemble the various results and save them:
  brms_daltonism$training_testing <- list("replications"=n_train,
                                          "performance" =list("success"   =do.call(rbind, lapply(results, function(x) x$performance$success)),   
                                                              "predictors"=do.call(rbind, lapply(results, function(x) x$performance$predictors))));
  
    
  # Save:
  save(brms_daltonism, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
} 
```


On the full dataset, Bayesian mixed effects logistic regression with family and macroarea as random effects and using all potential predictors as fixed effects, fits the data very well: `r sprintf("R^2^ = %.1f%%, RMSE = %.3f", brms_daltonism$all_data$full$R2*100, brms_daltonism$all_data$full$RMSE)`.
After iterative manual simplification, we still fit the data very well (`r sprintf("R^2^ = %.1f%%, RMSE = %.3f", brms_daltonism$all_data$reduced$R2*100, brms_daltonism$all_data$reduced$RMSE)`), and the retained predictors are:

```{r}
kable(data.frame("predictor"  =rownames(brms_daltonism$all_data$reduced$fixef), 
                 "estimate"   =round(brms_daltonism$all_data$reduced$fixef[,"Estimate"],2),
                 "hdi"=paste0("[", round(brms_daltonism$all_data$reduced$HDI[,"CI_low"],2), ",", round(brms_daltonism$all_data$reduced$HDI[,"CI_high"],2), "]"),
                 "p.rope"=sprintf("%.2g",brms_daltonism$all_data$reduced$ROPE[,"ROPE_Percentage"]),
                 row.names=NULL), 
      row.names=FALSE, col.names=c("Predictor", "*&beta;* (estimate)", "*&beta;* (95% HDI)", "*p*~ROPE~"),
      caption=capTab(paste0("Retained predictors for Bayesian mixed effects logistic regressions following iterative maual simplification on the full dataset. The ROPE is [", round(brms_daltonism$all_data$reduced$ROPE[1,"ROPE_low"],2), ",", round(brms_daltonism$all_data$reduced$ROPE[1,"ROPE_high"],2), "].")));
```


When randomly splitting the dataset into 80% training/20% testing subsets `r brms_daltonism$training_testing$replications` times, using all the potential predictors, we obtain `r sprintf("R^2^ = %.1f%% ±%.1f%%, and RMSE = %.3f ±%.3f", mean(brms_daltonism$training_testing$performance$success$R2,na.rm=TRUE)*100, sd(brms_daltonism$training_testing$performance$success$R2,na.rm=TRUE)*100, mean(brms_daltonism$training_testing$performance$success$RMSE,na.rm=TRUE), sd(brms_daltonism$training_testing$performance$success$RMSE,na.rm=TRUE))`.

```{r fig.cap=capFig(paste0("Various measures of success using Bayesian mixed effects logistic regressions splitting the dataset randomly into 80% training/20% testing subsets, ",brms_daltonism$training_testing$replications," times. Boxplots show the spread of the training/testing values, while the solid horizontal lines show the values when using the full dataset.")), fig.width=2*2, fig.height=5}
grid.arrange(ggplot(brms_daltonism$training_testing$performance$success, aes(y=R2*100, x="")) + 
               geom_jitter(alpha=0.50, color="steelblue") + geom_violin(alpha=0.25) + geom_boxplot(fill=alpha("blue",0.30)) + 
               theme_bw() +
               ylim(c(0,100)) + xlab("") + ylab(expression("R"^2)) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               # measures on the full dataset:
               geom_hline(yintercept=brms_daltonism$all_data$full$R2*100, color="blue", linetype="solid", size=1) + 
               theme(legend.position="right") +
             NULL,
             ggplot(brms_daltonism$training_testing$performance$success, aes(y=RMSE, x="")) + 
               geom_jitter(alpha=0.50, color="salmon") + geom_violin(alpha=0.25) + geom_boxplot(fill=alpha("red",0.30)) + 
               theme_bw() +
               ylim(c(0.0,0.1)) + xlab("") + ylab("RMSE") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               # measures on the full dataset:
               geom_hline(yintercept=brms_daltonism$all_data$full$RMSE, color="red", linetype="solid", size=1) + 
               NULL,
             ncol=2);
```



#### Conditional inference trees

We used conditional inference trees (as implemented by `ctree()` in package `partykit`) to predict the population frequency of *daltonism* from all the potential predictors:

```{r include=FALSE}
file_name <- "./cached_results/ctree_daltonism.RData";
if( !all(file.exists(file_name)) )
{
  ## On the full data:
  ctree_dalto <- ctree(daltonism_r ~ 
                         macroarea                                                   # macroarea
                       + UVB_r                                                       # UV-B incidence
                       + longitude_r + latitude_r                                    # geographic location
                       + subsistence  + log_popSize                                  # subsistence strategy and population size
                       + clim_PC1_r + clim_PC2_r + clim_PC3_r + hum_median + hum_IQR # climate, ecology and humidity
                       + elevation_r                                                 # altitude 
                       + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r  # distances to large
                       + exists_blue                                                 # dedicated word for blue
                       + gen_D1_r + gen_D2_r + gen_D3_r + gen_D4_r + gen_D5_r + gen_D6_r + gen_D7_r + gen_D8_r + gen_D9_r + gen_D10_r, # genetic distances between populations
                       data = d, control = ctree_control(testtype = c("MonteCarlo")));
  (r2_all   <- 1.0 - (sum((d$daltonism_r - predict(ctree_dalto))^2)) / (sum((d$daltonism_r - mean(d$daltonism_r))^2)));
  (rmse_all <- sqrt(mean((d$daltonism_r - predict(ctree_dalto))^2)));
  
    
  ## On the training/testing sets:
  results <- pblapply(1:n_train, function(i) # very computationally expensive
    {
      # split the data:
      data_train <- training(train_test_splits[[i]]);
      data_test  <- testing(train_test_splits[[i]]);

      # fit the models:
      ctree_train <- ctree(daltonism_r ~ 
                             macroarea                                                   # macroarea
                           + UVB_r                                                       # UV-B incidence
                           + longitude_r + latitude_r                                    # geographic location
                           + subsistence  + log_popSize                                  # subsistence strategy and population size
                           + clim_PC1_r + clim_PC2_r + clim_PC3_r + hum_median + hum_IQR # climate, ecology and humidity
                           + elevation_r                                                 # altitude 
                           + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r  # distances to large bodies of water 
                           + exists_blue                                                 # dedicated word for blue
                           + gen_D1_r + gen_D2_r + gen_D3_r + gen_D4_r + gen_D5_r + gen_D6_r + gen_D7_r + gen_D8_r + gen_D9_r + gen_D10_r, # genetic distances between populations
                           data = data_train, control = ctree_control(testtype = c("MonteCarlo")));
      
      # performance:
      err_pred <- NULL;
      try(err_pred <- (data_test$daltonism_r - predict(ctree_train, newdata=data_test, allow_new_levels=TRUE))^2, silent=TRUE);
      if( !is.null(err_pred) )
      {
        (r2_train   <- 1.0 - sum(err_pred) / (sum((data_test$daltonism_r - mean(data_test$daltonism_r))^2)));
        (rmse_train <- sqrt(mean(err_pred)));
        
        # return the results:
        success_train <- data.frame("replication" =i, # replication
                                    "R2"          =r2_train,
                                    "RMSE"        =rmse_train,
                                    row.names=NULL);
      } else
      {
        success_train <- data.frame("replication" =i, # replication
                                    "R2"          =NA,
                                    "RMSE"        =NA,
                                    row.names=NULL);
      }
      
      return (list("replication"       =i,
                   "data_train_indices"=train_test_splits[[i]]$in_id, 
                   "data_test_indices" =setdiff(1:nrow(train_test_splits[[i]]$data), train_test_splits[[i]]$in_id),
                   "performance"       =list("success"=success_train)));
  });
  
  
  # Save to file:
  ctree_daltonism <- list("data"=d,
                          "all"=ctree_dalto, 
                          "all_data"=list("R2"=r2_all, "RMSE"=rmse_all), 
                          "training_testing"=list("replications"   =n_train,
                                                  "performance"=list("success"=do.call(rbind, lapply(results, function(x) x$performance$success)))));
  save(ctree_daltonism, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
} 
```

On the full dataset, the conditional inference tree using all potential predictors as fixed effects, fits the data very well: `r sprintf("R^2^ = %.1f%%, RMSE = %.3f", ctree_daltonism$all_data$R2*100, ctree_daltonism$all_data$RMSE)`, and the tree is:

```{r fig.cap=capFig('Conditional inference trees for predicting *daltonism* using all the potential predictors.'), fig.width=10, fig.height=6, fig.show='hold', out.height="50%"}
plot(ctree_daltonism$all, gp=gpar(fontsize = 11));
```


When randomly splitting the dataset into 80% training/20% testing subsets `r ctree_daltonism$training_testing$replications` times, using all the potential predictors, we obtain `r sprintf("R^2^ = %.1f%% ±%.1f%%, and RMSE = %.3f ±%.3f", mean(ctree_daltonism$training_testing$performance$success$R2,na.rm=TRUE)*100, sd(ctree_daltonism$training_testing$performance$success$R2,na.rm=TRUE)*100, mean(ctree_daltonism$training_testing$performance$success$RMSE,na.rm=TRUE), sd(ctree_daltonism$training_testing$performance$success$RMSE,na.rm=TRUE))`.

```{r fig.cap=capFig(paste0("Various measures of success using Bayesian mixed effects logistic regressions splitting the dataset randomly into 80% training/20% testing subsets, ",ctree_daltonism$training_testing$replications," times. Boxplots show the spread of the training/testing values, while the solid horizontal lines show the values when using the full dataset.")), fig.width=2*2, fig.height=5}
grid.arrange(ggplot(ctree_daltonism$training_testing$performance$success, aes(y=R2*100, x="")) + 
               geom_jitter(alpha=0.50, color="steelblue") + geom_violin(alpha=0.25) + geom_boxplot(fill=alpha("blue",0.30)) + 
               theme_bw() +
               ylim(c(0,100)) + xlab("") + ylab(expression("R"^2)) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               # measures on the full dataset:
               geom_hline(yintercept=ctree_daltonism$all_data$R2*100, color="blue", linetype="solid", size=1) + 
               theme(legend.position="right") +
             NULL,
             ggplot(ctree_daltonism$training_testing$performance$success, aes(y=RMSE, x="")) + 
               geom_jitter(alpha=0.50, color="salmon") + geom_violin(alpha=0.25) + geom_boxplot(fill=alpha("red",0.30)) + 
               theme_bw() +
               ylim(c(0.0,0.1)) + xlab("") + ylab("RMSE") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               # measures on the full dataset:
               geom_hline(yintercept=ctree_daltonism$all_data$RMSE, color="red", linetype="solid", size=1) + 
               NULL,
             ncol=2);
```


#### Random forests

We used random forests (as implemented by `randomForest()` in package `randomForest`) and conditional random forests (as implemented by `cforest()` in package `partykit`) to predict the existence of a specific word for 'blue' (*blue*) from all the potential predictors:

```{r include=FALSE}
file_name <- "./cached_results/rforest_daltonism.RData";
if( !all(file.exists(file_name)) )
{
  # Collect the success measures and the predictor importance based on the gini index (randomForest) and the accuracy indices (randomForest and cForest):
  n_replications <- n_train; # the number of replications
  cf_ntree       <- 500; # the number of trees for cforest
  cf_nperm       <- 10;  # number of permutations for cforest:varimp
  results <- pblapply(1:n_replications, function(i) # very computationally expensive
    {
      # fit the random forests:
      rf_dalto <- randomForest(daltonism_r ~ 
                                 macroarea                                                   # macroarea
                               + UVB_r                                                       # UV-B incidence
                               + longitude_r + latitude_r                                    # geographic location
                               + subsistence  + log_popSize                                  # subsistence strategy and population size
                               + clim_PC1_r + clim_PC2_r + clim_PC3_r + hum_median + hum_IQR # climate, ecology and humidity
                               + elevation_r                                                 # altitude 
                               + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r  # distances to large bodies of water 
                               + exists_blue                                                 # dedicated word for blue
                               + gen_D1_r + gen_D2_r + gen_D3_r + gen_D4_r + gen_D5_r + gen_D6_r + gen_D7_r + gen_D8_r + gen_D9_r + gen_D10_r, # genetic distances between populations
                               data=d, importance=TRUE);
      cf_dalto <- cforest(daltonism_r ~ 
                            macroarea                                                   # macroarea
                          + UVB_r                                                       # UV-B incidence
                          + longitude_r + latitude_r                                    # geographic location
                          + subsistence  + log_popSize                                  # subsistence strategy and population size
                          + clim_PC1_r + clim_PC2_r + clim_PC3_r + hum_median + hum_IQR # climate, ecology and humidity
                          + elevation_r                                                 # altitude 
                          + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r  # distances to large bodies of water 
                          + exists_blue                                                 # dedicated word for blue
                          + gen_D1_r + gen_D2_r + gen_D3_r + gen_D4_r + gen_D5_r + gen_D6_r + gen_D7_r + gen_D8_r + gen_D9_r + gen_D10_r, # genetic distances between populations
                          data=d, ntree=cf_ntree);
      
      # performance:
      rf_err_pred <- (d$daltonism_r - rf_dalto$predicted)^2;
      (rf_r2   <- 1.0 - sum(rf_err_pred) / (sum((d$daltonism_r - mean(d$daltonism_r))^2)));
      (rf_rmse <- sqrt(mean(rf_err_pred)));
      cf_err_pred <- (d$daltonism_r - predict(cf_dalto))^2;
      (cf_r2   <- 1.0 - sum(cf_err_pred) / (sum((d$daltonism_r - mean(d$daltonism_r))^2)));
      (cf_rmse <- sqrt(mean(cf_err_pred)));
        
      # return the results:
      # success_train <- data.frame("replication" =i, # replication
      #                             "R2"          =r2_train,
      #                             "RMSE"        =rmse_train,
      #                             row.names=NULL);

      # predictor importance:
      pia_rf_dalto <- importance(rf_dalto, type=1); # accuracy-based
      pig_rf_dalto <- importance(rf_dalto, type=2); # gini-based
      piu_cf_dalto <- varimp(cf_dalto, conditional=FALSE, nperm=cf_nperm); # unconditional

      # return the results:
      success_dalto <- data.frame("replication"     =i,       # replication
                                  "rf_R2"           =rf_r2,   # randomForest
                                  "rf_RMSE"         =rf_rmse,
                                  "cf_R2"           =cf_r2,   # cforest
                                  "cf_RMSE"         =cf_rmse,
                                  row.names=NULL);
      pi_df_dalto <- data.frame("replication"      =i,                                  # replication
                                "predictor"        =attr(cf_dalto$terms,"term.labels"), # predictors
                                "rf_accuracy_based"=pia_rf_dalto,                       # randomForest accuracy-based
                                "rf_gini_based"    =pig_rf_dalto,                       # randomForest gini-based
                                "cf_unconditional" =piu_cf_dalto,                       # cforest unconditional
                                row.names=NULL);

      return (list("replication" =i,
                   "performance" =list("success"=success_dalto, "pred_importance"=pi_df_dalto)));
    }); #, mc.cores=max(detectCores(all.tests=TRUE, logical=FALSE), 1, na.rm=TRUE)); # try to use multiple cores, if present (limited to a maximum of 8)
  
  # Assemble the various results and save them:
  rforest_daltonism <- list("data"        =d,
                            "replications"=n_replications,
                            "results"=list("success"=do.call(rbind, lapply(results, function(x) x$performance$success)),   
                                           "pred_importance"=do.call(rbind, lapply(results, function(x) x$performance$pred_importance))));
  save(rforest_daltonism, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
} 
```

On the full dataset, the (conditional) random forests using all potential predictors as fixed effects, fit the data very well: 

- random forests: `r sprintf("R^2^ = %.1f%% ±%.1f%%, RMSE = %.3f ±%.3f", mean(rforest_daltonism$results$success$rf_R2*100), sd(rforest_daltonism$results$success$rf_R2*100), mean(rforest_daltonism$results$success$rf_RMSE), sd(rforest_daltonism$results$success$rf_RMSE))`
- conditional random forests: `r sprintf("R^2^ = %.1f%% ±%.1f%%, RMSE = %.3f ±%.3f", mean(rforest_daltonism$results$success$cf_R2*100), sd(rforest_daltonism$results$success$cf_R2*100), mean(rforest_daltonism$results$success$cf_RMSE), sd(rforest_daltonism$results$success$cf_RMSE))`.

```{r fig.cap=capFig(paste0("Various measures of success using random forests (RF) and conditional random forests (CRF).")), fig.width=4*2, fig.height=1*5}
grid.arrange(ggplot(rforest_daltonism$results$success, aes(y=rf_R2*100, x="")) + 
               geom_jitter(alpha=0.50, color="steelblue") + geom_violin(alpha=0.25) + geom_boxplot(fill=alpha("blue",0.30)) + 
               theme_bw() +
               ylim(c(0,100)) + xlab("RF") + ylab(expression("R"^2)) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               theme(legend.position="right") +
             NULL,
             ggplot(rforest_daltonism$results$success, aes(y=cf_R2*100, x="")) + 
               geom_jitter(alpha=0.50, color="steelblue") + geom_violin(alpha=0.25) + geom_boxplot(fill=alpha("blue",0.30)) + 
               theme_bw() +
               ylim(c(0,100)) + xlab("CRF") + ylab(expression("R"^2)) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               theme(legend.position="right") +
             NULL,
             ggplot(rforest_daltonism$results$success, aes(y=rf_RMSE, x="")) + 
               geom_jitter(alpha=0.50, color="salmon") + geom_violin(alpha=0.25) + geom_boxplot(fill=alpha("red",0.30)) + 
               theme_bw() +
               ylim(c(0.0,0.1)) + xlab("RF") + ylab("RMSE") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               NULL,
             ggplot(rforest_daltonism$results$success, aes(y=cf_RMSE, x="")) + 
               geom_jitter(alpha=0.50, color="salmon") + geom_violin(alpha=0.25) + geom_boxplot(fill=alpha("red",0.30)) + 
               theme_bw() +
               ylim(c(0.0,0.1)) + xlab("CRF") + ylab("RMSE") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               NULL,
             ncol=4);
```

The importance of the predictors is:

```{r fig.cap=capFig(paste0("Predictor importance from random forests using ",rforest_blue$replications," replications using random forests (left: accuracy-based, middle: Gini-index based) and conditional random forests (right).")), fig.width=4*3, fig.height=7*1}
grid.arrange(ggplot(rforest_daltonism$results$pred_importance, 
                    aes(x=reorder(predictor, X.IncMSE, median), y=X.IncMSE)) +
               theme_bw() +
               geom_boxplot(fill="salmon", color="black") +
               labs(x="Predictors (sorted by decreasing importance)", y="Mean decrease accuracy") +
               theme(axis.text.y = element_text(angle=30, hjust=1, size=10), 
                     axis.text.x = element_text(hjust=1, size=10),
                     axis.title=element_text(size=16),
                     legend.position="none") +
               coord_flip() + 
               NULL,
             ggplot(rforest_daltonism$results$pred_importance, 
                    aes(x=reorder(predictor, IncNodePurity, median), y=IncNodePurity)) +
               theme_bw() +
               geom_boxplot(fill="salmon", color="black") +
               labs(x="Predictors (sorted by decreasing importance)", y="Node purity") +
               theme(axis.text.y = element_text(angle=30, hjust=1, size=10), 
                     axis.text.x = element_text(hjust=1, size=10),
                     axis.title=element_text(size=16),
                     legend.position="none") +
               coord_flip() + 
               NULL,
             ggplot(rforest_daltonism$results$pred_importance, 
                    aes(x=reorder(predictor, cf_unconditional, median), y=cf_unconditional)) +
               theme_bw() +
               geom_boxplot(fill="salmon", color="black") +
               labs(x="Predictors (sorted by decreasing importance)", y="Conditional") +
               theme(axis.text.y = element_text(angle=30, hjust=1, size=10), 
                     axis.text.x = element_text(hjust=1, size=10),
                     axis.title=element_text(size=16),
                     legend.position="none") +
               coord_flip() + 
               NULL,
             ncol=3);
```


#### SVM

We used Support Vector Machines (SVMs, as implemented by `fit(...,model="svm")` in the `rminer` package) to predict the population frequency of daltonism (*daltonism*) from all the potential predictors.

```{r include=FALSE}
file_name <- "./cached_results/svm_daltonism.RData";
if( !all(file.exists(file_name)) )
{
  # The potential predictors:
  list_vars <- c("macroarea",                                                       # macroarea
                 "UVB_r",                                                           # UV-B incidence
                 "longitude_r", "latitude_r",                                       # geographic location
                 "subsistence", "log_popSize",                                      # subsistence strategy and population size
                 "clim_PC1_r", "clim_PC2_r", "clim_PC3_r", "hum_median", "hum_IQR", # climate, ecology and humidity
                 "elevation_r",                                                     # altitude 
                 "dist2water_r", "dist2ocean_r", "dist2lakes_r", "dist2rivers_r",   # distances to large bodies of water
                 "exists_blue",                                                     # dedicated word for blue
                 "gen_D1_r", "gen_D2_r", "gen_D3_r", "gen_D4_r", "gen_D5_r",        # genetic distances between populations
                 "gen_D6_r", "gen_D7_r", "gen_D8_r", "gen_D9_r", "gen_D10_r");
  # formula:
  f_svm <- formula(paste0("daltonism_r ~ ", paste0(list_vars,collapse=" + ")));
  # remove the NAs in the relevant variables as SVM complains...
  d <- d[ complete.cases(d[,c("daltonism_r", list_vars)]), c("daltonism_r", list_vars)];

  
  ## On the full data:
  # fit the SVMs:
  svm_fit_alldata <- rminer::fit(f_svm, d, model="svm", task="reg");
  
  # success:
  (r2_all   <- 1.0 - (sum((d$daltonism_r - predict(svm_fit_alldata, d))^2)) / (sum((d$daltonism_r - mean(d$daltonism_r))^2)));
  (rmse_all <- sqrt(mean((d$daltonism_r - predict(svm_fit_alldata, d))^2)));
  
  # predictor importance (based on sensitivity analysis):
  tmp             <- rminer::Importance(svm_fit_alldata, d, method="sensv"); 
  pia_svm_alldata <- tmp$imp; names(pia_svm_alldata) <- names(d); pia_svm_alldata <- pia_svm_alldata[names(pia_svm_alldata) %in% list_vars];

      
  # Using training/testing subsets:
  results <- pblapply(1:n_train, function(i)
    {
      # split the dataset into a training (80%) and a testing (20%) set stratified by exists_blue:
      data_train <- training(train_test_splits[[i]]);
      data_test  <- testing(train_test_splits[[i]]);
      # keep only the relevant columns and no NAs:
      data_train <- data_train[ complete.cases(data_train[,c("daltonism_r", list_vars)]), c("daltonism_r", list_vars)];
      data_test  <- data_test [ complete.cases(data_test [,c("daltonism_r", list_vars)]), c("daltonism_r", list_vars)];
      
      # fit the SVM:
      svm_fit_train <- rminer::fit(f_svm, data_train, model="svm", task="reg");

      # success:
      (r2_train   <- 1.0 - (sum((data_test$daltonism_r - predict(svm_fit_train, data_test))^2)) / (sum((data_test$daltonism_r - mean(data_test$daltonism_r))^2)));
      (rmse_train <- sqrt(mean((data_test$daltonism_r - predict(svm_fit_train, data_test))^2)));
    
      # predictor importance (based on sensitivity analysis):
      tmp           <- rminer::Importance(svm_fit_train, data_train, method="sensv"); 
      pia_svm_train <- tmp$imp; names(pia_svm_train) <- names(data_train); pia_svm_train <- pia_svm_train[names(pia_svm_train) %in% list_vars];

      # return the results:
      success_train <- data.frame("replication"=i, # replication
                                  "R2"  =r2_train,
                                  "RMSE"=rmse_train,
                                  row.names=NULL);
      pi_df_train   <- data.frame("replication"=i,                        # replication
                                  "predictor"  =list_vars,                # predictors
                                  "importance" =pia_svm_train[list_vars], # sensitivity-based
                                  row.names=NULL);
      
      return (list("replication"       =i,
                   "data_train_indices"=train_test_splits[[i]]$in_id, 
                   "data_test_indices" =setdiff(1:nrow(train_test_splits[[i]]$data), train_test_splits[[i]]$in_id),
                   "performance"       =list("success"=success_train, "pred_importance"=pi_df_train)));
    });
  
  # Save to file:
  svm_daltonism <- list("data"=d,
                        "all_data"=list("model"          =svm_fit_alldata, 
                                        "success"        =data.frame("R2"=r2_all, "RMSE"=rmse_all), 
                                        "pred_importance"=data.frame("predictor"  =list_vars,                  # predictors
                                                                     "importance" =pia_svm_alldata[list_vars], # sensitivity-based
                                                                     row.names=NULL)), 
                        "training_testing"=list("replications"   =n_train,
                                                "performance"=list("success"        =do.call(rbind, lapply(results, function(x) x$performance$success)),
                                                                   "pred_importance"=do.call(rbind, lapply(results, function(x) x$performance$pred_importance)))));
  save(svm_daltonism, file=file_name, compress="xz", compression_level=9);
} else
{
  load(file_name);
} 
```

On the full dataset, SVMs using all potential predictors, fits the data very well: `r sprintf("R^2^ = %.1f%%, RMSE = %.3f", svm_daltonism$all_data$success$R2*100, svm_daltonism$all_data$success$RMSE)`, and the variable importance is (only those with importance > 0):

```{r results='asis'}
tmp <- svm_daltonism$all_data$pred_importance[ order(svm_daltonism$all_data$pred_importance$importance, decreasing=TRUE), ];
tmp <- tmp[ tmp$importance > 0, ];
cat(paste0("*",tmp$predictor,"*", " (", round(tmp$importance,3), ")", collapse=", "));
```


When randomly splitting the dataset into 80% training/20% testing subsets `r svm_daltonism$training_testing$replications` times, using all the potential predictors, we obtain `r sprintf("R^2^ = %.1f%% ±%.1f%%, and RMSE = %.3f ±%.3f", mean(svm_daltonism$training_testing$performance$success$R2,na.rm=TRUE)*100, sd(svm_daltonism$training_testing$performance$success$R2,na.rm=TRUE)*100, mean(svm_daltonism$training_testing$performance$success$RMSE,na.rm=TRUE), sd(svm_daltonism$training_testing$performance$success$RMSE,na.rm=TRUE))`.

```{r fig.cap=capFig(paste0("Various measures of success using SVMs splitting the dataset randomly into 80% training/20% testing subsets, ",svm_daltonism$training_testing$replications," times. Boxplots show the spread of the training/testing values, while the solid horizontal lines show the values when using the full dataset.")), fig.width=2*2, fig.height=5}
grid.arrange(ggplot(svm_daltonism$training_testing$performance$success, aes(y=R2*100, x="")) + 
               geom_jitter(alpha=0.50, color="steelblue") + geom_violin(alpha=0.25) + geom_boxplot(fill=alpha("blue",0.30)) + 
               theme_bw() +
               ylim(c(0,100)) + xlab("") + ylab(expression("R"^2)) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               # measures on the full dataset:
               geom_hline(yintercept=svm_daltonism$all_data$success$R2*100, color="blue", linetype="solid", size=1) + 
               theme(legend.position="right") +
             NULL,
             ggplot(svm_daltonism$training_testing$performance$success, aes(y=RMSE, x="")) + 
               geom_jitter(alpha=0.50, color="salmon") + geom_violin(alpha=0.25) + geom_boxplot(fill=alpha("red",0.30)) + 
               theme_bw() +
               ylim(c(0.0,0.1)) + xlab("") + ylab("RMSE") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               # measures on the full dataset:
               geom_hline(yintercept=svm_daltonism$all_data$success$RMSE, color="red", linetype="solid", size=1) + 
               NULL,
             ncol=2);
```

The importance of the predictors is:

```{r fig.cap=capFig(paste0("Specificity-based predictor importance from SVMs using ",svm_daltonism$training_testing$replications," training/testing sets.")), fig.width=4*1, fig.height=7*1}
ggplot(svm_daltonism$training_testing$performance$pred_importance, 
       aes(x=reorder(predictor, importance, mean), y=importance)) +
  theme_bw() +
  geom_boxplot(color="black", fill="salmon") +
  labs(x="Predictors (sorted by importance)", y="Importance") +
  theme(axis.text.y = element_text(angle=30, hjust=1, size=10), 
        axis.text.x = element_text(hjust=1, size=10),
        axis.title=element_text(size=16),
        legend.position="bottom") +
  coord_flip() +
  NULL;
```



### Conclusions: daltonism, UV-B and the color vocabulary

When focusing on the evolutionary hypothesis [@brown_color_2004] linking UV-B incidence and the population frequency of red/green abnormal color perception ("daltonism"), we found that, indeed, this is supported by our data. 
The overall negative impact of UV-B is largely mediated by the existence of a dedicated word for 'blue', supporting the conjecture that already experiencing UV-B-induced blue/green perception loss generates selective pressures against red/green abnormal color perception.
Interestingly, the overall genetic distance between populations is not a good predictor of the frequency of daltonism, again suggesting that daltonism is affected by other evolutionary forces than just drift and demographic history.




# Overal conclusions and discussion

Overall, our extended dataset and more advanced methods do provide support for both hypotheses: on the one hand, the presence of a dedicated word for 'blue' is influenced negatively by the amount of UV-B incident radiation (hypothesis 1), and, on the other, that there is a negative effect of UV-B on the population frequency of red/green abnormal color perception ("daltonism") that is largely mediated by the existence of a dedicated word for 'blue' (hypothesis 2).
Moreover, our analyses suggest new findings: various proxies of "cultural complexity" (population size, subsistence strategy) seem to also have an effect on the presence of a word for blue, supporting the idea that more "complex" cultures favor more "complex" color vocabularies. 
Surprisingly, we also found that drier climates with higher seasonality may increase the probability of a dedicated word for 'blue', as does spatial closeness to large bodies of standing water (in particular, lakes), suggesting an effect of the environment on the color vocabulary probably mediated by the salience and importance of "water" and the open sky.
We also found that the overall genetic distance between populations is not a good predictor of the frequency of daltonism, suggesting that variation in daltonism between groups is not fully explained by genetic drift and demographic history, but may be under selective pressures. 

Thus, we found that the color vocabulary is shaped by environmental factors acting on the individual speakers, generating biases that are amplified by the repeated use and transmission of language in communities of similarly affected individuals.
This is akin to other such cases of bias amplification to the level of cross-linguistic diversity [@dediu_language_2017], either (presumably) rooted in genetics [@dediu_linguistic_2007;@moisik_anatomical_2017;@dediu_pushes_2019] or emerging during the lifetime of the individuals due to environmental or cultural factors [@blasi_human_2019].
But what sets this case apart, is that the cross-linguistic effects of UV-B incidence on the color vocabulary (presumably mediated by physiologic effects accrued during the life time of the speakers) are one manifestation of group-wide acquired color (green/blue) abnormal perception, the other being environmentally-mediated differential evolutionary pressures between groups against the incidence of congenial inherited abnormal color (red/green) perception.





# Acknowledgements

Thanks to the [Huma-Num](https://www.huma-num.fr/) for access to their computer cluster during the development of this analysis script.
DD was funded by an IDEXLYON (16-IDEX-0005) Fellowship grant (2018-2021), MJ by the NSCO doctoral school of Lyon. 
The project was indirectly supported by the LabEx ASLAN (ANR-10-LABX-0081) of the University of Lyon within the program Investissements d’Avenir (ANR-11-IDEX-0007) of the French National Research Agency (ANR).


# Author contributions

AM and DD designed research. 
EM extended the database and performed initial analysis. 
DD and MJ further extended the database and performed analyses.
DD, MJ and AM wrote first draft of the paper.
All authors contributed to the paper, read and approved it.


# Appendices


## Appendix I. Distance matrices

We use the *fixation Index* (F~ST~) as a measure of the genetic distance between pairs of populations, with high values indicating a high differentiation betwen populations; there are multiple approaches (see, for example, [Wikipedia](https://en.wikipedia.org/wiki/Fixation_index), @holsinger_genetics_2009), but we used here the method implemented in  [`Arlequin 3.5`](http://cmpg.unibe.ch/software/arlequin35/) [@excoffier_arlequin_2010] as detailed below. 

As *primary data*, we used a subset of the [ALFRED (The ALlele FREquency Database)](https://alfred.med.yale.edu/alfred/index.asp) [@rajeevan_alfred_2003], the [FROG-kb (Forensic Resource/Reference on Genetics knowledge base)](http://frog.med.yale.edu/FrogKB/), aimed at forensic applications.
More precisely, we downloaded (as of spring 2020) the following files:

| Panel                                           | File (hyperlink)                                                                 | N | #SNPs[^SNP] | #MHs[^Microhaplotype] |
|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------|----:|----:|----:|
| Microhaplotypes                                 | [*Microhap_alleleF_198.txt*](https://alfred.med.yale.edu/alfred/selectDownload/Microhap_alleleF_198.txt) | 96  | 0   | 198 |
| Seldin's list of 128 AISNPs                     | [*Seldin128_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Seldin128_alleleF.txt)         | 70  | 128 | 0   |
| SNPforID 34-plex                                | [*SNPForId34_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/SNPForId34_alleleF.txt)       | 53  | 34  | 0   |
| KiddLab - Set of 55 AISNPs                      | [*KiddLab55_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/KiddLab55_alleleF.txt)         | 139 | 55  | 0   |
| Kayser's set of 24 Ancestry Informative Markers | [*Kayser24_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Kayser24_alleleF.txt)           | 73  | 24  | 0   |
| Daniele Podini's list of 32 AISNPs              | [*Podini32_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Podini32_alleleF.txt)           | 111 | 29  | 0   |
| Eurasiaplex 23 SNP Panel                        | [*Eurasiaplex23_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Eurasiaplex23_alleleF.txt) | 76  | 23  | 0   |
| Nievergelt's Set of 41AIMs                      | [*Nievergelt41_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Nievergelt41_alleleF.txt)   | 123 | 41  | 0   |
| Overlap set of AISNPs                           | [*Overlap44_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Overlap44_alleleF.txt)         | 72  | 44  | 0   |
| Li's panel of 74 AIMS                           | [*Li74_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Li74_alleleF.txt)                   | 67  | 73  | 0   |

These text files were combined into a single file, and we used a custom `R` script (available online at [`mathjoss/Alfred2FST/alfredtxt2arlequin.R`](https://github.com/mathjoss/Alfred2FST/blob/master/alfredtxt2arlequin.R)) to extract one `Arlequin file` (`.arp`) per microhaplotype and per SNP. 
These `.arp` files were then processed with [`Arlequin 3.5`](http://cmpg.unibe.ch/software/arlequin35/) [@excoffier_arlequin_2010] (the parameters can be found in [mathjoss/Alfred2FST/arl_run.ars](https://github.com/mathjoss/Alfred2FST/blob/master/arl_run.ars)), using the output tables `Population average pairwise difference` (and, more precisely, the *corrected average pairwise difference* component), one per input `.arp` file. We averaged these tables using two other custom `R` scripts (available at [mathjoss/Alfred2FST/mean_all_files.R](https://github.com/mathjoss/Alfred2FST/blob/master/mean_all_files.R) and [mathjoss/Alfred2FST/mean_all_files_part2.R](https://github.com/mathjoss/Alfred2FST/blob/master/mean_all_files_part2.R)).
This procedure resulted in average pairwise distance between 145 unique populations, using 96 unique microhaplotypes and 382 unique SNP.

As there is missing data, we used the *additive* and the *ultrametric* methods of data imputation for distance matrices [@de_soete_ultrametric_1984; @lapointe_estimating_1995], followed by *Multi-Dimensional Scaling* (*MDS*); see the `R` script at [mathjoss/Alfred2FST/extract_MDS.R](https://github.com/mathjoss/Alfred2FST/blob/master/extract_MDS.R) for details. 

Finally, we mapped manually the 145 unique populations with F~ST~ distances to the 142 populations in our primary dataset `data_colors.csv`, but the mapping is far from trivial and unambiguous: for example, some ALFRED populations are rather large and imprecise (e.g., "African Americans", "Indian Mixed" ...). 
Consequently, we applied the following procedure: 

  (1) Exact population were linked to each other (e.g., "Basque" and "Yoruba" are present in both);
  (2) For populations in the primary dataset but not present in the F~ST~ distances matrix, we selected its most appropriate general category (e.g. for "Czech" we used "Europeans Mixed", and for "Gujarati", "Indian mixed");
  (3) If there is no appropriate general category, we attempted to find the closest matching population in the F~ST~ distances matrix (generally based on geographic distance and demographic history, as inferred from various sources).
  
The results of the first two matching rules (1 & 2) are in the *genetic_incomplete* column of the `data_colors.csv` primary dataset, while those of all the three rules (1, 2 & 3) are in the *genetic_full* column -- we used this "fully" matched column for further analyses.

[^SNP]: A [*Single Nucleotide Polymorphism* (*SNP*)](https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism) is a common and simple type of genetic variation representing the substitution of a single nucleotide at a specific position in the genome.

[^Microhaplotype]: A [*haplotype*](https://en.wikipedia.org/wiki/Haplotype) has several related meaning, in principle refering to a set of genetic variants that tend to be inherited together; a *microhaplotype* (microhap or *MH*) is a set of small set of closely linked SNPs useful in forensic applications [@oldoni_microhaplotypes_2019]. 


## Appendix II. Path analysis for 'blue'

The full output of this model, with all the fit indices, is:

```{r}
summary(sem_blue$model_full);
fitMeasures(sem_blue$model_full);
```

The full output of this model, with all the fit indices, and including only assumed causal arrows, is:


```{r}
summary(sem_blue$model_sure);
fitMeasures(sem_blue$model_sure);
```


## Appendix III. Path analysis for "daltonism"

The full output of this model with only the 4 essential variables, with all the fit indices, is:

```{r}
summary(sem_daltonism$model_essential);
fitMeasures(sem_daltonism$model_essential);
```

The full output of this model with only the 4 essential variables and *subsistence*, with all the fit indices, is:

```{r}
summary(sem_daltonism$model_essential_subs);
fitMeasures(sem_daltonism$model_essential_subs);
```



# Session information

```{r warning=FALSE, results='asis'}
if( require(benchmarkme) )
{
  # CPU:
  cpu_info <- benchmarkme::get_cpu();
  if( is.null(cpu_info) || is.na(cpu_info) )
  {
    cat("**CPU:** unknown.\n\n");
  } else
  {
    if( !is.null(cpu_info$model_name) && !is.na(cpu_info$model_name) )
    {
      cat(paste0("**CPU:** ",cpu_info$model_name));
      if( !is.null(cpu_info$no_of_cores) && !is.na(cpu_info$no_of_cores) )
      {
        cat(paste0(" (",cpu_info$no_of_cores," threads)"));
      }
      cat("\n\n");
    } else
    {
      cat("**CPU:** unknown.\n\n");
    }
  }
  
  # RAM:
  ram_info <- benchmarkme::get_ram();
  if( is.null(ram_info) || is.na(ram_info) )
  {
    cat("**RAM (memory):** unknown.\n\n");
  } else
  {
    cat("**RAM (memory):** "); print(ram_info); cat("\n");
  }
} else
{
  cat("**RAM (memory):** cannot get info (try installing package 'benchmarkme').\n\n");
}
```

```{r}
pander::pander(sessionInfo());
```



# References




