---
title: "Environmental influences on the evolution of the color lexicon"
subtitle: "Statistical analyses and plots"
author: "Mathilde Josserand, Emma Meeussen, Dan Dediu and Asifa Majid"
date: '`r date()`'
output:
  html_document: 
    fig_caption: yes
    highlight: textmate
    theme: readable
    toc: yes
    toc_depth: 6
    toc_float: yes
  word_document: default
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa-6th-edition.csl
---

# TODO

- rename variables/keep only the variables we need/recode variables: **DONE**
- check the `stage` variable (I guess `999` means `NA` so should not be considered as an actual value)
- add `climate_PC3` and `humidity`: **DONE**
- add UB incidence: **DONE**
- add elevation (altitude): **DONE**
- keep only the libraries we actually use: **DONE**
- integrate FST_methods.Rmd: **DONE**
- redo/expand the Bayesian regressions and model comparison
- decide what to keep to streamline the analysis
- keep only the needed input files
- do SEM/multiple mediators stuff as I did for syllable and tone

**What I also did:**

- removed `gen.dist.na` and keep only the imputed genetic distances
- removed climate and dist2... variables because I re-compute them from scratch
- fixed some language names and one glottocode (Nepali)
- reduced the precision of most numeric variables
- computed co-variates for the families as well
- I removed the old "altitude" variable because now I recompute everything (including for families) from the Mapzen data


```{r setup, echo=F, message=F, warning=FALSE}
## Knitting options:
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE,                    # default code chunk options
                      fig.width=11, fig.height=6, fig.align="center", comment=NA, # default figure dimensions
                      fig.path="./figures/",                                      # save images to ./figures/
                      dpi=72, dev="jpeg",                                         # please set dpi=300 and comment out dev="jpeg" for high resolution but very big images
                      cache=TRUE, autodep=TRUE);                                  # cache chunks


## Load needed packages:

# Rmarkdown:
library(knitr);
library(pander);

# System/parallel processing:
library(parallel);
library(future);

# Data processing:
library(plyr);
library(dplyr);

# Plotting:
library(ggplot2);
library(ggrepel);
library(gridExtra);
library(DiagrammeR); # box and arrow diagramns

# Spatial stats and maps:
library(maptools); # as.im.RasterLayer()
library(spatstat); # ppp()
library(scanstatistics); # dist_to_knn()
library(ape); # Moran.I()
library(spdep); # tri2nb()
library(deldir); # deldir()
library(igraph); # graph()
library(Imap); # gdist()
library(geosphere); # geographic distances

# Regression and mediation analysis:
library(lmerTest); # ML mixed-effects models
library(brms); # Bayesian mixed-effects models with Stan
library(bayestestR); # Bayesian model comparison
library(sjstats); # mediation
library(performance); # multicollinearity

# Decison trees, random forests, and SVMs:
library(partykit); # ctree()
#library(tree);
#library(rpart);
#library(rpart.plot);
#library(randomForest);
#library(e1071);


# Apparently not needed:
#library(gdata);
#library(maps);
#library(sna);
#library(fields);
#library(tidyr);
#library(geostatsp);
#library(MuMIn);
#library(maps);
#library(data.table);
#library(magrittr);
#library(sfsmisc);
#library(MASS);
#library(rsq);
#library(mediation);
#library(car);
#library(ROCR);
#library(ISLR)
#library(rsample);
#library(FactoMineR);


## Set seed for reproducibility:
set.seed(42);

## For Bayesian regressions with brms (some might be different for particular models to avoid too few or too many iterations):
brms_ncores  <- max(detectCores(all.tests=TRUE, logical=FALSE), 4, na.rm=TRUE); # ry to use multiple cores, if present
brms_iter    <- 5000; brms_warmup <- 2000; brms_thin <- 2; # burn-in and iterations 
brms_control <- list(adapt_delta=0.999, max_treedepth=15); # control params to pass to Stan: avoid the "divergent transitions after warmup" and "max_treedepth" warnings
brms_ci      <- 0.89; brms_rope <- c(-0.01, 0.01); # 89% HDI and a tight ROPE around 0.0 [-0.01, 0.01]
#plan(multiprocess); # parallel kfold using futures

## Various folders:
if( !dir.exists("./figures") ) dir.create("./figures", showWarnings=FALSE); # figures are saved here
if( !dir.exists("./cached_results") ) dir.create("./cached_results", showWarnings=FALSE); # cache expensive results here
if( !dir.exists("./input_files/") ) stop("The input files folder 'input_files' does not seem to exist!\n");


## Auxiliary functions:

# Figure and Table caption adapted from https://stackoverflow.com/questions/37116632/rmarkdown-html-number-figures: 
outputFormat = opts_knit$get("rmarkdown.pandoc.to"); # determine the output format of the document
if( is.null(outputFormat) ) outputFormat = ""; # probably not run within knittr
capTabNo = 1; capFigNo = 1; # figure and table caption numbering, for HTML do it manually
#Function to add the Table Number
capTab = function(x){
  if(outputFormat == 'html'){
    x = paste0("***Table ",capTabNo,".*** _",x,"_")
    capTabNo <<- capTabNo + 1
  }; x
}
#Function to add the Figure Number
capFig = function(x){
  if(outputFormat == 'html'){
    x = paste0("***Figure ",capFigNo,".*** _",x,"_")
    capFigNo <<- capFigNo + 1
  }; x
}

# Verbal interpretation of Bayes factors (BF):
BF_interpretation <- function(BF, model1_name="m1", model2_name="m2")
{
  if( BF > 100 )   return (paste0("extreme evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 30 )    return (paste0("very strong evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 10 )    return (paste0("strong evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 3 )     return (paste0("moderate evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 1 )     return (paste0("anecdotal evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF == 1 )    return (paste0("no evidence for ",model1_name," nor ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 0.33 )  return (paste0("anecdotal evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 0.10 )  return (paste0("moderate evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 0.033 ) return (paste0("strong evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 0.010 ) return (paste0("very strong evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
  return (paste0("extreme evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
}

# Here I hack brms' kfold code to make it run in parallel using good old mclapply instead of futures
# this avoid random crashes which seem to be due to future, but works only on *NIX (which, for me here, is not an issue)
# Adapted the code from https://github.com/paul-buerkner/brms/blob/master/R/loo.R and https://github.com/paul-buerkner/brms/blob/master/R/kfold.R
add_criterion_kfold_parallel <- function(model, K=10, chains=1)
{
  model <- restructure(model);

  mf <- model.frame(model); 
  attributes(mf)[c("terms", "brmsframe")] <- NULL;
  N <- nrow(mf);
  fold_type <- "random"; folds <- loo::kfold_split_random(K, N);
  Ksub <- seq_len(K);

  kfold_results <- mclapply(Ksub, function(k) 
  {
    omitted <- predicted <- which(folds == k);
    mf_omitted <- mf[-omitted, , drop=FALSE];
    
    if( exists("subset_data2", envir=asNamespace("brms")) )
    {
      # Newer versions of brms:
      model_updated <- base::suppressWarnings(update(model, newdata=mf_omitted, data2=brms:::subset_data2(model$data2, -omitted), refresh=0, chains=chains));
      
      lppds <- log_lik(model_updated, newdata=mf[predicted, , drop=FALSE], newdata2=brms:::subset_data2(model$data2, predicted), 
                       allow_new_levels=TRUE, resp=NULL, combine=TRUE, chains=chains);
    } else if( exists("subset_autocor", envir=asNamespace("brms")) )
    {
      # Older versions of brms:
      model2 <- brms:::subset_autocor(model, -omitted, incl_car=TRUE);
      model_updated <- base::suppressWarnings(update(model2, newdata=mf_omitted, refresh=0, chains=chains));
      
      lppds <- log_lik(model_updated, newdata=mf[predicted, , drop=FALSE], allow_new_levels=TRUE, resp=NULL, combine=TRUE, chains=chains);
    } else
    {
      stop("Unknonw version of brms!");
    }

    return (list("obs_order"=predicted, "lppds"=lppds));
  }, mc.cores=ifelse(exists("brms_ncores"), brms_ncores, detectCores()));
  
  # Put them back in the form expected by the the following unmodifed code:
  obs_order <- lapply(kfold_results, function(x) x$obs_order);
  lppds     <- lapply(kfold_results, function(x) x$lppds);
  
  elpds <- brms:::ulapply(lppds, function(x) apply(x, 2, brms:::log_mean_exp))
  # make sure elpds are put back in the right order
  elpds <- elpds[order(unlist(obs_order))]
  elpd_kfold <- sum(elpds)
  se_elpd_kfold <- sqrt(length(elpds) * var(elpds))
  rnames <- c("elpd_kfold", "p_kfold", "kfoldic")
  cnames <- c("Estimate", "SE")
  estimates <- matrix(nrow = 3, ncol = 2, dimnames = list(rnames, cnames))
  estimates[1, ] <- c(elpd_kfold, se_elpd_kfold)
  estimates[3, ] <- c(-2 * elpd_kfold, 2 * se_elpd_kfold)
  out <- brms:::nlist(estimates, pointwise = cbind(elpd_kfold = elpds))
  atts <- brms:::nlist(K, Ksub, NULL, folds, fold_type)
  attributes(out)[names(atts)] <- atts
  out <- structure(out, class = c("kfold", "loo"))
  
  attr(out, "yhash") <- brms:::hash_response(model, newdata=NULL, resp=NULL);
  attr(out, "model_name") <- "";
  
  model$criteria$kfold <- out;
  model;
}

# Bayesian fit indices for a given model:
brms_fit_indices <- function(model, indices=c("bayes_R2", "loo", "waic", "kfold"), K=10, verbose=TRUE)
{
  if( "bayes_R2" %in% indices )
  {
    if( verbose) cat("R^2...\n");
    #attr(model, "R2") <- bayes_R2(model); 
    model <- add_criterion(model, "bayes_R2"); 
  } else
  {
    # Remove the criterion (if already there):
    if( !is.null(model$criteria) && "bayes_R2" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "bayes_R2") ]] <- NULL;
  }
  
  if( "loo" %in% indices )
  {
    if( verbose) cat("LOO...\n");
    model <- add_criterion(model, "loo"); 
  } else
  {
    # Remove the criterion (if already there):
    if( !is.null(model$criteria) && "loo" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "loo") ]] <- NULL;
  }
  
  if( "waic" %in% indices )
  {
    if( verbose) cat("WAIC...\n");
    model <- add_criterion(model, "waic"); 
  } else
  {
    # Remove the criterion (if already there):
    if( !is.null(model$criteria) && "waic" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "waic") ]] <- NULL;
  }
  
  if( "kfold" %in% indices )
  {
    if( verbose) cat(paste0("KFOLD (K=",K,")...\n"));
    #model <- add_criterion(model, "kfold", K=K, chains=1);
    model <- add_criterion_kfold_parallel(model, K=K, chains=1);
  } else
  {
    # Remove the criterion (if already there):
    if( !is.null(model$criteria) && "kfold" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "kfold") ]] <- NULL;
  }

  gc();
  
  return (model);
}

# Bayesian model comparison:
brms_compare_models <- function(model1, model2, name1=NA, name2=NA, bayes_factor=TRUE, print_results=TRUE)
{
  if( !is.null(model1$criteria) && "bayes_R2" %in% names(model1$criteria) && !is.null(model1$criteria$bayes_R2) &&
      !is.null(model2$criteria) && "bayes_R2" %in% names(model2$criteria) && !is.null(model2$criteria$bayes_R2) )
  {
    R2_1_2 <- (mean(model1$criteria$bayes_R2) - mean(model2$criteria$bayes_R2));
  } else
  {
    R2_1_2 <- NA;
  }
  
  if( bayes_factor )
  {
    invisible(capture.output(bf_1_2 <- brms::bayes_factor(model1, model2)$bf));
    bf_interpret_1_2 <- BF_interpretation(bf_1_2, ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")); 
  }
  else
  {
    bf_1_2 <- NULL; bf_interpret_1_2 <- NA;
  }
  
  if( !is.null(model1$criteria) && "loo" %in% names(model1$criteria) && !is.null(model1$criteria$loo) &&
      !is.null(model2$criteria) && "loo" %in% names(model2$criteria) && !is.null(model2$criteria$loo) )
  {
    loo_1_2 <- loo_compare(model1, model2, criterion="loo", model_names=c(ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")));
  } else
  {
    loo_1_2 <- NA;
  }
  
  if( !is.null(model1$criteria) && "waic" %in% names(model1$criteria) && !is.null(model1$criteria$waic) &&
      !is.null(model2$criteria) && "waic" %in% names(model2$criteria) && !is.null(model2$criteria$waic) )
  {
    waic_1_2 <- loo_compare(model1, model2, criterion="waic", model_names=c(ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")));
    mw_1_2 <- model_weights(model1, model2, weights="waic", model_names=c(ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")));
  } else
  {
    waic_1_2 <- NA; 
    mw_1_2 <- NA;
  }
  
  if( !is.null(model1$criteria) && "kfold" %in% names(model1$criteria) && !is.null(model1$criteria$kfold) &&
      !is.null(model2$criteria) && "kfold" %in% names(model2$criteria) && !is.null(model2$criteria$kfold) )
  {
    kfold_1_2 <- loo_compare(model1, model2, criterion="kfold", model_names=c(ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")));
  } else
  {
    kfold_1_2 <- NA;
  }
  
  if( print_results )
  {
    cat(paste0("\nComparing models '",ifelse(!is.na(name1), name1, "model1"),"' and '",ifelse(!is.na(name2), name2, "model2"),"':\n\n"));
    cat(paste0("\ndelta R^2 = ",sprintf("%.1f%%",100*R2_1_2),"\n\n"));
    cat(bf_interpret_1_2,"\n\n");
    cat("\nLOO:\n"); print(loo_1_2);
    cat("\nWAIC:\n"); print(waic_1_2);
    cat("\nKFOLD:\n"); print(kfold_1_2);
    cat("\nModel weights (WAIC):\n"); print(mw_1_2);
    cat("\n");
  }
  
  gc();
  
  return (list("R2"=R2_1_2, "BF"=bf_1_2, "BF_interpretation"=bf_interpret_1_2, "LOO"=loo_1_2, "WAIC"=waic_1_2, "KFOLD"=kfold_1_2, "model_weights_WAIC"=mw_1_2));
}

# Standard error of the mean:
std <- function(x) sd(x)/sqrt(length(x))

# Root Mean Square Error (RMSE) between observed (y) and predicted (yrep) values:
rmse <- function(y, yrep)
{
  yrep_mean <- colMeans(yrep)
  sqrt(mean((yrep_mean - y)^2))
}

# Panel function for the pairwise plots:
panel.cor <- function(x, y){
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- cor.test(x, y, method="pearson");
  rho <- cor.test(x, y, method="spearman");
  txt <- sprintf("r=%.2f\n(p=%.4g)\nrho=%.2f\n(p=%.4g)", 
                 r$estimate, r$p.value, rho$estimate, rho$p.value)
  cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex=1.2)
}
upper.panel<-function(x, y){
  points(x,y, pch = 21, bg="lightgray", col="gray30")
  abline(lm(y ~ x), col="blue", lwd=2);
}
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "lightgray", ...)
}


## Draw diagrams:

# Draw an edge depending on its regression estimate
.gr_edge <- function(model, iv, precision=2, col_neg="blue", lty_neg="solid", col_pos="red", lty_pos="solid", col_null="gray", lty_null="dashed")
{
  if( is.null(model) )
  {
    paste0('[label = "ns", style = "',lty_null,'", color = "',col_null,'"]');
  } else
  {
    v <- fixef(model)[iv, "Estimate"];
    if( is.null(v) )
    {
      paste0('[label = "ns", style = "',lty_null,'", color = "',col_null,'"]');
    } else
    {
      paste0('[label = "',round(v, precision),'", style = "',ifelse(v<0,lty_neg,lty_pos),'", color = "',ifelse(v<0,col_neg,col_pos),'"]');
    }
  }
}

```

<style>
caption, .caption {
color: #555555;
font-weight: bold;
font-size: 105%;
text-align: left}

a[hreflang]:before{}
</style>



```{r load and prepare data, error=FALSE, include=FALSE}
# Load the world map and change it for Pacific-centered plotting:
mapWorld <- map_data("world", wrap=c(-20,340), ylim=c(-70,100));

# If the environmental variables are not already computed, compute them now:
if( !all(file.exists("./input_files/data_climate.tsv", 
                     "./input_files/data_humidity.tsv", 
                     "./input_files/data_dist2water.tsv", 
                     "./input_files/data_UV_incidence.tsv",
                     "./input_files/data_elevation.tsv")) )
{
  # Get the primary data and compute them:
  source("./00_preprocess_data.R", echo=FALSE);
}


# Load the colors data:
d_colors <- read.table("./input_files/data_colors.csv", header=TRUE, sep=",", quote='"', stringsAsFactors=FALSE); # comma-separated double-quoted CVS file

# Add the various info, both for the languages and for the families.
# For humidity, keep only the mean annual median and IQR as all the other measures are very highly correlated with these
# For UV, do not use the individual freqyency bands, but the wide categories "A" and "B" (and overall):
d_climate    <- read.table("./input_files/data_climate.tsv",      header=TRUE, sep="\t", quote="");
d_humidity   <- read.table("./input_files/data_humidity.tsv",     header=TRUE, sep="\t", quote="");
d_dist2water <- read.table("./input_files/data_dist2water.tsv",   header=TRUE, sep="\t", quote="");
d_uv         <- read.table("./input_files/data_UV_incidence.tsv", header=TRUE, sep="\t", quote="");
d_elevation  <- read.table("./input_files/data_elevation.tsv",    header=TRUE, sep="\t", quote="");
d_genetics   <- read.table("./input_files/data_genetics.tsv",     header=TRUE, sep="\t", quote="");

# for languages ...
d_colors <- merge(d_colors, d_climate[,c("glottocode", "clim_PC1", "clim_PC2", "clim_PC3")], by="glottocode", all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_humidity[,c("glottocode", "humidity_mean_annual_median", "humidity_mean_annual_IQR")], by="glottocode", all.x=TRUE, all.y=FALSE);
names(d_colors)[ncol(d_colors)-c(1,0)] <- c("hum_median", "hum_IQR");
d_colors <- merge(d_colors, d_dist2water[,c("glottocode", "dist2ocean", "dist2lakes", "dist2rivers", "dist2water")], by="glottocode", all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_uv[,c("glottocode", "UV_A_mean", "UV_A_sd", "UV_B_mean", "UV_B_sd", "UV_mean", "UV_sd")], by="glottocode", all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_elevation[,c("glottocode", "elevation")], by="glottocode", all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_genetics, by="glottocode", all.x=TRUE, all.y=FALSE);

# ... and for families
d_colors <- merge(d_colors, d_climate[,c("glottocode", "clim_PC1", "clim_PC2", "clim_PC3")], by.x="glottocode_family", by.y="glottocode", suffixes=c("","_family"), all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_humidity[,c("glottocode", "humidity_mean_annual_median", "humidity_mean_annual_IQR")], by.x="glottocode_family", by.y="glottocode", suffixes=c("","_family"), all.x=TRUE, all.y=FALSE);
names(d_colors)[ncol(d_colors)-c(1,0)] <- c("hum_median_family", "hum_IQR_family");
d_colors <- merge(d_colors, d_dist2water[,c("glottocode", "dist2ocean", "dist2lakes", "dist2rivers", "dist2water")], by.x="glottocode_family", by.y="glottocode", suffixes=c("","_family"), all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_uv[,c("glottocode", "UV_A_mean", "UV_A_sd", "UV_B_mean", "UV_B_sd", "UV_mean", "UV_sd")], by.x="glottocode_family", by.y="glottocode", suffixes=c("","_family"), all.x=TRUE, all.y=FALSE);
d_colors <- merge(d_colors, d_elevation[,c("glottocode", "elevation")], by.x="glottocode_family", by.y="glottocode", suffixes=c("","_family"), all.x=TRUE, all.y=FALSE);

# Rescale variables (we use the suffix _r for them):
# - elevation: log(+1) because there are some elevations set to 0 meters, so increase everything by 1m to avoid -Inf:
d_colors$elevation_r          <- log2(d_colors$elevation + 1);
d_colors$elevation_family_r   <- log2(d_colors$elevation_family + 1);
# - distance to water bodies: log() as distances to water are never 0.0, so no need to guard against -Inf here:
d_colors$dist2water_r         <- log2(d_colors$dist2water);
d_colors$dist2rivers_r        <- log2(d_colors$dist2rivers);
d_colors$dist2ocean_r         <- log2(d_colors$dist2ocean);
d_colors$dist2lakes_r         <- log2(d_colors$dist2lakes);
d_colors$dist2water_family_r  <- log2(d_colors$dist2water_family);
d_colors$dist2rivers_family_r <- log2(d_colors$dist2rivers_family);
d_colors$dist2ocean_family_r  <- log2(d_colors$dist2ocean_family);
d_colors$dist2lakes_family_r  <- log2(d_colors$dist2lakes_family);
# - longitude and latitude: cosine (degrees -> radians):
d_colors$latitude_r           <- cos(d_colors$latitude*(pi/180));
d_colors$longitude_r          <- cos(d_colors$longitude*(pi/180));
d_colors$latitude_family_r    <- cos(d_colors$latitude_family*(pi/180));
d_colors$longitude_family_r   <- cos(d_colors$longitude_family*(pi/180));
# - UV incidence: z-score:
d_colors$UV_A_mean_r          <- scale(d_colors$UV_A_mean);
d_colors$UV_A_sd_r            <- scale(d_colors$UV_A_sd);
d_colors$UV_B_mean_r          <- scale(d_colors$UV_B_mean);
d_colors$UV_B_sd_r            <- scale(d_colors$UV_B_sd);
d_colors$UV_mean_r            <- scale(d_colors$UV_mean);
d_colors$UV_sd_r              <- scale(d_colors$UV_sd);
d_colors$UV_A_mean_family_r   <- scale(d_colors$UV_A_mean_family);
d_colors$UV_A_sd_family_r     <- scale(d_colors$UV_A_sd_family);
d_colors$UV_B_mean_family_r   <- scale(d_colors$UV_B_mean_family);
d_colors$UV_B_sd_family_r     <- scale(d_colors$UV_B_sd_family);
d_colors$UV_mean_family_r     <- scale(d_colors$UV_mean_family);
d_colors$UV_sd_family_r       <- scale(d_colors$UV_sd_family);
# - climate: z-score:
d_colors$clim_PC1_r           <- scale(d_colors$clim_PC1);
d_colors$clim_PC2_r           <- scale(d_colors$clim_PC2);
d_colors$clim_PC3_r           <- scale(d_colors$clim_PC3);
d_colors$clim_PC1_family_r    <- scale(d_colors$clim_PC1_family);
d_colors$clim_PC2_family_r    <- scale(d_colors$clim_PC2_family);
d_colors$clim_PC3_family_r    <- scale(d_colors$clim_PC3_family);
# - daltonism:
d_colors$daltonism_r          <- max(d_colors$daltonism/100, 0.0001); # prepare for Beta regression: transform it from % into proportions, and replace 0 by a very small proportion (0.01%)
# - number of color categories: standardize to 0..1:
d_colors$n_colors_r           <- (d_colors$n_colors - min(d_colors$n_colors,na.rm=TRUE)) / (max(d_colors$n_colors,na.rm=TRUE) - min(d_colors$n_colors,na.rm=TRUE));
# - genetics: z-score:
s <- grep("gen_D",names(d_colors),fixed=TRUE); for( i in s ){ d_colors <- cbind(d_colors, scale(d_colors[,i])); names(d_colors)[ncol(d_colors)] <- paste0(names(d_colors)[i],"_r"); }

# Alternative view of the globe: all longitudes > 180 are "flipped" to negative values:
d_colors$longitude_180        <- ifelse(d_colors$longitude <= 180,        d_colors$longitude,        d_colors$longitude - 360);
d_colors$longitude_family_180 <- ifelse(d_colors$longitude_family <= 180, d_colors$longitude_family, d_colors$longitude_family - 360);

# Ensure factors have the right levels and contrasts:
d_colors$exists_blue <- factor(d_colors$exists_blue, levels=c("no", "yes"));
d_colors$subsistence <- factor(d_colors$subsistence, levels=c("HG", "AGR"));

# Collapse the families with few languages into the "Other"category (for plotting purposes):
d_colors$glottocode_family_other <- ifelse(d_colors$glottocode_family %in% c("abkh1242", "ainu1252", "araw1281", "ayma1253", 
                                                                             "basq1248", "chib1249", "hadz1240", "japo1237", 
                                                                             "jiva1245", "kore1284", "otom1299", "pama1250", 
                                                                             "pano1259", "taik1256", "ticu1244", "tupi1275", 
                                                                             "utoa1244", "yano1268"),
                                           "Other", 
                                           as.character(d_colors$glottocode_family));

# Just the language families data:
d_colors_families <- unique(d_colors[, grep("_family", names(d_colors), fixed=TRUE) ]);

# Just the coordinates:
d_coords <- cbind(d_colors[,c("glottocode_family", "latitude_family", "longitude_family", "longitude_family_180")], "type"="family");
names(d_coords) <- c("glottocode", "latitude", "longitude", "longitude_180", "type");
d_coords <- rbind(cbind(d_colors[,c("glottocode", "latitude", "longitude", "longitude_180")], "type"="language"), d_coords);
d_coords <- unique(d_coords);

 
# The Delaunay triangulation:
# This is based on code from Cysouw, M., Dediu, D., & Moran, S. (2012). Comment on “Phonemic Diversity Supports a Serial Founder Effect Model of Language Expansion from Africa.” Science, 335(6069), 657. https://doi.org/10.1126/science.1208841, 
# updated to run on R 3.6 by Marc Tang, and massively changed to accommodate more dense data by Dan Dediu:

# The barriers that nearest-neighbor links cannot cross:
.generate_barrier <- function(p, n_points=100) # p is a vector of (x,y) coordinates, i.e p=c(x1,y1, x2,y2, ... xn,yn)
{
  if( length(p) < 2 || length(p) %%2 != 0 ) stop("p must be of the form c(x1,y1, x2,y2, ... xn,yn).");
  t <- seq(0, 1, length.out=n_points);
  i <- 1; x <- y <- c();
  while( i <= length(p)-2 )
  {
    x <- c(x, t*p[i] + (1-t)*p[i+2]);
    y <- c(y, t*p[i+1] + (1-t)*p[i+3]);
    i <- i+2;
  }
  return (data.frame("longitude"=x, "latitude"=y));
}
geo_barriers <- rbind(.generate_barrier(c(-20,35.92, -5.65,35.92, -5.20,36.03, -1.32,36.29, 5.62,37.94, 11.32,37.72, 15.45,34.91, 32.82,32.68, 35.14, 35.53)), # Meditterean
                      .generate_barrier(c(34.30,43.47, 41.25,41.61)), # Black sea
                      .generate_barrier(c(52.90,46.27, 49.06,44.80, 51.47,40.20, 51.57,36.87)), # Caspian sea
                      .generate_barrier(c(59.00,17.59, 59.00,-50.00)), # Indian ocean (Africa)
                      .generate_barrier(c(89.34,21.04, 89.34,-20.00)), # Indian ocean (India)
                      .generate_barrier(c(89.34,-20.00, 126.42,-12.44, 131.42,-9.79, 140.00,-10.18)),  # Australia (Sunda)
                      .generate_barrier(c(157.93,-26.00, 157.93,-50.00)),               # Australia (Pacific)
                      .generate_barrier(c(124.55,29.58, 127.87,23.27, 134.33,30.00, 143.50,33.00)), # Pacific rim (1)
                      .generate_barrier(c(-145.00,30.00, -83.00,-22.00, -82.00,-50.00)), # Pacific rim (2)
                      .generate_barrier(c(-96.60,25.69, -80.59,24.17, -74.43,21.58, -68.72,20.29, -19.99,20.29)), # Caribbean
                      .generate_barrier(c(-168.99,68.03, -168.99,80.00)), # Chukchi sea
                      .generate_barrier(c(55.89,69.30, 46.80,71.78, 46.80,80.00)), # Arctic
                      .generate_barrier(c(3.50,70.00, 3.50,80.00)), # North Atlantic
                      .generate_barrier(c(10.70,-5.58, -15.00,-19.00)), # Africa (Gulf of Guinea)
                      .generate_barrier(c(-65.22,-45.25, -35.00,-45.25)), # Tierra del Fuego
                      NULL
);
geo_barriers$longitude <- ifelse( !is.na(geo_barriers$longitude) & geo_barriers$longitude < -20, 360 + geo_barriers$longitude, geo_barriers$longitude ); # Pacific-centered

tmp <- deldir(d_coords$longitude[d_coords$type == "language"], d_coords$latitude[d_coords$type == "language"], # the languages
              dpl=list(x=geo_barriers$longitude, y=geo_barriers$latitude), suppressMsge=TRUE);                 # "boundaries" as dummy points


# Remove the connections to the dummy points (the "boundaries"):
delaunay_triang <- tmp$delsgs;
delaunay_triang <- delaunay_triang[ delaunay_triang$ind1 <= tmp$n.data & delaunay_triang$ind2 <= tmp$n.data, ]; # keep only the actual language points (& deal with duplication)

# Extract the network data:
delaunay_edges <- rbind(delaunay_triang[,c(5,6)], delaunay_triang[,c(6,5)]);
delaunay_edges <- cbind(delaunay_edges, rep(1, times=dim(delaunay_edges)[1]));
attr(delaunay_edges,"n")      <- sum(d_coords$type == "language");
attr(delaunay_edges,"vnames") <- d_coords$glottocode;

# Create the list of vertices for the network:
vertices <- unname(unlist(as.list(data.frame(t(delaunay_triang[,c("ind1","ind2")]), stringsAsFactors=FALSE))));
delaunay_graph <- igraph::graph(edges=vertices, n=sum(d_coords$type == "language"), directed=FALSE);

# Extract all the neighbors:
neighbor_list <- igraph::ego(delaunay_graph);
neighbor_list <- plyr::ldply(neighbor_list, rbind);
colnames(neighbor_list) <- c("glottocode", paste0("Delaunay_N",1:(ncol(neighbor_list)-1)));
for( i in 1:ncol(neighbor_list) ) neighbor_list[,i] <- d_coords$glottocode[ neighbor_list[,i] ]; # use the glottocodes

# ... and merge them with the data:
d_colors <- merge(d_colors, neighbor_list, by="glottocode", all.x=TRUE, all.y=FALSE); # NA is used if there are no neighbours >= N


# Point Pattern Analysis:
# Store x and y coords in two vectors:
longitudes <- d_colors$longitude; latitudes <- d_colors$latitude;

# The spatial ranges:
xrange <- range(longitudes, na.rm=TRUE); yrange <- range(latitudes, na.rm=TRUE);

# Create ppp's:
ppp_languages           <- ppp(longitudes, latitudes, xrange, yrange); # without marks
ppp_languages_daltonism <- ppp(longitudes, latitudes, xrange, yrange, marks=d_colors$daltonism); # with daltonism as marks
ppp_languages_blue      <- ppp(longitudes, latitudes, xrange, yrange, marks=d_colors$exists_blue); # with exists_blue as marks
ppp_languages_blue_num  <- ppp(longitudes, latitudes, xrange, yrange, marks=as.numeric(d_colors$exists_blue)); # with exists_blue (as numbers) as marks

# Convert from the ppp format to the sp format:
sp_languages              <- data.frame(longitude=d_colors$longitude_180, latitude=latitudes);
coordinates(sp_languages) <- ~ longitude + latitude;
proj4string(sp_languages) <- CRS("+proj=longlat +datum=WGS84");
sp_languages$daltonism    <- d_colors$daltonism;
sp_languages$exists_blue  <- d_colors$exists_blue;


# Geographic and Delanunay neightbour distances:
# The shortest pairwise distances on the WGS84 ellipsoid in Km:
d_geo_dists <- matrix(NA, nrow=nrow(d_colors), ncol=nrow(d_colors), dimnames=list(d_colors$glottocode, d_colors$glottocode));
for( i in 1:(ncol(d_geo_dists)-1) )
{
  for( j in (i+1):ncol(d_geo_dists) )
  {
    d_geo_dists[i,j] <- d_geo_dists[j,i] <- distGeo(p1=d_colors[i,c("longitude_180", "latitude")], p2=d_colors[j,c("longitude_180", "latitude")]) / 1000.0;
  }
}
diag(d_geo_dists) <- 0.0;

# The inverse distance matrix:
d_geo_dists_inv <- 1.0 / d_geo_dists; diag(d_geo_dists_inv) <- 0.0;

# Delaunay neighbours:
nb_delaunay <- tri2nb(sp_languages);
for(i in 1:nrow(d_colors))
{
  nb_delaunay[[i]] <- which(d_colors$glottocode %in% na.omit(as.character(d_colors[i, grep("Delaunay_N", colnames(d_colors), fixed=TRUE)])));
}
nb_delaunay_spweights <- nb2listw(nb_delaunay, style='B') # convert it to spatial weights
#plot(nb_delaunay, d_colors[,c("longitude","latitude")]); # check it
#plot(nb_delaunay_spweights, d_colors[,c("longitude","latitude")]); # check it
```


# Data

This dataset is built starting from the data accompanying @brown_color_2004, to which Emma Meeussen added several new populations (and checked the pre-existing coding) using a variety of primary and secondary sources (see @meeussen_colour_2015 for details), supplemented with several environmental variables by Mathilde Josserand (see @josserand_speaking_2020).


## Populations and languages

In our dataset, there are `r length(unique(d_colors$glottocode))` populations, each uniquely identified by the [Glottolog](https://glottolog.org/) code (the *glottocode*) of the primary language they speak (the matching was done manually).


## Geographic location

The geographic coordinates of the populations were retrieved from the [Glottolog](https://glottolog.org/) based on their *glottocode*s.
As per @brown_color_2004, we computed the cosine of these *latitude*s and the *longitude*s to be used in the statistical models; `cos(latitude)` captures the closeness to the equator, ranging between 0.0 (one of the poles) and 1.0 (the equator), and `cos(longitude)`range between -1.0 and 1.0 (corresponding to -180 and 180 degrees, respectively); 
please note that longitude can be coded (and plotted) either as ranging between 0° and 360°, or between -180° and +180°, or, when we need to avoid the International Date Line (IDL) producing an artefactual boundary, we center the work on the Pacific.

```{r populations map, fig.cap=capFig("Map of the populations in our sample (with language names).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, aes(x=longitude, y=latitude), color="black") +
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=L1), color="blue", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  NULL;
```


## Elevation

We obtained elevation (altitude, in meters) using [Mapzen](https://en.wikipedia.org/wiki/Mapzen) data, which is still available (July 2020) on the [Terrain Tiles on Amazon Web Services](https://aws.amazon.com/public-datasets/terrain/) and accessible through the [`elevatr` package](https://cran.r-project.org/web/packages/elevatr/vignettes/introduction_to_elevatr.html#get_raster_elevation_data); please note that for the analyses, we use `log(elevation + 1)` (we added 1m to avoid errors for the locations recorded at sea level, 0m).

```{r summary elevation}
pander(summary(d_colors$elevation));
```

```{r distribution of elevation, fig.cap=capFig("Distribution of elevation.")}
ggplot(d_colors, aes(x=elevation)) +
  theme_bw() +
  geom_histogram(color="tomato", fill="tomato", alpha=0.5)+
  ggtitle("Distribution of elevation") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map elevation, fig.cap=capFig("Map of elevation (color scale).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=elevation), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=elevation), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Elevation (m)") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```


## Climate and ecology 

We reused the code from @bentz_evolution_2018 to extract historical data on global weather and climate from [WorldClim](https://www.worldclim.org/) for the period 1960-1990, encoded in 19 variables covering various measures (such as temperature, seasonality or precipitation). 
As in @bentz_evolution_2018, we conduced a A Principal Component Analysis (PCA) for our data only, and we found similar results, namely that the first two principal components (PCs) explain most of the data and have meaningful interpretations: the PC1 explains 49.7% of the variance and reflects low seasonality, wet and hot climate, whereas PC2 explains 24.7% of the variance and reflects high seasonality, hot and dry climate; PC3 explains only 8.6% and its interpretation is less straightforward; see @bentz_evolution_2018 for details (please note that the sign of the PCs is arbitrary). 
For the analyses, these variables were z-scored.


### Climate PC1

```{r summary PC1}
# Climate PC1
pander(summary(d_colors$clim_PC1))
```

```{r distribution of PC1, fig.cap=capFig("Distribution of climate PC1.")}
ggplot(d_colors, aes(x=clim_PC1)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  ggtitle("Distribution of climate PC1") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map PC1, fig.cap=capFig("Map of climate PC1 (color scale).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=clim_PC1), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Climate PC1") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```

### Climate PC2

```{r summary PC2}
# Climate PC2
pander(summary(d_colors$clim_PC2))
```

```{r distribution of PC2, fig.cap=capFig("Distribution of climate PC2.")}
ggplot(d_colors, aes(x=clim_PC2)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  ggtitle("Distribution of climate PC2") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map PC2, fig.cap=capFig("Map of climate PC2 (color scale).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=clim_PC2), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Climate PC2") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```

### Climate PC3

```{r summary PC3}
# Climate PC3
pander(summary(d_colors$clim_PC3))
```

```{r distribution of PC3, fig.cap=capFig("Distribution of climate PC3.")}
ggplot(d_colors, aes(x=clim_PC3)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  ggtitle("Distribution of climate PC3") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map PC3, fig.cap=capFig("Map of climate PC3 (color scale).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=clim_PC3), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Climate PC3") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```


## Humidity 

We obtained specific humidity ("the mass of water vapour in a unit mass of moist air, usually expressed as grams of vapour per kilogram of air"; [Encyclopaedia Britannica](https://www.britannica.com/science/specific-humidity)) data from the [NOAA](http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP-NCAR/.CDAS-1/.MONTHLY/.Diagnostic/.above_ground/.qa/datafiles.html), given as monthly extractions between 1 January 1949 and the download date, and we computed the overall mean, median, standard deviation and IQR across all measurements, as well as the mean across the yearly means, medians, standard deviations and IQRs.
However, all these measures are highly correlated by type (see plots below), such that we only retain here the mean of the yearly medians and IQRs.
We did not transform these variables for the analyses.

```{r echo=FALSE, message=FALSE, fig.cap=capFig("Pairwise correlations between measures of central tendency for humidity."), fig.width=8, fig.height=8}
pairs(d_humidity[,c("humidity_overall_mean", "humidity_overall_median", "humidity_mean_annual_mean", "humidity_mean_annual_median")], 
      lower.panel=panel.cor, diag.panel=panel.hist, upper.panel=upper.panel); 
```

```{r echo=FALSE, message=FALSE, fig.cap=capFig("Pairwise correlations between measures of dispersion for humidity."), fig.width=8, fig.height=8}
pairs(d_humidity[,c("humidity_overall_sd", "humidity_overall_IQR", "humidity_mean_annual_sd", "humidity_mean_annual_IQR")], 
      lower.panel=panel.cor, diag.panel=panel.hist, upper.panel=upper.panel); 
```

### Median (mean of yearly medians)

```{r summary median humidity}
pander(summary(d_colors$hum_median))
```

```{r distribution of median humidity, fig.cap=capFig("Distribution of median humidity.")}
ggplot(d_colors, aes(x=hum_median)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  ggtitle("Distribution of median humidity") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map median humidity, fig.cap=capFig("Map of median humidity (color scale).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=hum_median), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Median humidity") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```

### Variation (mean of yearly IQRs)

```{r summary IQR humidity}
pander(summary(d_colors$hum_IQR))
```

```{r distribution of IQR humidity, fig.cap=capFig("Distribution of variation in humidity (IQR).")}
ggplot(d_colors, aes(x=hum_IQR)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  ggtitle("Distribution of variation in humidity (IQR)") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map IQR humidity, fig.cap=capFig("Map of variation in humidity (IQR) (color scale).")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=hum_IQR), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "IQR humidity") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```


## Distance to bodies of water

We reused the code from @bentz_evolution_2018 to compute the distances from each of our populations to the nearest lake, ocean, river, and water in general, using [OpenStreetMap](http://openstreetmapdata.com/) raster files[^openstreetmaps]. 
For the analyses, we use the `log` of these distances.


### Distance to lakes

```{r summary dist lakes}
# Distance to lakes
pander(summary(d_colors$dist2lakes));
```

```{r distribution dist lakes, fig.cap=capFig("Distribution of distances to lakes.")}
ggplot(d_colors, aes(x=dist2lakes)) +
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  #ggtitle("Distribution of distances to lakes") +
  xlab("Dist. to lakes (km)") + 
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map dist lakes, fig.cap=capFig("Map of distances to lakes (color scale).")}
ggplot() + 
  theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=round(dist2lakes,0)), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=dist2lakes), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Dist. to lakes (km)") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```

### Distance to rivers

```{r summary dist rivers}
# Distance to lakes
pander(summary(d_colors$dist2rivers));
```

```{r distribution dist rivers, fig.cap=capFig("Distribution of distances to rivers.")}
ggplot(d_colors, aes(x=dist2rivers)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  #ggtitle("Distribution of log distance to rivers") +
  xlab("Dist. to rivers (km)") + 
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map dist rivers, fig.cap=capFig("Map of distances to rivers (color scale).")}
ggplot() + 
  theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=round(dist2rivers,0)), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=dist2rivers), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Dist. to rivers (km)") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```


### Distance to oceans

```{r summary dist oceans}
# Distance to oceans
pander(summary(d_colors$dist2ocean));
```

```{r distribution dist oceans, fig.cap=capFig("Distribution of distances to oceans.")}
ggplot(d_colors, aes(x=dist2ocean)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  #ggtitle("Distribution of log distance to oceans") +
  xlab("Dist. to oceans (km)") + 
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map dist oceans, fig.cap=capFig("Map of distances to oceans (color scale).")}
ggplot() + 
  theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=round(dist2ocean,0)), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=dist2ocean), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Dist. to oceans (km)") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```

### Distance to water

```{r summary dist water}
# Distance to water
pander(summary(d_colors$dist2water));
```

```{r distribution dist water, fig.cap=capFig("Distribution of distances to water.")}
ggplot(d_colors, aes(x=dist2water)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  #ggtitle("Distribution of log distance to v") +
  xlab("log dist water (km)") + 
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map dist water, fig.cap=capFig("Map of distances to water (color scale).")}
ggplot() + 
  theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=round(dist2water,0)), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=dist2water), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Dist. to water (km)") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```


## UV incidence

The incidence of ultra-violet light (UV) was calculated from the data available from the [NASA Total Ozone Mapping Spectrometer (TOMS)](http://toms.gsfc.nasa.gov/ery_uv/new_uv/)[^uv] for the year 1998, in order to replicate the procedure in @brown_color_2004. 
These data contain daily measures of UV radiation received by the human body at four wavelengths (305 nm, 310 nm, 320 nm, and 380 nm) in Joules per square meter (J/m^2^), taking into account the thickness of the ozone layer in the stratosphere, the amount of cloud cover, the elevation, and how high the sun is in the sky. 
Here, we computed the mean and standard deviation for the whole year for UV-A (315 nm to 400 nm), for UV-B (280 nm to 315 nm) and for the full spectrum; these were further z-scored for the statistical analyses.

### UV-A

Summaries for mean and standard deviation:
```{r summary UV-A}
pander(summary(d_colors$UV_A_mean))
pander(summary(d_colors$UV_A_sd))
```

```{r distribution UV-A, fig.cap=capFig("UV-A (mean and standard deviation).")}
grid.arrange(ggplot(d_colors, aes(x=UV_A_mean)) + 
               theme_bw() +
               geom_density(color="tomato", fill="tomato", alpha=0.5)+
               ggtitle("Distribution of mean UV-A") +
               theme(plot.title = element_text(size = 10, face = "bold")),
             ggplot(d_colors, aes(x=UV_A_sd)) + 
               theme_bw() +
               geom_density(color="tomato", fill="tomato", alpha=0.5)+
               ggtitle("Distribution of sd UV-A") +
               theme(plot.title = element_text(size = 10, face = "bold")),
             ncol=2);
```

```{r map UV-A, fig.cap=capFig("Map of UV-A (mean and standard deviation)."), fig.height=12}
grid.arrange(ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=UV_A_mean), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "Mean UV-A") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=UV_A_sd), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "SD UV-A") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ncol=1);
```

### UV-B

Summaries for mean and standard deviation:
```{r summary UV-B}
pander(summary(d_colors$UV_B_mean))
pander(summary(d_colors$UV_B_sd))
```

```{r distribution UV-B, fig.cap=capFig("UV-B (mean and standard deviation).")}
grid.arrange(ggplot(d_colors, aes(x=UV_B_mean)) + 
               theme_bw() +
               geom_density(color="tomato", fill="tomato", alpha=0.5)+
               ggtitle("Distribution of mean UV-B") +
               theme(plot.title = element_text(size = 10, face = "bold")),
             ggplot(d_colors, aes(x=UV_B_sd)) + 
               theme_bw() +
               geom_density(color="tomato", fill="tomato", alpha=0.5)+
               ggtitle("Distribution of sd UV-B") +
               theme(plot.title = element_text(size = 10, face = "bold")),
             ncol=2);
```

```{r map UV-B, fig.cap=capFig("Map of UV-B (mean and standard deviation)."), fig.height=12}
grid.arrange(ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=UV_B_mean), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "Mean UV-B") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=UV_B_sd), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "SD UV-B") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ncol=1);
```

### UV (overall)

Summaries for mean and standard deviation:
```{r summary UV}
pander(summary(d_colors$UV_mean))
pander(summary(d_colors$UV_sd))
```

```{r distribution UV, fig.cap=capFig("UV (mean and standard deviation).")}
grid.arrange(ggplot(d_colors, aes(x=UV_mean)) + 
               theme_bw() +
               geom_density(color="tomato", fill="tomato", alpha=0.5)+
               ggtitle("Distribution of mean UV-B") +
               theme(plot.title = element_text(size = 10, face = "bold")),
             ggplot(d_colors, aes(x=UV_sd)) + 
               theme_bw() +
               geom_density(color="tomato", fill="tomato", alpha=0.5)+
               ggtitle("Distribution of sd UV-B") +
               theme(plot.title = element_text(size = 10, face = "bold")),
             ncol=2);
```

```{r map UV, fig.cap=capFig("Map of UV (mean and standard deviation)."), fig.height=12}
grid.arrange(ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=UV_mean), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "Mean UV") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ggplot() + theme_bw() +
               geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
               geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=UV_sd), color="black") +
               theme(legend.position = c(0.75, 0.5), 
                     legend.justification = c(1, 1), 
                     legend.title = element_text(size = 9), 
                     legend.text = element_text(size = 10)) +
               labs(fill = "SD UV") +
               scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
               NULL,
             ncol=1);
```



## Log of population size

The log of population size was obtained from @bentz_evolution_2018.

```{r summary popsize}
# population size
pander(summary(d_colors$log_popSize))
```

```{r distribution popsize, fig.cap=capFig("Log population size.")}
ggplot(d_colors, aes(x=log_popSize)) + 
  theme_bw() +
  geom_density(color="tomato", fill="tomato", alpha=0.5)+
  #ggtitle("Distribution of population size (log)") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map popsize, fig.cap=capFig("Map of log population size.")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=round(log_popSize,1)), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=log_popSize), color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Log(pop. size)") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;
```


## Subsistence strategy

Subsistence strategy was obtained mainly from [AUTOTYP](http://www.autotyp.uzh.ch/) [@bickel_autotyp_2017] as coded in [@blasi_human_2019], supplemented with information from other databases such as [D-Place](https://d-place.org/) [@kirby_dplace_2016] and [Seshat](http://seshatdatabank.info/) [@turchin_seshat_2015]. 
This is represented by the binary variable *subsistence* with values "HG" for populations whose subsistence mode is based on hunting, fishing, gathering and/or foraging, and "AGR" for populations with subsistence modes centered around food production. 

```{r summary subsistence}
pander(summary(d_colors$subsistence))
```

```{r distribution subsistence, fig.cap=capFig("Subsistence strategy.")}
ggplot(data =d_colors)+
  theme_bw() +
  geom_bar(aes(x=as.factor(subsistence), fill=as.factor(subsistence)), alpha=0.75) + 
  labs(x="Hunting-gathering subsistence strategy?", fill="")+
  guides(fill=FALSE) +
  scale_fill_viridis_d()
```

```{r map subsistence, fig.cap=capFig("Map of subsistence strategy.")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data= d_colors, shape=21, aes(x = longitude, y = latitude, fill=subsistence), color="red") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "subsistence?") +
  scale_fill_viridis_d()
```


## Color vocabulary

@brown_color_2004 and @meeussen_colour_2015 have collected information about the color vocabularies according to the Basic Color Categories of @berlin_basic_1991. 
Here, we used only two variables: the *size of the color lexicon* (the variable *n_colors*, varying in our data between `r min(d_colors$n_colors, na.rm=TRUE)` and `r max(d_colors$n_colors, na.rm=TRUE)`), and the *presence of a specific term for blue* (variable *exists_blue* with values "yes" or "no").

### Number of color categories

```{r summary no color categories}
# Visualize data
pander(summary(d_colors$n_colors))
```

```{r distribution no color categories, fig.cap=capFig("Number of color categories.")}
# DISTRIBUTION color vocab
ggplot(data =d_colors)+
  theme_bw() +
  geom_bar(aes(x=as.factor(n_colors), fill=as.factor(n_colors)), alpha=0.75) + 
  labs(x="# color categories", fill="")+
  guides(fill=FALSE)
```

```{r map no color categories, fig.cap=capFig("Map of color categories (color scale).")}
# plot Colar vocab on map
ggplot() + theme_bw() + 
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=n_colors), color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data= d_colors, aes(x = longitude, y = latitude, fill=n_colors), shape = 21, color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  labs(fill = "# color categories") 
```

### Specific term for "blue"

```{r summary word for blue}
# Visualize data
pander(summary(d_colors$exists_blue))
```

```{r distribution word for blue, fig.cap=capFig('Is there a specific term for "blue"?')}
# DISTRIBUTION color vocab
ggplot(data =d_colors)+
  theme_bw() +
  geom_bar(aes(x=as.factor(exists_blue), fill=as.factor(exists_blue)), alpha=0.75) + 
  labs(x='Is there a specific term for "blue"?', fill="")+
  guides(fill=FALSE) +
  scale_fill_viridis_d()
```

```{r map word for blue, fig.cap=capFig("Map of specific terms for blue.")}
# plot exists_blue on map
ggplot() + theme_bw() + 
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data= d_colors, shape=21, aes(x = longitude, y = latitude, fill=exists_blue), color="red") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = 'Term for "blue"?') +
  scale_fill_viridis_d() + 
  NULL

```


## Abnormal red/green color perception

Data on the incidence of abnormal red/green color perception for males was collected from 85 references, using the Ishihara test [@ishihara_tests_1917], the anomaloscope [@knoll_anomaloscope_1968], the [Holmgren-Thomson wool test](https://www.psych.utoronto.ca/sites/www.psych.utoronto.ca/files/Professor%20Holmgren%E2%80%99s%20Test%20For%20Colour%20Blindness.pdf) [@thomson_instrument_1880], or the [Hardy-Rand-Rittler pseudoisochromatic plate test](https://www.good-lite.com/Details.cfm?ProdID=107) [@hardy_hrr_1954]; see @meeussen_colour_2015 and @josserand_speaking_2020 for details. 
These data represent the percent of red/green "color blind" males in the population, and, for our data, varies between 0% and 11%. 
Overall "color blindness" rates were used, unless specific information was available; we include only *deuteranopia*, *deuteranomaly*, *protanopia* and *protanomaly*, and we specifically did not include data on *tritanopia* (as this concerns abnormal color perception in the yellow/blue range). 
We excluded from the analyses samples which did not distinguish between male and female, or which had data for less than 50 individuals.
We named here this variable *daltonism*, but, while this is a very short and evocative variable name, as discussed above, we emphatically also considere milder forms of red/green abnormal color perception.

```{r summary daltonism}
pander(summary(d_colors$daltonism))
```

```{r distribution of daltonism, fig.cap=capFig("Incidence of red/green abnormal color perception.")}
# Plot
ggplot(d_colors, aes(x=daltonism)) + 
  theme_bw() +
  geom_histogram(color="tomato", fill="tomato", alpha=0.5)+
  xlab("Incidence of red/green abnormal color perception (%)") +
  #ggtitle("Distribution of daltonism") +
  theme(plot.title = element_text(size = 10, face = "bold")); 
```

```{r map word for daltonism, fig.cap=capFig("Map of incidence of red/green abnormal color perception (color scale).")}
ggplot() + theme_bw() + 
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=paste0(round(daltonism,1),"%")), 
                   color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data= d_colors, aes(x = longitude, y = latitude, fill=daltonism), shape = 21, color="black") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "% red/green abn. perc.") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  NULL;

```


## Language family

For each language, we obtained its family affiliation from the [Glottolog](https://glottolog.org/) [@hammarstrom_glottolog_2018]; there are `r length(unique(d_colors$glottocode_family))` unique language families, most languages belonging to the Indo-European (indo1319), Atlantic-Congo (atla1278) and Afro-Asiatic (afro1255). 

```{r summary families}
# language family
pander(table(d_colors$glottocode_family))
```

```{r distribution families, fig.cap=capFig("The distribution of languages across families. Only the 15 most represented families are explicitely shown, the others being gathered in the umbrella category 'Other'.")}
# Make sure "Other" is the last one:
d_colors$glottocode_family_other <- factor(d_colors$glottocode_family_other, levels=c(sort(setdiff(unique(d_colors$glottocode_family_other), "Other")), "Other"));
ggplot(data =d_colors)+
  theme_bw() +
  geom_bar(aes(x=as.factor(glottocode_family_other), fill=as.factor(glottocode_family_other)), alpha=0.5) + 
  labs(x="Language families", fill="") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  scale_fill_viridis_d() + 
  guides(fill=FALSE)
```

```{r map family, fig.cap=capFig("Map of the main language families. Only the 15 most represented language families have individual colors.")}
# Use an other variable to plot: only 15 main language families, other families are gathered under "Other"
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors, aes(x=longitude, y=latitude, label=glottocode_family, fill=glottocode_family_other), 
                   color="black", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors, shape=21, aes(x=longitude, y=latitude, fill=glottocode_family_other), color="black") +
  theme(legend.position = "none") +
  scale_fill_viridis_d() + 
  NULL
```

### Putative origins of (macro-)families

The putative geographic origins of the language (macro-)families (latitude and longitude) were obtained from [@wichmann_homelands_2010], supplemented with information from the [Glottolog](https://glottolog.org/) [@hammarstrom_glottolog_2018].

```{r map family origins, fig.cap=capFig("Origins of the putative origins of language (macro-)families.")}
ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_label_repel(data=d_colors_families, aes(x=longitude_family, y=latitude_family, label=macro_family), 
                   color="black", fill="lightyellow", alpha=0.75, size=2.75, label.padding=0.10, label.r=0.05) + 
  geom_point(data=d_colors_families, shape=21, aes(x=longitude_family, y=latitude_family, fill=macro_family), color="black", alpha=0.75) +
  theme(legend.position = "none") +
  scale_fill_viridis_d() + 
  NULL
```

We also collected data corresponding to these locations for elevation, climate and ecology, humidity, distance to bodies of water and UV incidence; please note that (a) these locations are highly speculative, and (b) the information associated does not necessarily reflect the state of the world at the time when the proto-languages were spoken.


## Macroarea

We collected the macroareas as given by the [Glottolog](https://glottolog.org/) [@hammarstrom_glottolog_2018].

```{r map macroareas, fig.cap=capFig("Map of populations in our sample highlighting their macroarea.")}
# macroarea
pander(table(d_colors$macroarea));

ggplot() + theme_bw() +
  geom_polygon(data=mapWorld, aes(x=long, y=lat, group=group), fill="grey") + xlab("Longitude (°)") + ylab("Latitude (°)") + # landmasses
  geom_point(data=d_colors, aes(x=longitude, y=latitude, color=macroarea, shape=macroarea)) +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(color="Macroarea", shape="Macroarea") +
  scale_color_viridis_d() + 
  NULL
```



## Genetic distances between populations

Unfortunately, we could not obtain information about the *opsin* genes ([*OPN1MW*](https://www.omim.org/entry/300821) and [*OPN1LW*](https://www.omim.org/entry/300822)) for the populations in our sample. 
However, using a set of microhaplotypes and SNPs from the [ALFRED database](https://alfred.med.yale.edu/) [@rajeevan_alfred_2003], and the [Arlequin](http://cmpg.unibe.ch/software/arlequin35/) [@excoffier_arlequin_2010] software package, we computed the overall genetic distances between the populations.
We imputed the missing values in the obtained distances matrix with the ultrametric methd [@lapointe_estimating_1995;@de_soete_ultrametric_1984].
Please see [Appendix I. Distance matrices] for more details.

Finally, we applied classic multi-dimensional scaling (`cmdscale`) to this imputed distance matrix, and we decided to retain the first 10 dimensions (resulting in a goodness-of-fit of 10.5%).
These dimensions should represent an overall genetic similarity between our populations.



## Delaunay neghbours

We computed the [Delaunay triangulation](https://en.wikipedia.org/wiki/Delaunay_triangulation) of our language locations taking into account the constraints to population movement imposed by large bodies of water; this is based on the method developed in @cysouw_comment_2012, adapted by M. Tang and D. Dediu.
This results in a list of neighbors for each population in our dataset.

```{r plot Delaunay, fig.cap=capFig("Restricted Delaunay triangulation for the languages in our dataset (in a Pacific-centered view). The yellow dots are the language locations, the thin black lines the connections to their immediate neighbours, and the thik red lines represent barriers to movement.")}
# Plot this Delaunay network:
ggplot() + theme_bw() +
  geom_polygon(data=map_data('world', wrap=c(-20,340), ylim=c(-70,100)), aes(x=long, y=lat, group=group), fill="grey") + # landmasses as polygons (Pacific-centered)
  geom_curve(data=delaunay_triang, aes(x=x1, y=y1, xend=x2, yend=y2), color="black", curvature=0, alpha=0.5) +
  geom_point(data=d_coords[d_coords$type == "language",], aes(x=longitude, y=latitude), shape=21, alpha=0.5, color="blue", fill="yellow", size=2) + # languages
  geom_point(data=geo_barriers, aes(x=longitude, y=latitude), col="red", shape=20, size=1) + # "barriers"
  theme(legend.position = "none",
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.title = element_blank(),
        legend.text=element_blank()) +
  NULL; 
```



## Input files and variables

These data can be found in the following input files (collected in the `./input_files` folder); "pre-processed" means produced by the associated `R` script `00_preprocess_data.R` (automatically, if needed); when transformed, the variable names usually takes the suffix "_r"; when applied to families, the variable takes the suffix "_family":

| Data                        | Input file              | Pre-processed?       | Variable name(s) | Transformed? | Also for families? |
|-----------------------------|-------------------------|----------------------|------------------|--------------|--------------------|
| Populations and languages   | `data_colors.csv`       | No | *glottocode*, *L1* | No | No |
| Language family             | `data_colors.csv`       | No | *glottocode_family*, *macro_family* | No | Yes |
| Geographic locations        | `data_colors.csv`       | No | *latitude*, *longitude* | `cos()` | Yes |
| Elevation                   | `data_elevation.tsv`    | From [Mapzen](https://en.wikipedia.org/wiki/Mapzen) | *elevation* | `log(1+)` | Yes |
| Climate and ecology         | `data_climate.tsv`      | From [WorldClim](https://www.worldclim.org/)        | *clim_PC1*, *clim_PC2*, *clim_PC3* | z-scored | Yes |
| Humidity                    | `data_chumidity.tsv`    | From [NOAA](http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP-NCAR/.CDAS-1/.MONTHLY/.Diagnostic/.above_ground/.qa/datafiles.html) | *hum_median*, *hum_IQR* | No | Yes |
| Distance to bodies of water | `data_dist2water.tsv`   | From [OpenStreetMap](http://openstreetmapdata.com/) | *dist2ocean*, *dist2lakes*, *dist2rivers*, *dist2water* | `log()` | Yes |
| UV incidence                | `data_UV_incidence.tsv` | From [NASA TOMS](http://toms.gsfc.nasa.gov/ery_uv/new_uv/) | *UV_A_mean*, *UV_A_sd*, *UV_B_mean*, *UV_B_sd*, *UV_mean*, *UV_sd* | z-scored | Yes |
| Log of population size      | `data_colors.csv`       | No | *log_popSize* | No (already `log`'d) | No |
| Subsistence strategy        | `data_colors.csv`       | No | *subsistence* | No | No |
| Number of color categories  | `data_colors.csv`       | No | *n_colors* | standardized [0..1] | No |
| Specific term for “blue”    | `data_colors.csv`       | No | *exists_blue* | No | No |
| Abnormal red/green color perception  | `data_colors.csv` | No | *exists_blue* | % &rarr; proportion (division by 100); 0% &rarr; 0.01% | No |
| Putative origins of (macro-)families | `data_colors.csv` | No | *latitude_family*, *longitude_family* | `cos()` | Yes |
| Macroarea                   | `data_colors.csv`       | No | *macroarea* | No | Yes |
| Genetic distances           | `data_cgenetics.tsv`    | From [ALFRED](https://alfred.med.yale.edu/) | *gen_D1* .. *gen_D10* | z-scored | No |
| Delaunay neighbors          | computed                | From languages' geographic locations | *Delaunay_N1*, *Delaunay_N2*, .. | No | No |



# Notes on methods

## Bayesian mixed-effects regressions

For `brms`, to select the best model, we used Bayes factors, WAIC, LOO and KFOLD.
Please note that, form `brms`, there might be differences between Bayes factors, on the one hand, and WAIC/LOO/KFOLD, on the other, due to the default use of improper priors (see, for example, https://stats.stackexchange.com/questions/407964/bayes-factors-and-predictive-accuracy-in-model-comparison-in-rstan-brms); therefore, we will both methods for model selection.


## Mediation analysis

For the mediation analysis, we used a Bayesian approach as implemented by `brms`: in essence, given a treatment" $T$ (for example, latitude), a mediator $M$ (for example, UV incidence), as an outcome $O$ (for example, the existence of a specific word for "blue"), the mediation model looks like:

```{r}
# Mediation:
DiagrammeR::grViz('
  digraph mediation_simple {

  # the graph:
  graph [overlap = true]
  rankdir="LR";

  # the nodes:
  node [shape = box]
  T [label = "treatment", style = "filled", fillcolor = "green"]; 
  M [label = "mediator",  style = "filled", fillcolor = "yellow"]; 
  O [label = "outcome",   style = "filled", fillcolor = "red"]; 

  # the edges:
  T->O [label = "c", style = "solid", color = "black"]
  T->M [label = "a", style = "solid", color = "blue"]
  M->O [label = "b", style = "solid", color = "blue"]
}
')
```

The *total effect* (i.e., the overall influence of $T$ on $O$, defined as $a \cdot b + c$) is decomposed into the *direct effect* (the arrow $T \longrightarrow O$, defined as being equal to $c$) and the *indirect effect* "flowing" through $M$ ($T \longrightarrow M \longrightarrow O$, defined as $a \cdot b$).
These are estimated by fitting the two mixed-effects regressions (with *family* and *macroarea* as random effects) to the data jointly:

$$
\begin{array}{l}
  M \sim T + (1 | family) + (1 | macroarea) \\
  O \sim T + M + (1 | family) + (1 | macroarea)
\end{array}
$$



# Results


## Spatial distribution of the data

How is our data distributed in geographical space?
Is it random, or, if not, how does it deviate from spatial randomness?
In this context, randomness means *complete spatial randomness* (CSR), where points are independent of each other, have the same likelihood of being found at any location, and their position is modeled by a Poisson distribution [@spielman_point_2017;@mediation_spatstat].

Comparing the distribution of our data to a CSR process using the *χ*^2^ test based on quadrat counts, clearly rejects spatial randomness:

```{r chi-sq CSR tests}
pander(quadrat.test(ppp_languages, method="Chisq", nx=5, ny=15), 
       caption=capTab("Test of CSR using quadrat counts unsing *χ*^2^."));
pander(quadrat.test(ppp_languages, method="MonteCarlo", nx=5, ny=15, conditional=TRUE), 
       caption=capTab("Test of CSR using quadrat counts unsing *χ*^2^ (conditional Monte Carlo)."));
```

The same is found using the Kolmogorov-Smirnov test:

```{r kolmogorov-smironov, fig.cap=capFig("Kolmogorov-Smirnov versus the CSR using on two dimensions.")}
k <- cdf.test(ppp_languages, density(ppp_languages));
pander(k, caption=capTab("Spatial Kolmogorov-Smirnov test of CSR in two dimensions."));
#plot(k, main="");
```

Given that our locations are not randomly distributed, do they tend to be regularly spaced or clustered?
The *G* function (using nearest neighbor distances) shows that the distances between the nearest neighbors are shorter than expected for a Poisson process, suggesting a clustering.
The *F* function (using empty space distances) shows that the observed values are larger than the expected one, indicating that empty space distances in our empirical point pattern are shorter than for a Poisson process, suggesting again clustering. 
Finally, the *K* function (using pairwise distance) also suggests clustering.

```{r G function, fig.cap=capFig("The *G*, *F* and *K* functions (one per panel, from left to right). *r* is the distance between points. Expected distribution: blue; all others are the observed distributions with various edge correction methods (see help for `Gest` in package `spatstat`)."), fig.width=9, fig.height=4}
par(mfrow=c(1,3));
plot(Gest(ppp_languages), main="G function");
plot(Fest(ppp_languages), main="F function");
plot(Kest(ppp_languages), main="K function");
par(mfrow=c(1,1));
```

Thus, it is clear (and not surprising) that our datapoints are *significantly clustered*, reflecting, on the one hand, the actual patterning of linguistic diversity constrained by geography, ecology and climate, and, on the other, vagaries of data availability for the color vocabulary and the incidence of abnormal color perception.


## Spatial autocorrelation

Spatial autocorrelation describes the degree to which locations are similar to each other at different distances, with respect to a given variable. 
We use here Moran’s *I* [@moran_notes_1950], which is a measure of global spatial autocorrelation, with either the inverse of the shortest geographical distance ("as the crow flies") on the WGS84 ellipsoid, or the nearest neighbor distance on the Delaunay triangulation, as the weight matrix.

```{r moran I}
# Inverse distance:
moran_I_inv_daltonism <- Moran.I(d_colors$daltonism, d_geo_dists_inv, scaled=TRUE);
moran_I_inv_blue      <- Moran.I(as.numeric(d_colors$exists_blue == "yes"), d_geo_dists_inv, scaled=TRUE); # convert to numeric (0 = "no")
moran_I_inv_UV_A_mean <- Moran.I(d_colors$UV_A_mean, d_geo_dists_inv, scaled=TRUE);
moran_I_inv_UV_B_mean <- Moran.I(d_colors$UV_B_mean, d_geo_dists_inv, scaled=TRUE);
moran_I_inv_n_colors  <- Moran.I(d_colors$n_colors, d_geo_dists_inv, scaled=TRUE);
# Dealunay neighbours:
moran_I_del_daltonism <- moran.test(d_colors$daltonism, nb_delaunay_spweights);
moran_I_del_blue      <- moran.test(as.numeric(d_colors$exists_blue == "yes"), nb_delaunay_spweights); # convert to numeric (0 = "no")
moran_I_del_UV_A_mean <- moran.test(d_colors$UV_A_mean, nb_delaunay_spweights);
moran_I_del_UV_B_mean <- moran.test(d_colors$UV_B_mean, nb_delaunay_spweights);
moran_I_del_n_colors  <- moran.test(d_colors$n_colors, nb_delaunay_spweights);
```

Using the inverse of the geographic distance, Moran's *I* finds *significant positive autocorrelations* for the incidence of red/green abnormal color perception (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_inv_daltonism$observed, moran_I_inv_daltonism$expected, moran_I_inv_daltonism$p.value)`), the incidence of UV-A  (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_inv_UV_A_mean$observed, moran_I_inv_UV_A_mean$expected, moran_I_inv_UV_A_mean$p.value)`) and UV-B (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_inv_UV_B_mean$observed, moran_I_inv_UV_B_mean$expected, moran_I_inv_UV_B_mean$p.value)`), the presence of a specific word for blue (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_inv_blue$observed, moran_I_inv_blue$expected, moran_I_inv_blue$p.value)`), and for the number of color terms (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_inv_n_colors$observed, moran_I_inv_n_colors$expected, moran_I_inv_n_colors$p.value)`).
Likewise, using the Delaunay neighbors, Moran's *I* finds *significant positive autocorrelations* for the incidence of red/green abnormal color perception (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_del_daltonism$estimate["Moran I statistic"], moran_I_del_daltonism$estimate["Expectation"], moran_I_del_daltonism$p.value)`), the incidence of UV-A  (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_del_UV_A_mean$estimate["Moran I statistic"], moran_I_del_UV_A_mean$estimate["Expectation"], moran_I_del_UV_A_mean$p.value)`) and UV-B (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_del_UV_B_mean$estimate["Moran I statistic"], moran_I_del_UV_B_mean$estimate["Expectation"], moran_I_del_UV_B_mean$p.value)`), the presence of a specific word for blue (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_del_blue$estimate["Moran I statistic"], moran_I_del_blue$estimate["Expectation"], moran_I_del_blue$p.value)`), and for the number of color terms (`r sprintf("observed = %.4f > expected = %.4f, *p* = %.4g", moran_I_del_n_colors$estimate["Moran I statistic"], moran_I_del_n_colors$estimate["Expectation"], moran_I_del_n_colors$p.value)`).




## Hypothesis 1: UV &rarr; specific word for "blue"

Here we test the first hypothesis, linking UV incidence and the structure of the color vocabulary, and, in particular, the existence of a specific word for blue.

```{r fig.cap=capFig("Graphical representation of Hypothesis 1.")}
DiagrammeR::grViz(paste0('
  digraph hypothesis_1 {

  # the graph:
  graph [overlap = true]
  rankdir="LR";

  # the nodes:
  node [shape = box, style = "filled", fillcolor = "gray90"];
  UV   [label = "UV incidence"]; 
  Eye  [label = "eye anatomy & physiology"]; 
  Perc [label = "color perception"]; 
  Blue [label = "word for blue"]; 

  # the edges:
  edge       [style = "solid", color = "black"];
  UV->Eye    ;
  Eye->Perc  ;
  Perc->Blue ;
}
'))
```

```{r include=FALSE}
# Null model (intercept) -> word for blue:
if( !all(file.exists("./cached_results/b_0__blue.RData")) )
{
  # n_colors:
  b_0__blue <- brm(exists_blue ~ 1 + (1 | glottocode_family) + (1 | macroarea), # the null model for exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_0__blue <- brms_fit_indices(b_0__blue);
  summary(b_0__blue); mcmc_plot(b_0__blue, type="trace"); mcmc_plot(b_0__blue);

  # Save results:
  save(b_0__blue, 
       file="./cached_results/b_0__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_0__blue.RData");
}
```



### # of color terms &rarr; "blue" [@berlin_basic_1991's stages]

According to @berlin_basic_1991, color categories are acquired through successive stages. 
In particular, a language with 6 color words should have a specific word for blue, but a language with only 5 color terms should not. 
If this were absolutely true, then the number of color categories in a language should fully predict the presence or not of a specific word for blue, but, as has been observed before, this is rather a statistical tendency cross-linguistically.

For our data, the distribution of these cases is given below (*&chi;*^2^(`r (tmp <- chisq.test(table(d_colors$n_colors < 6, d_colors$exists_blue)))$parameter`) = `r round(tmp$statistic,2)`, *p* = `r sprintf("%.4g",tmp$p.value)`):

```{r}
pander(table(ifelse(d_colors$n_colors < 6, "< 6 categories", "≥ 6 categories"), ifelse(d_colors$exists_blue=="no", "no 'blue'", "has 'blue'")));
```

```{r include=FALSE}
# Number of color categories -> word for blue:
if( !all(file.exists("./cached_results/b_0__blue.RData", "./cached_results/b_ncol__blue.RData")) )
{
  # n_colors:
  b_ncol__blue <- brm(exists_blue ~ 1 + n_colors + (1 | glottocode_family) + (1 | macroarea), # n_colors -> exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                              prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_ncol__blue <- brms_fit_indices(b_ncol__blue);
  summary(b_ncol__blue); mcmc_plot(b_ncol__blue, type="trace"); mcmc_plot(b_ncol__blue);
  (h_ncol__blue <- brms::hypothesis(b_ncol__blue, c("n_colors = 0", "n_colors > 0")));
  (hdi_ncol__blue <- hdi(b_ncol__blue, ci=0.95));
  compare_0_ncol__blue <- brms_compare_models(b_0__blue, b_ncol__blue, "null", "+ n_colors"); # clear positive effect of n_colors!
  
  # Save results:
  save(compare_0_ncol__blue, h_ncol__blue, hdi_ncol__blue, #b_ncol__blue, # no need to save the actual model
       file="./cached_results/b_ncol__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_ncol__blue.RData");
}
```

Conversely, the number of color categories (*n_colors*) predicts the presence of a specific word for blue (*exists_blue*), as shown by the plot below, and by a Bayesian mixed-effects logistic regression of *exists_blue* on *n_colors* (with family and macroarea as random effects): *&beta;~n_colors~* = `r round(h_ncol__blue$hypothesis$Estimate,2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_ncol__blue$CI_low[ hdi_ncol__blue$Parameter == "b_n_colors" ], hdi_ncol__blue$CI_high[ hdi_ncol__blue$Parameter == "b_n_colors" ])`), posterior probability that *&beta;~n_colors~*=0, *p*(*&beta;*=0) = `r sprintf("%.4g",h_ncol__blue$hypothesis$Post.Prob[1])`; this effect should be specifically *positive*, which is clearly supported by the posterior probability that *&beta;~n_colors~*>0, *p*(*&beta;*>0) = `r sprintf("%.4g",h_ncol__blue$hypothesis$Post.Prob[2])`.

```{r exploratory blue, fig.cap=capFig('The distribution of the number of color categories for language with (blue) and without (red) a specific word for "blue".')}
ggplot(d_colors, aes(x=n_colors, fill=exists_blue)) + 
  geom_density(alpha=0.5) +
  theme_bw() +
  labs(x="Number of colour categories") +
  theme(axis.text.x = element_text(size =14, face = "bold"), 
        axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
        axis.title=element_text(size=14),
        legend.title=element_text(size =16),
        legend.text=element_text(size =16),
        legend.position="bottom") +
  geom_vline(xintercept = 6, color = "red") +
  scale_x_continuous(breaks=seq(2,12,1)) + 
  scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
  NULL;
```



### UV &rarr; "blue" [@lindsey_color_2002;@brown_color_2004]

Here we check if there is a relationship between measures of UV incidence and the existence of a word for "blue"; as per @lindsey_color_2002 and @brown_color_2004, this relationship should be *negative* (i.e., higher UV-B incidence should reduce the probability of a word for "blue").

```{r fig.cap=capFig('Probability of having a specific word for "blue" function of UV incidence. Left: mean incidence across 1998, right: standard deviation of incidence over the days of 1998; top: UV-A, bottom: UV-B.')}
grid.arrange(ggplot(d_colors, aes(x=UV_A_mean, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               xlab(expression("Mean of UV-A incidence (J/m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ggplot(d_colors, aes(x=UV_A_sd, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               xlab(expression("Std.dev. of UV-A incidence (J/m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ggplot(d_colors, aes(x=UV_B_mean, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               xlab(expression("Mean of UV-B incidence (J/m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ggplot(d_colors, aes(x=UV_B_sd, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               xlab(expression("Std.dev. of UV-B incidence (J/m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ncol=2);
```

```{r include=FALSE}
# UV incidence -> word for blue:
if( !all(file.exists("./cached_results/b_uv__blue.RData")) )
{
  # Individually:
  # UV_A_mean:
  b_uvam__blue <- brm(exists_blue ~ 1 + UV_A_mean_r + I(UV_A_mean_r^2) + (1 | glottocode_family) + (1 | macroarea), # UV_A_mean -> exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                              prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_uvam__blue <- brms_fit_indices(b_uvam__blue);
  summary(b_uvam__blue); mcmc_plot(b_uvam__blue, type="trace"); mcmc_plot(b_uvam__blue);
  (h_uvam__blue <- brms::hypothesis(b_uvam__blue, c("UV_A_mean_r = 0", "UV_A_mean_r < 0", "IUV_A_mean_rE2 = 0")));
  (hdi_uvam__blue <- hdi(b_uvam__blue, ci=0.95));
  # Try to remove the quadratic effect:
  b_uvam__blue_2 <- update(b_uvam__blue, . ~ . - I(UV_A_mean_r^2), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_uvam__blue_2 <- brms_fit_indices(b_uvam__blue_2);
  summary(b_uvam__blue_2); mcmc_plot(b_uvam__blue_2, type="trace"); mcmc_plot(b_uvam__blue_2);
  (h_uvam__blue_2 <- brms::hypothesis(b_uvam__blue_2, c("UV_A_mean_r = 0", "UV_A_mean_r < 0")));
  (hdi_uvam__blue_2 <- hdi(b_uvam__blue_2, ci=0.95));
  brms_compare_models(b_uvam__blue_2, b_uvam__blue, "linear only", "linear + quadratic"); # quadratic is not needed, linear is enough...
  compare_0_uvam__blue <- brms_compare_models(b_0__blue, b_uvam__blue_2, "null", "+ UV_A_mean"); # clear negative linear effect of UV_A_mean!
  # The model to keep:
  b_uvam__blue <- b_uvam__blue_2; h_uvam__blue <- h_uvam__blue_2; hdi_uvam__blue <- hdi_uvam__blue_2;
  
  # UV_A_sd:
  b_uvas__blue <- brm(exists_blue ~ 1 + UV_A_sd_r + I(UV_A_sd_r^2) + (1 | glottocode_family) + (1 | macroarea), # UV_A_sd -> exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                              prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_uvas__blue <- brms_fit_indices(b_uvas__blue);
  summary(b_uvas__blue); mcmc_plot(b_uvas__blue, type="trace"); mcmc_plot(b_uvas__blue);
  (h_uvas__blue <- brms::hypothesis(b_uvas__blue, c("UV_A_sd_r = 0", "IUV_A_sd_rE2 = 0")));
  (hdi_uvas__blue <- hdi(b_uvas__blue, ci=0.95));
  compare_0_uvas__blue <- brms_compare_models(b_0__blue, b_uvas__blue, "null", "+ UV_A_sd"); # UV_A_sd does not seem to have an effect...
  
  # UV_B_mean:
  b_uvbm__blue <- brm(exists_blue ~ 1 + UV_B_mean_r + I(UV_B_mean_r^2) + (1 | glottocode_family) + (1 | macroarea), # UV_B_mean -> exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                              prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_uvbm__blue <- brms_fit_indices(b_uvbm__blue);
  summary(b_uvbm__blue); mcmc_plot(b_uvbm__blue, type="trace"); mcmc_plot(b_uvbm__blue);
  (h_uvbm__blue <- brms::hypothesis(b_uvbm__blue, c("UV_B_mean_r = 0", "UV_B_mean_r < 0", "IUV_B_mean_rE2 = 0")));
  (hdi_uvbm__blue <- hdi(b_uvbm__blue, ci=0.95));
  # Try to remove the quadratic effect:
  b_uvbm__blue_2 <- update(b_uvbm__blue, . ~ . - I(UV_B_mean_r^2), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_uvbm__blue_2 <- brms_fit_indices(b_uvbm__blue_2);
  summary(b_uvbm__blue_2); mcmc_plot(b_uvbm__blue_2, type="trace"); mcmc_plot(b_uvbm__blue_2);
  (h_uvbm__blue_2 <- brms::hypothesis(b_uvbm__blue_2, c("UV_B_mean_r = 0", "UV_B_mean_r < 0")));
  (hdi_uvbm__blue_2 <- hdi(b_uvbm__blue_2, ci=0.95));
  brms_compare_models(b_uvbm__blue_2, b_uvbm__blue, "linear only", "linear + quadratic"); # quadratic is not needed, linear is enough...
  compare_0_uvbm__blue <- brms_compare_models(b_0__blue, b_uvbm__blue_2, "null", "+ UV_B_mean"); # clear negative effect of UV_B_mean!
  # The model to keep:
  b_uvbm__blue <- b_uvbm__blue_2; h_uvbm__blue <- h_uvbm__blue_2; hdi_uvbm__blue <- hdi_uvbm__blue_2;
  
  # UV_B_sd:
  b_uvbs__blue <- brm(exists_blue ~ 1 + UV_B_sd_r + I(UV_B_sd_r^2) + (1 | glottocode_family) + (1 | macroarea), # UV_B_sd -> exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                              prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_uvbs__blue <- brms_fit_indices(b_uvbs__blue);
  summary(b_uvbs__blue); mcmc_plot(b_uvbs__blue, type="trace"); mcmc_plot(b_uvbs__blue);
  (h_uvbs__blue <- brms::hypothesis(b_uvbs__blue, c("UV_B_sd_r = 0", "IUV_B_sd_rE2 = 0")));
  (hdi_uvbs__blue <- hdi(b_uvbs__blue, ci=0.95));
  compare_0_uvbs__blue <- brms_compare_models(b_0__blue, b_uvbs__blue, "null", "+ UV_B_sd"); # UV_B_sd does not seem to have an effect...
  
  # Compare UV_A_mean and UV_B_mean:
  compare_uvam_uvbm__blue <- brms_compare_models(b_uvam__blue, b_uvbm__blue, "UV_A_mean", "UV_B_mean"); # UV_B_mean fits the data better than UV_A_mean
  
  # As check_collinearity() does not currently work with brm, use glm:
  m_uv__blue <- glmer(exists_blue ~ 1 + UV_A_mean + UV_B_mean + (1 | glottocode_family) + (1 | macroarea),
                      family=binomial(), data=d_colors);
  summary(m_uv__blue);
  (m_uv__blue_VIF <- performance::check_collinearity(m_uv__blue)); # very high VIF -> we must keep only UV_B_mean
  
  # Save results:
  save(compare_0_uvam__blue, h_uvam__blue, hdi_uvam__blue, #b_uvam__blue, # no need to save the actual model
       compare_0_uvas__blue, h_uvas__blue, hdi_uvas__blue, #b_uvas__blue, # no need to save the actual model
       compare_0_uvbm__blue, h_uvbm__blue, hdi_uvbm__blue, #b_uvbm__blue, # no need to save the actual model
       compare_0_uvbs__blue, h_uvbs__blue, hdi_uvbs__blue, #b_uvbs__blue, # no need to save the actual model
       compare_uvam_uvbm__blue, m_uv__blue_VIF,
       file="./cached_results/b_uv__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_uv__blue.RData");
}
```

Fitting individual Bayesian mixed-effects logistic regressions (with family and macroarea as random effects) predicting *exists_blue* from the mean and standard deviation of the incidence of UV-A and UV-B for 1998 (both linear and quadratic effects), we found that:

- systematically, the quadratic effects do not contribute,
- the standard deviations (*UV_A_sd* and *UV_B_sd*) do not predict *exists_blue* (*&beta;~UV_A_sd~* = `r round(h_uvas__blue$hypothesis$Estimate,2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_uvas__blue$CI_low[ hdi_uvas__blue$Parameter == "b_UV_A_sd_r" ], hdi_uvas__blue$CI_high[ hdi_uvas__blue$Parameter == "b_UV_A_sd_r" ])`), *p*(*&beta;*=0) = `r sprintf("%.2g",h_uvas__blue$hypothesis$Post.Prob[1])`; *&beta;~UV_B_sd~* = `r round(h_uvbs__blue$hypothesis$Estimate,2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_uvbs__blue$CI_low[ hdi_uvbs__blue$Parameter == "b_UV_B_sd_r" ], hdi_uvbs__blue$CI_high[ hdi_uvbs__blue$Parameter == "b_UV_B_sd_r" ])`), *p*(*&beta;*=0) = `r sprintf("%.2g",h_uvbs__blue$hypothesis$Post.Prob[1])`),
- the means (*UV_A_mean* and *UV_B_mean*) both have negative effects on *exists_blue* (*&beta;~UV_A_mean~* = `r round(h_uvam__blue$hypothesis$Estimate,2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_uvam__blue$CI_low[ hdi_uvam__blue$Parameter == "b_UV_A_mean_r" ], hdi_uvam__blue$CI_high[ hdi_uvam__blue$Parameter == "b_UV_A_mean_r" ])`), *p*(*&beta;*=0) = `r sprintf("%.3g",h_uvam__blue$hypothesis$Post.Prob[1])`, *p*(*&beta;*<0) = `r sprintf("%.3g",h_uvam__blue$hypothesis$Post.Prob[2])`; *&beta;~UV_B_mean~* = `r round(h_uvbm__blue$hypothesis$Estimate,2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_uvbm__blue$CI_low[ hdi_uvbm__blue$Parameter == "b_UV_B_mean_r" ], hdi_uvbm__blue$CI_high[ hdi_uvbm__blue$Parameter == "b_UV_B_mean_r" ])`), *p*(*&beta;*=0) = `r sprintf("%.3g",h_uvbm__blue$hypothesis$Post.Prob[1])`, *p*(*&beta;*<0) = `r sprintf("%.3g",h_uvbm__blue$hypothesis$Post.Prob[2])`),
- the two means are highly multicollinear (VIF~UV_A_mean~ = `r round(m_uv__blue_VIF$VIF[ m_uv__blue_VIF$Parameter == "UV_A_mean" ],1)`, VIF~UV_B_mean~ = `r round(m_uv__blue_VIF$VIF[ m_uv__blue_VIF$Parameter == "UV_B_mean" ],1)`),
- but *UV_A_mean* fits the data worse than *UV_B_mean* (Bayes factor = `r round(compare_uvam_uvbm__blue$BF,2)`, LOO = `r sprintf("%.2f [SE=%.2f]", compare_uvam_uvbm__blue$LOO["UV_A_mean", "elpd_diff"], compare_uvam_uvbm__blue$LOO["UV_A_mean", "se_diff"])`, WAIC = `r sprintf("%.2f [SE=%.2f]", compare_uvam_uvbm__blue$WAIC["UV_A_mean", "elpd_diff"], compare_uvam_uvbm__blue$WAIC["UV_A_mean", "se_diff"])`, K-fold = `r sprintf("%.2f [SE=%.2f]", compare_uvam_uvbm__blue$KFOLD["UV_A_mean", "elpd_diff"], compare_uvam_uvbm__blue$KFOLD["UV_A_mean", "se_diff"])`).

Therefore, we will only consider *UV_B_mean* (renamed to *UVB* for shortness) in the following analyses.

```{r}
# UV-B variables rename for shortness:
d_colors$UVB <- d_colors$UV_B_mean; d_colors$UVB_r <- d_colors$UV_B_mean_r; 
d_colors$UVB_family <- d_colors$UV_B_mean_family; d_colors$UVB_family_r <- d_colors$UV_B_mean_family_r; 
```



### Latitude &rarr; "blue"

The effect of UV-B on the existence of a specific word for "blue" [@lindsey_color_2002;@brown_color_2004] should be due to the physical negative relationship between the latitude of a location on the Earth's surface and the mean annual UB-V radiation it receives; thus, it is justified to investigate the relationship between latitude and "blue". 
This relationship should specifically be *positive* (i.e., higher latitudes receive less UV-B incidence), but, given the cosine transformation of latitude in our statistical models, with large values closer to the equator, the expected statistical relationship should be *negative*.

```{r fig.cap=capFig('The effect of latitude for languages with (blue area) and without (red area) a specific word for "blue". From left to right: the linear effect of latitude, the effect of the absolute value of latitude (i.e., `abs(latitude)`), and the quadratic effect of latitude (i.e., `latitude`^2^).')}
grid.arrange(ggplot(d_colors, aes(x=latitude, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               labs(x="latitude (°)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ggplot(d_colors, aes(x=abs(latitude), fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               labs(x="abs(latitude) (°)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ggplot(d_colors, aes(x=latitude^2, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               labs(x=expression("latitude"^"2"~" (°"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ncol=3);
```

```{r include=FALSE}
# Latitude -> word for blue:
if( !all(file.exists("./cached_results/b_lat__blue.RData")) )
{
  b_lat__blue_all <- brm(exists_blue ~ 1 + latitude_r + I(latitude_r^2) + abs(latitude_r) + (1 | glottocode_family) + (1 | macroarea), # latitude -> exists_blue
                         family=bernoulli(link="logit"), data=d_colors, 
                         prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                 prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                         save_all_pars=TRUE, # needed for Bayes factors
                         sample_prior=TRUE,  # needed for hypotheses tests
                         cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_lat__blue_all <- brms_fit_indices(b_lat__blue_all);
  summary(b_lat__blue_all); mcmc_plot(b_lat__blue_all, type="trace"); mcmc_plot(b_lat__blue_all);
  (h_lat__blue <- brms::hypothesis(b_lat__blue_all, c("latitude_r = 0", "latitude_r < 0", "Ilatitude_rE2 = 0", "Ilatitude_rE2 < 0", "abslatitude_r = 0", "abslatitude_r < 0")));
  (hdi_lat__blue <- hdi(b_lat__blue_all, ci=0.95));
  compare_0_lat__blue <- brms_compare_models(b_0__blue, b_lat__blue_all, "null", "+ latitude"); # latitude matters!
  # Try to remove the linear effect:
  b_lat__blue_1 <- update(b_lat__blue_all, . ~ . - latitude_r, cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_lat__blue_1 <- brms_fit_indices(b_lat__blue_1);
  summary(b_lat__blue_1); mcmc_plot(b_lat__blue_1, type="trace"); mcmc_plot(b_lat__blue_1);
  (h_lat__blue_1 <- brms::hypothesis(b_lat__blue_1, c("Ilatitude_rE2 = 0", "Ilatitude_rE2 < 0", "abslatitude_r = 0", "abslatitude_r < 0")));
  (hdi_lat__blue_1 <- hdi(b_lat__blue_1, ci=0.95));
  brms_compare_models(b_lat__blue_1, b_lat__blue_all, "- linear", "all"); # linear does not seem needed
  # Try to remove the abs() also:
  b_lat__blue_1a <- update(b_lat__blue_1, . ~ . - abs(latitude_r), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_lat__blue_1a <- brms_fit_indices(b_lat__blue_1a);
  summary(b_lat__blue_1a); mcmc_plot(b_lat__blue_1a, type="trace"); mcmc_plot(b_lat__blue_1a);
  (h_lat__blue_1a <- brms::hypothesis(b_lat__blue_1a, c("Ilatitude_rE2 = 0", "Ilatitude_rE2 < 0")));
  (hdi_lat__blue_1a <- hdi(b_lat__blue_1a, ci=0.95));
  brms_compare_models(b_lat__blue_1a, b_lat__blue_all, "- linear - abs", "all"); # linear nor abs() do not seem needed...
  compare_0_lat__blue <- brms_compare_models(b_0__blue, b_lat__blue_1a, "null", "+ latitude"); # negative effect of latitude^2!
  # -> quadratic-only model:
  b_lat__blue <- b_lat__blue_1a; h_lat__blue <- h_lat__blue_1a; hdi_lat__blue <- hdi_lat__blue_1a;

  # Save results:
  save(compare_0_lat__blue, h_lat__blue, hdi_lat__blue, #b_lat__blue, # no need to save the actual model
       file="./cached_results/b_lat__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_lat__blue.RData");
}
```

Interestingly, when testing simultaneously the influence of *latitude* (linear effect), `abs(`*latitude*`)` (absolute value), and *latitude*^2^ (quadratic effect) on the presence of a dedicated word for "blue", only the quadratic term has a negative effect: *&beta;~latitude^2^~* = `r round(h_lat__blue$hypothesis$Estimate,2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_lat__blue$CI_low[ hdi_lat__blue$Parameter == "b_Ilatitude_rE2" ], hdi_lat__blue$CI_high[ hdi_lat__blue$Parameter == "b_Ilatitude_rE2" ])`), posterior probability *p*(*&beta;*=0) = `r sprintf("%.4g",h_lat__blue$hypothesis$Post.Prob[1])`, *p*(*&beta;*<0) = `r sprintf("%.4g",h_lat__blue$hypothesis$Post.Prob[2])`.

Please note that @lindsey_color_2002 and @brown_color_2004 would predict that the effect of latitude (mediated by the incidence of UV-B radiation) should be symmetric across the equator, so it should be modeled either as its absolute value (i.e., `abs(`*latitude*`)`) or as an even power of *latitude* (i.e., *latitude*^2*k*^, *k*≥2).
Thus, the fact that *latitude* is not retained supports this conjecture, while the non-retention of `abs(`*latitude*`)` but of *latitude*^2^ seems to suggest a genuinely non-linear symmetric effect.



### Subsistence &rarr; "blue"

It has been suggested that various aspects of culture, including the subsistence mode, might have an effect on the existence of a dedicated word for "blue", and we should specifically expect that AGR populations have a *higher chance* of having a dedicated word for "blue".

```{r}
pander(tmp <- table(d_colors$subsistence, ifelse(d_colors$exists_blue=="no", "no 'blue'", "has 'blue'")));
```

```{r fig.cap=capFig('The relationship between subsistence type and the existence of a specific word for "blue".')}
ggplot(as.data.frame(tmp) %>% mutate(Var2 = ifelse(Var2=="has 'blue'","Yes","No")), aes(x=Var1, y=Freq, fill=Var2)) +
  geom_bar(stat="identity", color="black") + 
  geom_text(aes(label=Freq, color=Var2), position = position_stack(vjust = 0.5), size=3.0)+
  xlab("Subsistence") + ylab("Count") + 
  scale_fill_viridis_d('Specific word for "blue"?') + scale_color_manual('Specific word for "blue"?', values=c("No"="yellow", "Yes"="black")) + 
  NULL;
```

```{r include=FALSE}
# Subsistence -> word for blue:
if( !all(file.exists("./cached_results/b_subsist__blue.RData")) )
{
  # subsistence:
  b_subsist__blue <- brm(exists_blue ~ 1 + subsistence + (1 | glottocode_family) + (1 | macroarea), # subsistence -> exists_blue
                      family=bernoulli(link="logit"), data=d_colors, 
                      prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                              prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                      save_all_pars=TRUE, # needed for Bayes factors
                      sample_prior=TRUE,  # needed for hypotheses tests
                      cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_subsist__blue <- brms_fit_indices(b_subsist__blue);
  summary(b_subsist__blue); mcmc_plot(b_subsist__blue, type="trace"); mcmc_plot(b_subsist__blue);
  (h_subsist__blue <- brms::hypothesis(b_subsist__blue, c("subsistenceAGR = 0", "subsistenceAGR > 0")));
  (hdi_subsist__blue <- hdi(b_subsist__blue, ci=0.95));
  compare_0_subsist__blue <- brms_compare_models(b_0__blue, b_subsist__blue, "null", "+ subsistence"); # no clear effect of subsistence...
  
  # Save results:
  save(compare_0_subsist__blue, h_subsist__blue, hdi_subsist__blue, #b_subsist__blue, # no need to save the actual model
       file="./cached_results/b_subsist__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_subsist__blue.RData");
}
```

Subsistence does not seem to make a contribution to predicting the presence of a specific word for blue (*exists_blue*): a Bayesian mixed-effects logistic regression of *exists_blue* on *subsistence* [AGR vs HG] (with family and macroarea as random effects) has *&beta;~AGR-HG~* = `r round(h_subsist__blue$hypothesis$Estimate,2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_subsist__blue$CI_low[ hdi_subsist__blue$Parameter == "b_subsistenceAGR" ], hdi_subsist__blue$CI_high[ hdi_subsist__blue$Parameter == "b_subsistenceAGR" ])`), posterior probability *p*(*&beta;*=0) = `r sprintf("%.4g",h_subsist__blue$hypothesis$Post.Prob[1])`, but this could be due to the very small proportion of HG populations in our sample (`r sprintf("%.1f%%", sum(d_colors$subsistence=="HG")/nrow(d_colors)*100)`).
Interestingly, the specific directional hypothesis of a positive effect of AGR is relatively well supported by the posterior probability *p*(*&beta;*>0) = `r sprintf("%.4g",h_subsist__blue$hypothesis$Post.Prob[2])`.



### Elevation &rarr; "blue"

Arguably there may be a *positive* relationship between a location's altitude above sea level and the average amount of UV-B it receives in a year.

```{r fig.cap=capFig('The effect of elevation for languages with (blue area) and without (red area) a specific word for "blue". From left to right: the linear effect of elevation and the quadratic effect of elevation (i.e., `elevation`^2^).')}
grid.arrange(ggplot(d_colors, aes(x=elevation, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               labs(x="elevation (m)") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ggplot(d_colors, aes(x=elevation^2, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               labs(x=expression("elevation"^"2"~" (m"^"2"~")")) +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ncol=2);
```

```{r include=FALSE}
# Elevation -> word for blue:
if( FALSE )
{
  b_alt__blue_all <- brm(exists_blue ~ 1 + elevation_r + I(elevation_r^2) + (1 | glottocode_family) + (1 | macroarea), # elevation -> exists_blue
                         family=bernoulli(link="logit"), data=d_colors, 
                         prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                 prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                         save_all_pars=TRUE, # needed for Bayes factors
                         sample_prior=TRUE,  # needed for hypotheses tests
                         cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_alt__blue_all <- brms_fit_indices(b_alt__blue_all);
  summary(b_alt__blue_all); mcmc_plot(b_alt__blue_all, type="trace"); mcmc_plot(b_alt__blue_all);
  (h_alt__blue <- brms::hypothesis(b_alt__blue_all, c("elevation_r = 0", "elevation_r > 0", "Ielevation_rE2 = 0", "Ielevation_rE2 > 0")));
  (hdi_alt__blue <- hdi(b_alt__blue_all, ci=0.95));
  compare_0_alt__blue <- brms_compare_models(b_0__blue, b_alt__blue_all, "null", "+ elevation"); # elevation seems to have no effect...
  # Try to remove the quadratic effect:
  b_alt__blue_2 <- update(b_alt__blue_all, . ~ . - I(elevation_r^2), cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_alt__blue_2 <- brms_fit_indices(b_alt__blue_2);
  summary(b_alt__blue_2); mcmc_plot(b_alt__blue_2, type="trace"); mcmc_plot(b_alt__blue_2);
  (h_alt__blue_2 <- brms::hypothesis(b_alt__blue_2, c("elevation_r = 0", "elevation_r > 0")));
  (hdi_alt__blue_2 <- hdi(b_alt__blue_2, ci=0.95));
  brms_compare_models(b_alt__blue_2, b_alt__blue_all, "- quadratic", "all"); # quadratic does not seem needed
  compare_0_alt__blue <- brms_compare_models(b_0__blue, b_alt__blue_2, "null", "+ elevation"); # no effect of elevation!
}
```

However, there is no effect of *elevation* (linear effect) nor of *elevation*^2^ (quadratic effect) on the presence of a dedicated word for "blue" in our data, even when testing the specific directional hypothesis.



### Climate, ecology and humidity &rarr; "blue"

As opposed to the above, there do not seem to be good *a priori* reasons why climate, ecology and air humidity should influence the presence of a dedicated word for "blue".

```{r fig.cap=capFig('The effect of climate, ecology and humidity for languages with (blue area) and without (red area) a specific word for "blue". From left to right and top to bottom: climate PC1, PC2 and PC3, median and IQR of humidity.')}
grid.arrange(ggplot(d_colors, aes(x=clim_PC1, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               labs(x="clim_PC1") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ggplot(d_colors, aes(x=clim_PC2, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               labs(x="clim_PC2") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ggplot(d_colors, aes(x=clim_PC3, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               labs(x="clim_PC3") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ggplot(d_colors, aes(x=hum_median, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               labs(x="hum_median") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ggplot(d_colors, aes(x=hum_IQR, fill=exists_blue)) + 
               geom_density(alpha=0.5) +
               theme_bw() +
               labs(x="hum_IQR") +
               theme(axis.text.x = element_text(size =14, face = "bold"), 
                     axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
                     axis.title=element_text(size=14),
                     legend.title=element_text(size =16),
                     legend.text=element_text(size =16),
                     legend.position="bottom") +
               scale_fill_manual('Word for blue?', values=c("yes"="blue", "no"="red")) + 
               NULL,
             ncol=3);
```

```{r include=FALSE}
# Climate, ecology & humidity -> word for blue:
if( !all(file.exists("./cached_results/b_clim__blue.RData")) )
{
  # VIF:
  m_clim__blue_all <- glm(exists_blue ~ 1 + clim_PC1_r + clim_PC2_r + clim_PC3_r + hum_median + hum_IQR, family=binomial(), data=d_colors);
  summary(m_clim__blue_all);
  performance::check_collinearity(m_clim__blue_all); # not much collinearity...
  
  # brms modeling:
  b_clim__blue_all <- brm(exists_blue ~ 1 + clim_PC1_r + clim_PC2_r + clim_PC3_r + hum_median + hum_IQR + 
                            (1 | glottocode_family) + (1 | macroarea), # climate, ecology, humidity -> exists_blue
                          family=bernoulli(link="logit"), data=d_colors, 
                          prior=c(prior(student_t(3, 0, 2.5), class="Intercept"),
                                  prior(student_t(3, 0, 2.5), class="b")), # pretty wide priors centered on 0
                          save_all_pars=TRUE, # needed for Bayes factors
                          sample_prior=TRUE,  # needed for hypotheses tests
                          cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_clim__blue_all <- brms_fit_indices(b_clim__blue_all);
  summary(b_clim__blue_all); mcmc_plot(b_clim__blue_all, type="trace"); mcmc_plot(b_clim__blue_all);
  (h_clim__blue <- brms::hypothesis(b_clim__blue_all, c("clim_PC1_r = 0", "clim_PC2_r = 0", "clim_PC3_r = 0", "hum_median = 0", "hum_IQR = 0")));
  (hdi_clim__blue <- hdi(b_clim__blue_all, ci=0.95));
  compare_0_clim__blue <- brms_compare_models(b_0__blue, b_clim__blue_all, "null", "+ climate"); # marginally better...
  # Iterative manual simplification:
  b_clim__blue_1 <- update(b_clim__blue_all, . ~ . - clim_PC2_r - hum_IQR - clim_PC3_r - hum_median, 
                           cores=brms_ncores, iter=4000, warmup=2000, thin=2, control=list(adapt_delta=0.99, max_treedepth=10));
  b_clim__blue_1 <- brms_fit_indices(b_clim__blue_1);
  summary(b_clim__blue_1); mcmc_plot(b_clim__blue_1, type="trace"); mcmc_plot(b_clim__blue_1);
  (h_clim__blue_1 <- brms::hypothesis(b_clim__blue_1, c("clim_PC1_r = 0")));
  (hdi_clim__blue_1 <- hdi(b_clim__blue_1, ci=0.95));
  brms_compare_models(b_clim__blue_1, b_clim__blue_all, "simplified", "full"); # simplified is better than full
  compare_0_clim__blue <- brms_compare_models(b_0__blue, b_clim__blue_1, "null", "+ clim_PC1"); # positive effect of clim_PC1!
  # -> clim_PC1-only model:
  b_clim__blue <- b_clim__blue_1; h_clim__blue <- h_clim__blue_1; hdi_clim__blue <- hdi_clim__blue_1;

  # Save results:
  save(compare_0_clim__blue, h_clim__blue, hdi_clim__blue, #b_clim__blue, # no need to save the actual model
       file="./cached_results/b_clim__blue.RData", compress="xz", compression_level=9);
} else
{
  load("./cached_results/b_clim__blue.RData");
}
```

However, when testing simultaneously the influence of the three climate PCs and of the median and IQR of air humidity on the presence of a dedicated word for "blue", only the first climate PC (*clim_PC1*) has a positive effect: *&beta;~clim_PC1~* = `r round(h_clim__blue$hypothesis$Estimate,2)` (95%HDI = `r sprintf("[%.2f, %.2f]", hdi_clim__blue$CI_low[ hdi_clim__blue$Parameter == "b_clim_PC1_r" ], hdi_clim__blue$CI_high[ hdi_clim__blue$Parameter == "b_clim_PC1_r" ])`), posterior probability *p*(*&beta;*=0) = `r sprintf("%.4g",h_clim__blue$hypothesis$Post.Prob[1])`.



### Distance to water &rarr; "blue"












```{r cache=FALSE}
knitr::knit_exit();
```







### Mediation analysis


```{r blue mediation analysis}

if( !file.exists("./cached_results/mediation_blue.RData") )
{
  ## WITH ALL DATA, with normal regression: UVR --> n_colors --> exists_blue
  model_mediator <- bf(n_colors ~ UVB_r + (1 | macroarea) + (1 | glottocode_family)) + gaussian()
  model_outcome <- bf(exists_blue ~ UVB_r + n_colors + (1 | macroarea) + (1 | glottocode_family)) + bernoulli(link = "logit")
  
  blue_UV_Nbcol <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
            cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
            control=brms_control)
  
  ## WITH ALL DATA, with normal regression: lat --> UVB_r --> exists_blue
  model_mediator <- bf(UVB_r ~ latitude_r + (1 | macroarea) + (1 | glottocode_family)) + gaussian()
  model_outcome <- bf(exists_blue ~ UVB_r + latitude_r + (1 | macroarea) + (1 | glottocode_family)) + bernoulli(link = "logit")
  blue_lat_UV <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
            cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
            control=brms_control)

  # ## WITH ALL DATA, with normal regression: lat² --> UVB_r --> exists_blue
  # model_mediator <- bf(UVB_r ~ I(latitude_r^2) + (1 | macroarea) + (1 | glottocode_family)) + gaussian()
  # model_outcome <- bf(exists_blue ~ UVB_r + I(latitude_r^2) + (1 | macroarea) + (1 | glottocode_family)) + bernoulli(link = "logit")
  # blue_lat2_UV <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
  #           cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
  #           control=brms_control)

  ## WITH ALL DATA, with normal regression: lat + lat² --> UVB_r --> exists_blue
  model_mediator <- bf(UVB_r ~ latitude_r + I(latitude_r^2)  + (1 | macroarea) + (1 | glottocode_family)) + gaussian()
  model_outcome <- bf(exists_blue ~ UVB_r  + latitude_r +  I(latitude_r^2) + (1 | macroarea) + (1 | glottocode_family)) + bernoulli(link = "logit")
  blue_latlat2_UV <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
            cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
            control=brms_control)

  ## WITH ALL DATA, with normal regression: latitude_family --> UVB_r --> exists_blue
  model_mediator <- bf(UVB_r ~ latitude_family_r + (1 | macroarea)) + gaussian()
  model_outcome <- bf(exists_blue ~ UVB_r + latitude_family_r + (1 | macroarea)) + bernoulli(link = "logit")
  blue_latfam_UV <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
            cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
            control=brms_control)

  # ## WITH ALL DATA, with normal regression: latitude_family² --> UVB_r --> exists_blue
  # model_mediator <- bf(UVB_r ~ I(latitude_family_r^2) + (1 | macroarea) + (1 | glottocode_family)) + gaussian()
  # model_outcome <- bf(exists_blue ~ UVB_r + I(latitude_family_r^2) + (1 | macroarea) + (1 | glottocode_family)) + bernoulli(link = "logit")
  # blue_latfam2_UV <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
  #           cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
  #           control=brms_control)

  ## WITH ALL DATA, with normal regression: latitude_family + latitude_family² --> UVB_r --> exists_blue
  model_mediator <- bf(UVB_r ~ latitude_family_r + I(latitude_family_r^2) + (1 | macroarea)) + gaussian()
  model_outcome <- bf(exists_blue ~ UVB_r  + latitude_family_r + I(latitude_family_r^2) + (1 | macroarea)) + bernoulli(link = "logit")
  blue_latfamlatfam2_UV <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
            cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
            control=brms_control)

  # cache these results:
  save(blue_UV_Nbcol, blue_lat_UV, blue_latlat2_UV, blue_latfam_UV, blue_latfamlatfam2_UV,
       file="./cached_results/mediation_blue.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/mediation_blue.RData")
}

```
 

 
```{r function plot mediation result}

# Homemade function to plot the result of mediation
plot_med <- function(x){
  h <- mediation(x)
  h <- h[h$effect=="indirect" | h$effect=="direct" | h$effect=="total",]
  h <- h[order(h$effect),]
  #scale_color <- c("#DCE319FF", "#73D055FF", "#29AF7FFF", "#287D8EFF", "#39568CFF")
  scale_color <- c( "#73D055FF", "#29AF7FFF",  "#39568CFF")
  p <- ggplot(data=h, aes(x = value, y= effect, color=effect)) +
    geom_point(data=h, aes(x = value, y= effect, color=effect, size=0.55)) +
    geom_point(data=h, aes(x = hdi.low, y= effect, color=effect)) +
    geom_point(data=h, aes(x = hdi.high, y= effect, color=effect)) +
    theme_bw()  +
    scale_colour_manual(values = scale_color) +
    geom_vline(xintercept = 0, linetype="dashed") +
    guides(color=FALSE, size=FALSE) +
    theme(axis.text.x = element_text(size =14, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =14, face = "bold"),
             axis.title=element_text(size=14),
             legend.title=element_text(size =16),
             legend.text=element_text(size =16)) 
  
  for (el in 1: length(h$effect)){
    p <- p + geom_segment(x = h$hdi.low[h$effect==h$effect[el]], y = el, xend = h$hdi.high[h$effect==h$effect[el]], yend = el, color=scale_color[el])
  }
  p
} 

# function 2
plot_answer_med <- function(x, outcome, mediator){
  if (outcome=="daltonismbeta" & mediator =="UVBz"){
    outcome1 <- "daltonism"
    mediator1 <- "UVR"
  } else if (outcome=="exists_blue" & mediator =="UVBz"){
    outcome1 <- "exists_blue"
    mediator1 <- "UVR"
  } else if (mediator=="Nbcolorcat" & outcome=="exists_blue"){
    mediator1 <- "Nb of colour categories"
    outcome1 <- "exists_blue"
  } else {
    outcome1 <- outcome
    mediator1 <- mediator
  }
  p1 <- pp_check(x, resp = outcome) + ggtitle(paste("Outcome (", outcome1, ")", sep=""))
  p2 <- pp_check(x, resp = mediator) + ggtitle(paste("Mediator (", mediator1, ")", sep=""))
  grid.arrange(p1, p2, ncol=2)
}

```
 

 
#### UVR --> Number of color categories --> exists_blue 


##### Bayes model result and diagnostic
```{r show mediation nbcolor blue 1}
# Plot Bayes model
summary(blue_UV_Nbcol)
plot(blue_UV_Nbcol)
```

##### Mediation result

With prob = 0.9:
```{r show mediation nbcolor blue 2 }
# Plot mediation
mediation(blue_UV_Nbcol)
plot_med(blue_UV_Nbcol)
```

With prob = 0.95:
```{r show mediation nbcolor blue 3}
mediation(blue_UV_Nbcol, prob=0.95)

```

Please note that as exists_blue (outcome) is binary, direct and indirect effects may be on different scales.

##### Comparaison of predicted and observed values for the mediator and outcome

The faint lines (yrep) are posterior predictive draws, and y is the observed distribution.

```{r show mediation nbcolor blue 4}
plot_answer_med(blue_UV_Nbcol, "Blue", "Nbcolorcat")

```

The faint lines (yrep) are posterior predictive draws. 

#### latitude --> UVR --> exists_blue 

##### Bayes model result and diagnostic
```{r show mediation lat blue 1}
# Plot Bayes model
summary(blue_lat_UV)
plot(blue_lat_UV)
```

##### Mediation result

With prob = 0.9:
```{r show mediation lat blue 2 }
# Plot mediation
mediation(blue_lat_UV)
plot_med(blue_lat_UV)
```

With prob = 0.95:
```{r show mediation lat blue 3}
mediation(blue_lat_UV, prob=0.95)
```

Please note that as exists_blue (outcome) is binary, direct and indirect effects may be on different scales.

##### Comparaison of predicted and observed values for the mediator and outcome

The faint lines (yrep) are posterior predictive draws, and y is the observed distribution.

```{r show mediation lat blue 4}
plot_answer_med(blue_lat_UV, "Blue", "UVBz")

```


```{r show mediation lat2 UV exists_blue}
##### Latitude quadratic
# mediation(blue_lat2_UV)
# pp_check(blue_lat2_UV, resp = 'exists_blue') + ggtitle('Outcome (exists_blue)')
# pp_check(blue_lat2_UV, resp = 'UVBz') + ggtitle('Mediator (UVR)')
##### Both latitude linear and quadratic
# mediation(blue_latlat2_UV)
# bayes_R2(blue_latlat2_UV)
# 
# plot_answer_med(blue_latlat2_UV, "exists_blue", "UVBz")
# plot_med(blue_latlat2_UV)
# plot(blue_latlat2_UV)
``` 



#### Putative latitude of language family origin --> UVR --> exists_blue 

##### Bayes model result and diagnostic

```{r show mediation latfam blue 1}
# Plot Bayes model
summary(blue_latfam_UV)
plot(blue_latfam_UV)
```

##### Mediation result

With prob = 0.9:

```{r show mediation latfam blue 2 }
# Plot mediation
mediation(blue_latfam_UV)
plot_med(blue_latfam_UV)
```

With prob = 0.95:

```{r show mediation latfam blue 3}
mediation(blue_latfam_UV, prob=0.95)

```

Please note that as exists_blue (outcome) is binary, direct and indirect effects may be on different scales.

##### Comparaison of predicted and observed values for the mediator and outcome

The faint lines (yrep) are posterior predictive draws, and y is the observed distribution.

```{r show mediation latfam blue 4}
plot_answer_med(blue_latfam_UV, "Blue", "UVBz")

```



```{r show mediation latfam2 UV exists_blue}
##### Latitude quadratic
# mediation(blue_latfam2_UV)
# pp_check(blue_latfam2_UV, resp = 'exists_blue') + ggtitle('Outcome (exists_blue)')
# pp_check(blue_latfam2_UV, resp = 'UVBz') + ggtitle('Mediator (UVR)')

##### Both latitude linear and quadratic

# summary(blue_latfamlatfam2_UV)
# mediation(blue_latfamlatfam2_UV)
# bayes_R2(blue_latfamlatfam2_UV)
# 
# plot_answer_med(blue_latfamlatfam2_UV, "exists_blue", "UVBz")
# plot_med(blue_latfamlatfam2_UV)
# plot(blue_latfamlatfam2_UV)
``` 

#### Conclusions

The mediation analysis shows that UVR has both a direct and an indirect effect on exists_blue, the latter mediated by the number of color categories (when having macroarea and family as random effects).

On the other hand, latitude (and latitude^2^) does have an effect on exists_blue, but this is fully mediated by UVR incidence (when having macroarea and family as random effects).


### Regression models

#### UVR incidence only

We model here the effects of UVR incidence on exists_blue using Bayesian mixed-effects models (implemented in package `brms`; @brms_2018). 
Because the repartition of populations is uneven with respect to geographical location and language family, we used language family and macroarea as random effects in all our models.

```{r brm blue on uvb}
if( !file.exists("./cached_results/blue_uvb.RData") )
{
  # fit model, with only UVB as a fixed effect
  model_uvb_rs <- brm(exists_blue ~ UVB_r + (1 + UVB_r | macroarea) + (1 + UVB_r | glottocode_family), 
                      data=d_colors, # very similar notation to lmer
                      family=bernoulli(link = "logit"),
                      cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control); 
  summary(model_uvb_rs);
  loo_uvb_rs <- loo(model_uvb_rs); waic_uvb_rs <- waic(model_uvb_rs); kfold_uvb_rs <- kfold(model_uvb_rs, chains=1);
  
  # Check if the random slopes are needed or not:
  model_uvb_nors <- update(model_uvb_rs, . ~ . - (1 + UVB_r | macroarea) - (1 + UVB_r | glottocode_family) + (1 | macroarea) + (1 | glottocode_family), cores=brms_ncores);
  summary(model_uvb_nors);
  loo_uvb_nors <- loo(model_uvb_nors); waic_uvb_nors <- waic(model_uvb_nors); kfold_uvb_nors <- kfold(model_uvb_nors, chains=1);
  (bf_rs_nors <- bayes_factor(model_uvb_rs, model_uvb_nors)); BF_interpretation(bf_rs_nors$bf, "with random slopes", "random intercepts only") 
  loo_compare(loo_uvb_rs, loo_uvb_nors);
  loo_compare(waic_uvb_rs, waic_uvb_nors);
  loo_compare(kfold_uvb_rs, kfold_uvb_nors);
  model_weights(model_uvb_rs, model_uvb_nors, weights = "waic");
  
  model_uvb <- model_uvb_nors; # random slopes are not needed
  loo_uvb <- loo_uvb_nors; waic_uvb <- waic_uvb_nors; kfold_uvb <- kfold_uvb_nors;
  
  # compute Rsquared
  Rsquared <- bayes_R2(model_uvb)

  # Check if UVB_r matters:
  model_uvb_0 <- update(model_uvb, . ~ . - UVB_r, cores=brms_ncores);
  summary(model_uvb_nors);
  loo_uvb_0 <- loo(model_uvb_0); waic_uvb_0 <- waic(model_uvb_0); kfold_uvb_0 <- kfold(model_uvb_0, chains=1);
  (bf_uvb_0 <- bayes_factor(model_uvb, model_uvb_0)); BF_interpretation(bf_uvb_0$bf, "with UVR incidence", "without UVR incidence") 
  loo_compare(loo_uvb, loo_uvb_0);
  loo_compare(waic_uvb, waic_uvb_0);
  loo_compare(kfold_uvb, kfold_uvb_0);
  model_weights(model_uvb, model_uvb_0, weights = "waic");
  # Clearly UVB_r is needed!
  
  # initialize variables
  acc_all <- NULL ; prec_all <- NULL ; rec_all <- NULL; # initialize variables
  
  # fit algorithm for N iteration
  for (k in 1:30){
    
    # split data in training and testing set
    data_split <- initial_split(d_colors, prop=0.80, strata = "daltonism")
    data_train <- training(data_split)
    data_test <- testing(data_split)
  
    # fit model on training dataset
    fit_train <-  brm(exists_blue ~  UVB_r + (1 | macroarea) + (1 | glottocode_family),
                      data=data_train,
                      family=bernoulli(link = "logit"),
                      cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);
  
    # create confusion matrix
    Pred <- predict(fit_train, type = "response", newdata= data_test, allow_new_levels=TRUE)
    Pred <- if_else(Pred[,1] > 0.5, 1, 0)
    ConfusionMatrix <- table(Pred, pull(data_test, exists_blue)) #`pull` results in a vector

    # compute classification rate, precision, recall andsave those data in table
    acc_all[k] <- sum(diag(ConfusionMatrix)) / sum(ConfusionMatrix)
    prec_all[k] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[1,2])
    rec_all[k] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[2,1])
  }
  
  # mean those values
  accuracy_brms <- mean(acc_all)
  precision_brms <- mean(prec_all)
  recall_brms <- mean(rec_all)

  # cache these results:
  save(model_uvb, Rsquared, bf_rs_nors, bf_uvb_0,
       loo_uvb_rs, loo_uvb_nors, waic_uvb_rs, waic_uvb_nors, kfold_uvb_rs, kfold_uvb_nors, loo_uvb_0, waic_uvb_0 , kfold_uvb_0,
       accuracy_brms, recall_brms, precision_brms,
       file="./cached_results/blue_uvb.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_uvb.RData")
}

summary(model_uvb)
(m_hdi <- hdi(model_uvb, ci=brms_ci)); rope(model_uvb, range=brms_rope, ci=brms_ci); p_rope(model_uvb, range=brms_rope);
```

While we also included random slopes, they did not make any significant contribution, and we retained the random intercepts-only model (Bayes factor `r round(bf_rs_nors$bf,2)`, `r BF_interpretation(bf_rs_nors$bf, "with random slopes", "random intercepts only")`).

UVR has a *strong negative* effect on exists_blue (Bayes factor `r sprintf("%.2f",bf_uvb_0$bf)`, `r BF_interpretation(bf_uvb_0$bf, "model with UVR incidence", "null model")`; `r brms_ci*100`% HDI for *&beta;*~UVR~ = [`r round(m_hdi$CI_low[m_hdi$Parameter == "b_UVB_r"],2)`, `r round(m_hdi$CI_high[m_hdi$Parameter == "b_UVB_r"],2)`]), explaining R^2^ = `r round(Rsquared[1]*100,2)`% of the variance, and correctly classifying `r round(accuracy_brms*100, 2)`% of the populations (recall = `r round(recall_brms*100, 2)`%, precision = `r round(precision_brms*100,2)`%).


#### Adding covariates

##### Using Bayes factors

The best model is:

```{r brm blue on uvb best}
if( !file.exists("./cached_results/blue_uvb_best.RData") )
{
  # adding number of color categories brought 25468 (bayes factor)
  # adding latitude_family quadratic brought 1729
  # adding latitude quadratic brought 1205
  # adding number of color cat quadratic brought 1131 (bayes factor)
  # adding latitude_family brought 181
  # adding distance to lakes brought 143 (bayes factor)
  # adding UVB  brought 93 (bayes factor)
  # adding longitude_family quadratic brought 93 (bayes factor)
  # adding latitude brought 79 (bayes factor)
  # adding subsistence brought 14 (bayes factors)
  # adding longitude brought 7.66 (bayes factor)
  # adding longitude_family brought 5.47 (bayes factor)
  
  bestmodel <- brm(exists_blue ~ UVB_r + n_colors + dist2lakes_r + subsistence + latitude_family_r + longitude_family_r + latitude_r + longitude_r + I(latitude_family_r^2) + I(longitude_family_r^2) +  I(latitude_r^2) + I(n_colors^2) + (1 | macroarea) + (1 | glottocode_family), 
                   data=d_colors, # very similar notation to lmer
                   family=bernoulli(link = "logit"),
                   cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);
  
  
  # slightly better with popSize: 1.90 (Bayes factor)
  # slightly better with daltonism: 1.46 (Bayes factor)
  # slightly worst with clim_PC2: 0.71 (Bayes factor)
  # slightly worst with clim_PC1: 0.66 (Bayes factor)
  # really worst with altitude_rescaled: 0.36 (Bayes factor)
  # really worst with dist2rivers: 0.004 (Bayes factor)
  # really worst with dist2oceans: 0.01 (Bayes factor)
  # really worst with dist2water: 0.06 (Bayes factor)
  # really worst with longitude quadratic: 0.08
  
  # cache these results:
  save(bestmodel,
       file="./cached_results/blue_uvb_best.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_uvb_best.RData")
}

summary(bestmodel)
hdi(bestmodel, ci=brms_ci); rope(bestmodel, range=brms_rope, ci=brms_ci); p_rope(bestmodel, range=brms_rope);

plot(
  bestmodel,
  pars = NA,
  combo = c("dens", "trace")
)
```

###### Multicollinearity

We used the *variance inflation factor* (VIF) with a threshold of 5, to detect multicollinearity between the covariates in this model.

```{r blue on uvb best vif}
# use glm because vif function does not support brms format
mc_best <- glm(exists_blue ~ UVB_r + n_colors + dist2lakes_r + subsistence + latitude_family_r + longitude_family_r + latitude_r + longitude_r, data=d_colors, family=binomial(link = "logit")); 

pander(vif(mc_best))
```

UVR and latitude have the highest VIF scores in our dataset, but, according to our mediation analysis, latitude has an effect on exists_blue only through UVR. 
We compared (using both Bayes factors and K-fold) the following four models:

 -  *model_nolat*: UVR as only predictor
 -  *model_nouvb_quadr*: latitude^2^ as only predictor
 -  *model_nouvb_lin*: latitude as only predictor
 -  *model_nouvb*: latitude and latitude^2^ as predictors

```{r brm blue on uvb best vif}
if( !file.exists("./cached_results/blue_uvb_best_vif.RData") )
{
  # model with only UVB (no latitude normal and quadratic)
  model_nolat <- brm(exists_blue ~ UVB_r + (1 | macroarea) + (1 | glottocode_family), 
                     data=d_colors, # very similar notation to lmer
                     family=bernoulli(link = "logit"),
                     cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);
  
  
  # model with only latitude quadratic (nouvb)
  model_nouvb_quadr <- brm(exists_blue ~ I(latitude_r^2) + (1 | macroarea) + (1 | glottocode_family), 
                           data=d_colors, # very similar notation to lmer
                           family=bernoulli(link = "logit"),
                           cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);
  
  # model with only latitude linear (no uvb)
  model_nouvb_lin <- brm(exists_blue ~ latitude_r + (1 | macroarea) + (1 | glottocode_family), 
                         data=d_colors, # very similar notation to lmer
                         family=bernoulli(link = "logit"),
                         cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);
  
  # model with both latitude quadratic linear (no uvb)
  model_nouvb <- brm(exists_blue ~ I(latitude_r^2) + latitude_r + (1 | macroarea) + (1 | glottocode_family), 
                     data=d_colors, # very similar notation to lmer
                     family=bernoulli(link = "logit"),
                     cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);
  
  #bestmodel_nouvb <- add_ic(bestmodel_nouvb, ic = c("loo", "waic"), reloo=TRUE)
  kfold_nouvb       <- kfold(model_nouvb, chains = 1)
  kfold_nouvb_lin   <- kfold(model_nouvb_lin, chains = 1)
  kfold_nouvb_quadr <- kfold(model_nouvb_quadr, chains = 1)
  
  #bestmodel_nolat <- add_ic(bestmodel_nolat, ic = c("loo", "waic"), reloo=TRUE)
  kfold_nolat <- kfold(model_nolat, chains = 1)
  
  ### MODEL COMPARAISON
  
  # with bayes
  bf_model_nolat_nouvb       <- bayes_factor(model_nolat, model_nouvb);  # --> evidence for having both latitude linear and quadratic, against only uvb
  bf_model_nolat_nouvb_lin   <- bayes_factor(model_nolat, model_nouvb_lin);  # --> evidence for having UVB instead of latitude linear
  bf_model_nolat_nouvb_quadr <- bayes_factor(model_nolat, model_nouvb_quadr);  # --> evidence for having UVB instead of latitude quadratic
  
  # with KFOLD, WAIC and LOO
  waic_loo_model_nolat_nouvb       <- loo_compare(kfold_nolat, kfold_nouvb) 
  waic_loo_model_nolat_nouvb_lin   <- loo_compare(kfold_nolat, kfold_nouvb_lin) 
  waic_loo_model_nolat_nouvb_quadr <- loo_compare(kfold_nolat, kfold_nouvb_quadr) 
  
  # cache these results:
  save(model_nolat, model_nouvb_quadr, model_nouvb_lin, model_nouvb, 
       kfold_nouvb, kfold_nouvb_lin, kfold_nouvb_quadr, kfold_nolat,
       bf_model_nolat_nouvb, bf_model_nolat_nouvb_lin, bf_model_nolat_nouvb_quadr,
       waic_loo_model_nolat_nouvb, waic_loo_model_nolat_nouvb_lin, waic_loo_model_nolat_nouvb_quadr,
       file="./cached_results/blue_uvb_best_vif.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_uvb_best_vif.RData")
}

# Bayes factors:
cat(paste0("Bayes factor: model_nolat vs model_nouvb: ",round(bf_model_nolat_nouvb$bf,3),"; ",BF_interpretation(bf_model_nolat_nouvb$bf, "model_nolat", "model_nouvb"),".\n"));
cat(paste0("Bayes factor: model_nolat vs model_nouvb_lin: ",round(bf_model_nolat_nouvb_lin$bf,3),"; ",BF_interpretation(bf_model_nolat_nouvb_lin$bf, "model_nolat", "model_nouvb_lin"),".\n"));
cat(paste0("Bayes factor: model_nolat vs model_nouvb_quadr: ",round(bf_model_nolat_nouvb_quadr$bf,3),"; ",BF_interpretation(bf_model_nolat_nouvb_quadr$bf, "model_nolat", "model_nouvb_quadr"),".\n"));

# KFOLD, WAIC and LOO:
waic_loo_model_nolat_nouvb;
waic_loo_model_nolat_nouvb_lin; 
waic_loo_model_nolat_nouvb_quadr;
```

With Bayes factors: (latitude + latitude^2^) > **UVR** > [latitude^2^ & latitude]

With LOO/WAIC/KFOLD: all models seem comparable (the difference between models is smaller than the SE).

Let's remove UVR from our best complex model:

```{r brm blue best no uvb}
if( !file.exists("./cached_results/blue_uvb_remove_uvb.RData") )
{
  # best model without UVB
  bestmodel_nouvb <- brm(exists_blue ~ n_colors + dist2lakes_r + subsistence + latitude_family_r + longitude_family_r + latitude_r + longitude_r + 
                           I(latitude_family_r^2) + I(longitude_family_r^2) +  I(latitude_r^2) + I(n_colors^2) + (1 | macroarea) + (1 | glottocode_family), 
                         data=d_colors, # very similar notation to lmer
                         family=bernoulli(link = "logit"),
                         cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);
  
  # Bayes factor
  bf_best_nouvb <- bayes_factor(bestmodel, bestmodel_nouvb); 
  
  # cache these results:
  save(bestmodel_nouvb, bf_best_nouvb,
       file="./cached_results/blue_uvb_remove_uvb.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_uvb_remove_uvb.RData")
}

# BF:
cat(paste0("Bayes factor: model_nolat vs model_nouvb: ",round(bf_best_nouvb$bf,3),"; ",BF_interpretation(bf_best_nouvb$bf, "best model", "best model without UVB"),".\n"));
```

... and let's remove latitude from our best model (quadratic and linear):

```{r  brm blue best no lat}
if( !file.exists("./cached_results/blue_uvb_remove_lat.RData") )
{
  # best model without latitude
  bestmodel_nolat <- brm(exists_blue ~ n_colors + dist2lakes_r + subsistence + latitude_family_r + longitude_family_r  + longitude_r + 
                           I(latitude_family_r^2) + I(longitude_family_r^2) + I(n_colors^2) + (1 | macroarea) + (1 | glottocode_family), 
                         data=d_colors, # very similar notation to lmer
                         family=bernoulli(link = "logit"),
                         cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);
  
  # Bayes factor
  bf_best_nolat <- bayes_factor(bestmodel, bestmodel_nolat);  
  
  # cache these results:
  save(bestmodel_nolat, bf_best_nolat,
       file="./cached_results/blue_uvb_remove_lat.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_uvb_remove_lat.RData")
}

# BF:
cat(paste0("Bayes factor: model_nolat vs model_nouvb: ",round(bf_best_nolat$bf,3),"; ",BF_interpretation(bf_best_nolat$bf, "best model", "best model without latitude and latitude^2"),".\n"));
```

Thus, we should keep, UVR and latitude (and latitude^2^) as predictors of exists_blue. 

This complex model includes many variables. In order to check which variables are the most important, we computed the best model according to LOO, WAIC and KFOLD methods. 


##### Using WAIC, LOO and KFOLD

With this the best model is:

```{r brm best light model}
if( !file.exists("./cached_results/blue_bestlightmodel.RData") )
{
  bestlightmodel <- brm(exists_blue ~ n_colors + dist2lakes + I(n_colors^2) + (1 | macroarea) + (1 | glottocode_family), 
                        data=d_colors, # very similar notation to lmer
                        family=bernoulli(link = "logit"),
                        cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control); 
  
 
  # cache these results:
  save(bestlightmodel,
       file="./cached_results/blue_bestlightmodel.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_bestlightmodel.RData")
}

summary(bestlightmodel)
hdi(bestlightmodel, ci=brms_ci); rope(bestlightmodel, range=brms_rope, ci=brms_ci); p_rope(bestlightmodel, range=brms_rope);

plot(
  bestlightmodel,
  pars = NA,
  combo = c("dens", "trace")
)
```



#### Adding covariates except number of color categories

We use the same approach as before, but this time we remove the number of color categories from the set of potential predictors, as its inclusion is debatable.

##### Using Bayes factors

Whith these, the best fit (according to Bayes factors) is very similar to the best fit obtained before:

```{r uvb best model 2}
# complex best model with Bayes factor for Model 2 (without number of color cat) 
if( !file.exists("./cached_results/blue_bestmodel2.RData") )
{
  bestmodel2 <- brm(exists_blue ~ UVB_r + log_popSize + dist2lakes_r + subsistence + latitude_family_r + longitude_family_r + latitude_r + longitude_r + 
                      I(latitude_family_r^2) + I(longitude_family_r^2) +  I(latitude_r^2)  + (1 | macroarea) + (1 | glottocode_family), 
                    data=d_colors, # very similar notation to lmer
                    family=bernoulli(link = "logit"),
                    cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control); 
  
  # cache these results:
  save(bestmodel2,
       file="./cached_results/blue_bestmodel2.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_bestmodel2.RData")
}

summary(bestmodel2)
hdi(bestmodel2, ci=brms_ci); rope(bestmodel2, range=brms_rope, ci=brms_ci); p_rope(bestmodel2, range=brms_rope);

plot(
  bestmodel2,
  pars = NA,
  combo = c("dens", "trace")
)
```

Cheking the importance of UVR:

```{r uvb best model 2 no uvb}
if( !file.exists("./cached_results/blue_bestmodel2_nouvb.RData") )
{
  bestmodel2_nouvb <- brm(exists_blue ~ log_popSize + dist2lakes_r + subsistence + latitude_family_r + longitude_family_r + latitude_r + longitude_r + 
                            I(latitude_family_r^2) + I(longitude_family_r^2) +  I(latitude_r^2)  + (1 | macroarea) + (1 | glottocode_family), 
                          data=d_colors, # very similar notation to lmer
                          family=bernoulli(link = "logit"),
                          cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control); 
  
  #... compare with Bayes factor:
  bf_bestmodel2_nouvb <- bayes_factor(bestmodel2, bestmodel2_nouvb);  
  
  # cache these results:
  save(bestmodel2_nouvb, bf_bestmodel2_nouvb,
       file="./cached_results/blue_bestmodel2_nouvb.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_bestmodel2_nouvb.RData")
}

# BF:
cat(paste0("Bayes factor: bestmodel2 vs bf_bestmodel2_nouvb: ",round(bf_bestmodel2_nouvb$bf,3),"; ",BF_interpretation(bf_bestmodel2_nouvb$bf, "keeping UVR", "dropping UVR"),".\n"));
```

##### Using WAIC, LOO and KFOLD

With this the best model is:

```{r uvb best model 2 kfold}
if( !file.exists("./cached_results/blue_bestmodel2_kfold.RData") )
{
  bestmodel2_kfold <- brm(exists_blue ~ UVB_r + log_popSize + dist2lakes_r + (1 | macroarea) + (1 | glottocode_family), 
                          data=d_colors, # very similar notation to lmer
                          family=bernoulli(link = "logit"),
                          cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control); 
  
  # cache these results:
  save(bestmodel2_kfold,
       file="./cached_results/blue_bestmodel2_kfold.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_bestmodel2_kfold.RData")
}

summary(bestmodel2_kfold)
hdi(bestmodel2_kfold, ci=brms_ci); rope(bestmodel2_kfold, range=brms_rope, ci=brms_ci); p_rope(bestmodel2_kfold, range=brms_rope);

plot(
  bestmodel2_kfold,
  pars = NA,
  combo = c("dens", "trace")
)

```

Cheking the importance of UVR:

```{r uvb best model 2 kfold no uvb}
if( !file.exists("./cached_results/blue_bestmodel2_kfold_nouvb.RData") )
{
  bestmodel2_kfold_nouvb <- brm(exists_blue ~ log_popSize + dist2lakes_r  + (1 | macroarea) + (1 | glottocode_family), 
                                data=d_colors, # very similar notation to lmer
                                family=bernoulli(link = "logit"),
                                cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control); 
  
  #... compare with Bayes factor:
  bf_bestmodel2_kfold_nouvb <- bayes_factor(bestmodel2_kfold, bestmodel2_kfold_nouvb);  
  # ... and compare with Kfold:
  kfold_nouvb <- kfold(bestmodel2_kfold_nouvb, chains = 1)
  kfold <- kfold(bestmodel2_kfold, chains = 1)
  
  # cache these results:
  save(bestmodel2_kfold_nouvb,
       bf_bestmodel2_kfold_nouvb, kfold_nouvb, kfold,
       file="./cached_results/blue_bestmodel2_kfold_nouvb.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_bestmodel2_kfold_nouvb.RData")
}

# BF
cat(paste0("Bayes factor: bestmodel2_kfold vs bestmodel2_kfold_nouvb: ",round(bf_bestmodel2_kfold_nouvb$bf,3),"; ",BF_interpretation(bf_bestmodel2_kfold_nouvb$bf, "keeping UVR", "dropping UVR"),".\n"));

# LOO
loo_compare(kfold_nouvb, kfold)
```


### Plotting the predictors

**Number of color categories.** As predicted, this has a strong positive effect on exists_blue; thus, the more color categories in a language, the more likely it is to have a word for blue.

**Population Size.** This becomes predictive only if we do not consider the number of color categories (in fact, those two variables are highly positively correlated); thus, the larger the population, the more likely to have a word for blue.

```{r plot predictor blue pop size, fig.cap=capFig("Population size vs having a word for exists_blue.")}
ggplot(d_colors, aes(x=exists_blue, y=log_popSize, color=exists_blue)) + 
  theme_bw() +
  geom_point(alpha=0.45, position=position_jitter(h=0.0,w=0.15)) + 
  geom_violin(alpha=0.3) +
  theme(axis.text.x = element_text(size =15, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =15, face = "bold"),
             axis.title=element_text(size=15),
             legend.title=element_text(size =15),
             legend.text=element_text(size =15)) +
  guides(color=FALSE) + 
  labs(y = "Population size");
```

**UVR.** As suggested by Lindsey and Brown, UVR incidence is an important predictor for exists_blue; the higher the UVR incidence, the less likely it is to have a word for blue.

```{r plot predictor blue uv, fig.cap=capFig("UVR incidence vs having a word for exists_blue.")}
ggplot(d_colors, aes(x=exists_blue, y=UVB_r, color=exists_blue)) + 
  theme_bw() +
  geom_point(alpha=0.45, position=position_jitter(h=0.0,w=0.15)) + 
  geom_violin(alpha=0.3) +
  theme(axis.text.x = element_text(size =15, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =15, face = "bold"),
             axis.title=element_text(size=15),
             legend.title=element_text(size =15),
             legend.text=element_text(size =15)) +
  guides(color=FALSE) + 
  labs(y = "UV incidence (z scored)", x="exists_blue") ;
```

```{r plot predictor blue uv 2, fig.cap=capFig("Map of UVR incidence on having a word for exists_blue.")}
# plot latitude 
ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(data=d_colors, aes(x = longitude, y =latitude, fill=UVB, shape=exists_blue), color="red") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "UVR incidence", shape="exists_blue") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  scale_shape_manual(values=c("no"=24, "yes"=21))
```

**Latitude**. Latitude is highly correlated with UVR incidence, and its effects on exists_blue fully mediated by it; the closer to the equator a language is, the less likely it is to have a specific word for blue.

```{r plot predictor blue lat, fig.cap=capFig("Latitude on having a word for exists_blue.")}
# violin plot latitude 
ggplot(d_colors, aes(x=exists_blue, y=latitude_r, color=exists_blue)) + 
  theme_bw() +
  geom_point(alpha=0.45, position=position_jitter(h=0.0,w=0.15)) + 
  geom_violin(alpha=0.3) +
  theme(axis.text.x = element_text(size =15, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =15, face = "bold"),
             axis.title=element_text(size=15),
             legend.title=element_text(size =15),
             legend.text=element_text(size =15)) +
  guides(color=FALSE) + 
  labs(y = "Latitude") ;

```

**Distance to lakes.** It is unclear why the distance to lakes is a predictor for the presence of words for exists_blue, but it very probably is a proxy for other (unmeasured) variables (see decision trees below); the more distant from lakes a language is, the less likely it is to have a word for blue.

```{r plot predictor blue dist2lakes, fig.cap=capFig("Map of log(dist2lakes) on having a word for exists_blue.")}
# plot latitude 
ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(data=d_colors, aes(x = longitude, y =latitude, fill=log(dist2lakes), shape=exists_blue), color="red") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "log(dist2lakes)", shape="exists_blue") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  scale_shape_manual(values=c("no"=24, "yes"=21))
```

```{r plot predictor blue dist2lakes 2,fig.cap=capFig("log(dist2lakes) on having a word for exists_blue.")}
ggplot(d_colors, aes(x=exists_blue, y=log(dist2lakes), color=exists_blue)) + 
  theme_bw() +
  geom_point(alpha=0.45, position=position_jitter(h=0.0,w=0.15)) + 
  geom_violin(alpha=0.3) +
  theme(axis.text.x = element_text(size =15, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =15, face = "bold"),
             axis.title=element_text(size=15),
             legend.title=element_text(size =15),
             legend.text=element_text(size =15)) +
  guides(color=FALSE) + 
  labs(y = "Distance to lakes (log)") ;
```

**Latitude and longitude of putative language family origins**. Depending on the model selection method used, these are included or not in the set of relevant predictors.

```{r plot predictor blue latfam, fig.cap=capFig("Map of latitude of language family origins on having a word for exists_blue.")}
# plot latitude of language family origins
ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(data= d_colors, aes(x = longitude, y = latitude, color=latitude_family_r, shape=exists_blue)) +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(color = "Latitude of language family origin", shape="exists_blue") 
```

```{r plot predictor blue lonfam, fig.cap=capFig("Map of longitude of language family origins on having a word for exists_blue.")}
# plot longitude of language family origins
ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(data=d_colors, aes(x = longitude, y = latitude, color=longitude_family_r, shape=exists_blue)) +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(color = "Longitude of language family origin", shape="exists_blue") 
```

```{r plot predictor blue latfam and lonfam, fig.cap=capFig("Location of putative origins of language family on having a word for exists_blue.")}
# violin plot: latitude on the left, longitude on the right
plat <- ggplot(d_colors, aes(x=exists_blue, y=latitude_family_r, color=exists_blue)) + 
  theme_bw() +
  geom_point(alpha=0.45, position=position_jitter(h=0.0,w=0.15)) + 
  geom_violin(alpha=0.3) +
  theme(axis.text.x = element_text(size =15, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =15, face = "bold"),
             axis.title=element_text(size=15),
             legend.title=element_text(size =15),
             legend.text=element_text(size =15)) +
  guides(color=FALSE) + 
  labs(y = "Latitude of language family origin") ;

plon <- ggplot(d_colors, aes(x=exists_blue, y=longitude_family_r, color=exists_blue)) + 
  theme_bw() +
  geom_point(alpha=0.45, position=position_jitter(h=0.0,w=0.15)) + 
  geom_violin(alpha=0.3) +
  theme(axis.text.x = element_text(size =15, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =15, face = "bold"),
             axis.title=element_text(size=15),
             legend.title=element_text(size =15),
             legend.text=element_text(size =15)) +
  guides(color=FALSE) + 
  labs(y = "Longitude of language family origin") ;

grid.arrange(plat, plon, ncol=2) 
```

**subsistence**. The effect of subsistence strategies is one of the weakest predictors, with hunter-gatherers (subsistence) being less likely to possess a specific term for blue.

```{r plot predictor blue subsistence, fig.cap=capFig("Map of subsistence strategy on having a word for exists_blue.")}
# plot latitude 
ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(data=d_colors, aes(x = longitude, y =latitude, color=subsistence, shape=exists_blue)) +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(color = "Hunting-gathering?", shape="exists_blue") +
  scale_shape_manual(values=c("no"=24, "yes"=21)) + 
  scale_color_manual(values=c("no"="red", "yes"="blue"))

tmp <- d_colors[,c("subsistence", "exists_blue")];
tmp$subsistence <- paste0("subsistence:",tmp$subsistence);
tmp$exists_blue <- paste0("exists_blue:",tmp$exists_blue);
pander(table(tmp))
```


### Random forests

We used random forests to estimate of the relative importance of our variables in predicting the existence of words for exists_blue.

*Random Forests* are supervised machine learning algorithms which can be used for both classification and regression. It constructs a multitude of decision trees on the training sample and gets the prediction from each of the tree (bagging). Then, it selects the best solution by means of voting. Compared to classic decision tree methods, random forests algorithm structure avoid overfitting on training data.

Relative importance of our variables can be measured with two indexes: *gini impurity* and *accuracy-based* method. While *Gini impurity* is a measurement of the likelihood of incorrect classification of randomly classified new instances, *accuracy-based* methods use the mean dicrease in classification accuracy after permuting each element over all trees. 
Because the choice of training/testing dataset matters, we will run the algorithm several times with random allocations of the training and testing sets, and computed the mean variable importance.

However, it should be note that random forests have troubles modelling shared variance due to hierarchical structure, here in particular due to language family and macroarea.

See https://www.displayr.com/how-is-variable-importance-calculated-for-a-random-forest/ for details:

**Gini-based importance**: When a tree is built, the decision about which variable to split at each node uses a calculation of the Gini impurity. For each variable, the sum of the Gini decrease across every tree of the forest, every time that variable is chosen to split a node. The sum is divided by the number of trees in the forest to give an average. The scale is irrelevant: only the relative values matter. There is a bias towards using numeric variables to split nodes because there are potentially many split points.

**Accuracy-based importance**: Each tree has its testing sample of data that was not used during construction. This sample is used to calculate importance of a specific variable. First, the prediction accuracy on the testing sample is measured. Then, the values of the variable in the testing sample are randomly shuffled, keeping all other variables the same. Finally, the decrease in prediction accuracy on the shuffled data is measured. The mean decrease in accuracy across all trees is reported. This importance measure is also broken down by outcome class. 
Intuitively, the random shuffling means that, on average, the shuffled variable has no predictive power. This importance is a measure of by how much removing a variable decreases accuracy, and vice versa — by how much including a variable increases accuracy. Note that if a variable has very little predictive power, shuffling may lead to a slight increase in accuracy due to random noise. This in turn can give rise to small negative importance scores, which can be essentially regarded as equivalent to zero importance.


#### All predictors

```{r variable importance blue}
if( !file.exists("./cached_results/blue_var_importance.RData") )
{
  
  # choose variable to analyze
  list_variables <- c("UVB_r" , "daltonism" , "subsistence" , "latitude_r", "longitude_r" , "log_popSize" , "clim_PC1", "clim_PC2" , "altitude_r"  , "dist2water_r", "dist2ocean_r" , "dist2lakes_r", "dist2rivers_r" , "n_colors" , "latitude_family_r", "longitude_family_r")
  
  # initialize empty dataframe and list for gini index (randomForest algorithm)
  vimp_gini <- data.frame(matrix(NA, nrow = 1, ncol = length(list_variables)))
  colnames(vimp_gini) <- list_variables
  accuracy_rf <- NULL; recall_rf <- NULL; precision_rf <- NULL

  # initialize empty dataframe for accuracy index (randomForest algorithm)
  vimp_permut1 <- data.frame(matrix(NA, nrow = 1, ncol = length(list_variables)))
  colnames(vimp_permut1) <- list_variables
  
  # initialize empty dataframe for accuracy index (cForest algorithm)
  vimp_permut2 <- data.frame(matrix(NA, nrow = 1, ncol = length(list_variables)))
  colnames(vimp_permut2) <- list_variables
  accuracy_cf <- NULL; recall_cf <- NULL; precision_cf <- NULL

  # choose number of iteration
  n_iter_gini <- 50;
  
  for (it in 1:n_iter_gini)
  {
    # compute random forest algorithm on all variable except multicollinear
    rf.db = randomForest(exists_blue ~  UVB_r + daltonism  + subsistence + latitude_r + longitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r + n_colors + latitude_family_r + longitude_family_r, data = d_colors, importance=TRUE)
    
    # compute classification accuracy, precision and recall for randomForest algorithm
    ConfusionMatrix = rf.db$confusion[,1:2]
    accuracy_rf[it] <- sum(diag(ConfusionMatrix)) / sum(ConfusionMatrix)
    precision_rf[it] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[1,2])
    recall_rf[it] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[2,1])

    # compute gini-based variable importance (randomForest)
    var_imp_gini <- data.frame(importance(rf.db, type=2))
    
    # compute accuracy-based variable importance( andomForest)
    var_imp_permut <- data.frame(importance(rf.db, type=1))
    
    # use an other algorithm to compute the accuracy-based variable importance
    obj <- cforest(exists_blue ~  UVB_r + daltonism  + subsistence +  latitude_r +longitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r + n_colors + latitude_family_r + longitude_family_r, data = d_colors)
    
    # compute classification accuracy, precision and recall for cforest algorithm
    oobPredicted <- predict(obj,OOB=T)
    ConfusionMatrix <- table(d_colors$exists_blue, oobPredicted)
    accuracy_cf[it] <- sum(diag(ConfusionMatrix)) / sum(ConfusionMatrix)
    precision_cf[it] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[1,2])
    recall_cf[it] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[2,1])

    # compute accuracy-based variable importance (cForest)
    var_imp_permut2 <- varimp(obj)
    
    # fill dataframe with variables importance values
    for (variable in list_variables){
      vimp_gini[it,variable] <- var_imp_gini[variable, 1]
      vimp_permut1[it,variable] <- var_imp_permut[variable, 1]
      vimp_permut2[it,variable] <- as.numeric(var_imp_permut2[variable])
      
    }
  }
  
  
  # mean all iterations
  vimp_gini <- colMeans(vimp_gini, na.rm=TRUE)
  vimp_gini.df <- data.frame(namevar = as.character(names(vimp_gini)), mean_all = as.numeric(vimp_gini))
  
  vimp_permut1 <- colMeans(vimp_permut1, na.rm=TRUE)
  vimp_permut1.df <- data.frame(namevar = as.character(names(vimp_permut1)), mean_all = as.numeric(vimp_permut1))
  
  vimp_permut2 <- colMeans(vimp_permut2, na.rm=TRUE)
  vimp_permut2.df <- data.frame(namevar = as.character(names(vimp_permut2)), mean_all = as.numeric(vimp_permut2))
  
  mean_acc_rf <- mean(accuracy_rf)
  mean_prec_rf <- mean(precision_rf)
  mean_rec_rf <- mean(recall_rf)
  
  mean_acc_cf <- mean(accuracy_cf)
  mean_prec_cf <- mean(precision_cf)
  mean_rec_cf <- mean(recall_cf)
  
  # cache these results:
  save(list_variables, vimp_gini.df, vimp_permut1.df, vimp_permut2.df, n_iter_gini, mean_acc_rf, mean_prec_rf, mean_rec_rf,  mean_acc_cf, mean_prec_cf, mean_rec_cf,
       file="./cached_results/blue_var_importance.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_var_importance.RData")
}
```

The `randomForest` algorithm correctly classifies `r round(mean_acc_rf*100,2)` % of the populations (recall = `r round(mean_rec_rf*100,2)` and precision `r round(mean_prec_rf*100,2)`).

```{r plot randomforst blue gini, fig.width=5, fig.cap=capFig("Variable importance according to randomForest algorithm using the Gini index.")}
# Plot the result for GINI
ggplot(mapping = aes(x = reorder(namevar, mean_all), y = mean_all), 
       data = vimp_gini.df) +
       theme_bw() +
       geom_col(fill="gold", color="black") +
       labs(x ="", y = paste0("Mean of ",n_iter_gini," iterations")) +
       theme(axis.text.x = element_text(angle = 60, hjust = 1, size =13, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =13, face = "bold"),
             axis.title=element_text(size=16),
             legend.position="none") +
       scale_x_discrete(labels=c("subsistence" = "Subsistence mode", "longitude_family_r" = "Long of family origin", "latitude_family_r" = "Lat of family origin", "dist2ocean_r" = "Dist to ocean","dist2rivers_r" = "Dist to rivers", "longitude_r" = "Longitude", "altitude_r" = "Altitude", "dist2water_r" = "Dist to water", "clim_PC1" = "Climate PC1", "clim_PC2" = "Climate PC2", "dist2lakes_r" = "Dist to lakes", "latitude_r" = "Latitude", "log_popSize" = "Population size", "UVB_r" = "UVR","daltonism" = "daltonism", "n_colors" = "Nb of colour categ")) +
       guides(fill=FALSE)
```

```{r plot randomforest blue acc, fig.width=5, fig.cap=capFig("Variable importance according to randomForest algorithm using the accuracy index.")}
# Plot the result for 
ggplot(mapping = aes(x = reorder(namevar, mean_all), y = mean_all), 
       data = vimp_permut1.df) +
       theme_bw() +
       geom_col(fill="gold", color="black") +
       labs(x ="", y = paste0("Mean of ",n_iter_gini," iterations")) +
       theme(axis.text.x = element_text(angle = 60, hjust = 1, size =13, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =13, face = "bold"),
             axis.title=element_text(size=16),
             legend.position="none") +
  scale_x_discrete(labels=c("subsistence" = "Subsistence mode", "longitude_family_r" = "Long of family origin", "latitude_family_r" = "Lat of family origin", "dist2ocean_r" = "Dist to ocean","dist2rivers_r" = "Dist to rivers", "longitude_r" = "Longitude", "altitude_r" = "Altitude", "dist2water_r" = "Dist to water", "clim_PC1" = "Climate PC1", "clim_PC2" = "Climate PC2", "dist2lakes_r" = "Dist to lakes", "latitude_r" = "Latitude", "log_popSize" = "Population size", "UVB_r" = "UVR","daltonism" = "daltonism", "n_colors" = "Nb of colour categ")) +
        guides(fill=FALSE)
```

The `cForest` algorithm correctly classifies `r round(mean_acc_cf*100,2)` % of the populations (recall = `r round(mean_rec_cf*100,2)` and precision `r round(mean_prec_cf*100,2)`).


```{r plot cforest blue acc, fig.width=5, fig.cap=capFig("Variable importance according to cforest algorithm using the accuracy index.")}
# Plot the result
ggplot(mapping = aes(x = reorder(namevar, mean_all), y = mean_all), 
       data = vimp_permut2.df) +
       theme_bw() +
       geom_col(fill="gold", color="black") +
       labs(x ="", y = paste0("Mean of ",n_iter_gini," iterations")) +
       theme(axis.text.x = element_text(angle = 60, hjust = 1, size =13, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =13, face = "bold"),
             axis.title=element_text(size=16),
             legend.position="none") +
  scale_x_discrete(labels=c("subsistence" = "Subsistence mode", "longitude_family_r" = "Long of family origin", "latitude_family_r" = "Lat of family origin", "dist2ocean_r" = "Dist to ocean","dist2rivers_r" = "Dist to rivers", "longitude_r" = "Longitude", "altitude_r" = "Altitude", "dist2water_r" = "Dist to water", "clim_PC1" = "Climate PC1", "clim_PC2" = "Climate PC2", "dist2lakes_r" = "Dist to lakes", "latitude_r" = "Latitude", "log_popSize" = "Population size", "UVB_r" = "UVR","daltonism" = "daltonism", "n_colors" = "Nb of colour categ")) +
      guides(fill=FALSE)
```

Here, we find again the *number of color categories* as the most important predictor, followed by *UVR* incidence, *daltonism*, *population size*, *latitude* and *distance to lakes*.
Thus, we found relatively similar results to those form the regression analysis, but also differences: daltonism was not predictive in the regression models, population size only when excluding the number color categories, and the latitude and longitude of language family origins do not appear to be important variables here.


#### Excluding the number of color categories

```{r variable importance blue no nbcolors}
if( !file.exists("./cached_results/blue_nonbcolors_var_importance.RData") )
{
  
  # choose variable to analyze
  list_variables <- c("UVB_r" , "daltonism"  , "subsistence" , "latitude_r", "longitude_r" , "log_popSize" , "clim_PC1", "clim_PC2" , "altitude_r"  , "dist2water_r", "dist2ocean_r" , "dist2lakes_r", "dist2rivers_r"  , "latitude_family_r", "longitude_family_r")
  
  # initialize empty dataframe and list for gini index (randomForest algorithm)
  vimp_gini <- data.frame(matrix(NA, nrow = 1, ncol = length(list_variables)))
  colnames(vimp_gini) <- list_variables
  accuracy_rf <- NULL; recall_rf <- NULL; precision_rf <- NULL

  # initialize empty dataframe for accuracy index (randomForest algorithm)
  vimp_permut1 <- data.frame(matrix(NA, nrow = 1, ncol = length(list_variables)))
  colnames(vimp_permut1) <- list_variables
  
  # initialize empty dataframe for accuracy index (cForest algorithm)
  vimp_permut2 <- data.frame(matrix(NA, nrow = 1, ncol = length(list_variables)))
  colnames(vimp_permut2) <- list_variables
  accuracy_cf <- NULL; recall_cf <- NULL; precision_cf <- NULL

  # choose number of iteration
  n_iter_gini <- 50;
  
  for (it in 1:n_iter_gini){
    # compute random forest algorithm on all variable except multicollinear
    rf.db = randomForest(exists_blue ~  UVB_r + daltonism  + subsistence + latitude_r + longitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r + dist2water_r  + dist2ocean_r + dist2lakes_r + dist2rivers_r  + latitude_family_r + longitude_family_r, data = d_colors, importance=TRUE)
      
    # compute classification accuracy, precision and recall for randomForest algorithm
    ConfusionMatrix = rf.db$confusion[,1:2]
    accuracy_rf[it] <- sum(diag(ConfusionMatrix)) / sum(ConfusionMatrix)
    precision_rf[it] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[1,2])
    recall_rf[it] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[2,1])

    # compute gini-based variable importance (randomForest)
    var_imp_gini <- data.frame(importance(rf.db, type=2))
    
    # compute accuracy-based variable importance( andomForest)
    var_imp_permut <- data.frame(importance(rf.db, type=1))

    # use an other algorithm to compute the accuracy-based variable importance
    obj <- cforest(exists_blue ~  UVB_r + daltonism  + subsistence + latitude_r + longitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r  + latitude_family_r + longitude_family_r, data = d_colors)
    
    # compute classification accuracy, precision and recall for cforest algorithm
    oobPredicted <- predict(obj,OOB=T)
    ConfusionMatrix <- table(d_colors$exists_blue, oobPredicted)
    accuracy_cf[it] <- sum(diag(ConfusionMatrix)) / sum(ConfusionMatrix)
    precision_cf[it] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[1,2])
    recall_cf[it] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[2,1])

    # compute accuracy-based variable importance (cForest)
    var_imp_permut2 <- varimp(obj)

    # fill dataframe with variables importance values
    for (variable in list_variables){
      vimp_gini[it,variable] <- var_imp_gini[variable, 1]
      vimp_permut1[it,variable] <- var_imp_permut[variable, 1]
      vimp_permut2[it,variable] <- as.numeric(var_imp_permut2[variable])
      
    }
  }
  
  # mean all iterations
  vimp_gini <- colMeans(vimp_gini, na.rm=TRUE)
  vimp_gini.df <- data.frame(namevar = as.character(names(vimp_gini)), mean_all = as.numeric(vimp_gini))
  
  vimp_permut1 <- colMeans(vimp_permut1, na.rm=TRUE)
  vimp_permut1.df <- data.frame(namevar = as.character(names(vimp_permut1)), mean_all = as.numeric(vimp_permut1))
  
  vimp_permut2 <- colMeans(vimp_permut2, na.rm=TRUE)
  vimp_permut2.df <- data.frame(namevar = as.character(names(vimp_permut2)), mean_all = as.numeric(vimp_permut2))
  
  mean_acc_rf <- mean(accuracy_rf)
  mean_prec_rf <- mean(precision_rf)
  mean_rec_rf <- mean(recall_rf)
  
  mean_acc_cf <- mean(accuracy_cf)
  mean_prec_cf <- mean(precision_cf)
  mean_rec_cf <- mean(recall_cf)

  
  # cache these results:
  save(list_variables, vimp_gini.df, vimp_permut1.df, vimp_permut2.df,  mean_acc_rf, mean_prec_rf, mean_rec_rf,  mean_acc_cf, mean_prec_cf, mean_rec_cf,
       file="./cached_results/blue_nonbcolors_var_importance.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_nonbcolors_var_importance.RData")
}
```

The `randomForest` algorithm correctly classifies `r round(mean_acc_rf*100,2)` % of the populations (recall = `r round(mean_rec_rf*100,2)` and precision `r round(mean_prec_rf*100,2)`).

```{r plot randomforest blue no nbcolorcat gini, fig.width=5, fig.cap=capFig("Variable importance according to randomForest algorithm using the Gini index.")}
# Plot the result
ggplot(mapping = aes(x = reorder(namevar, mean_all), y = mean_all), 
       data = vimp_gini.df) +
       theme_bw() +
       geom_col(fill="skyblue", color="black") +
       labs(x ="", y = paste0("Mean of ",n_iter_gini," iterations")) +
       theme(axis.text.x = element_text(angle = 60, hjust = 1, size =13, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =13, face = "bold"),
             axis.title=element_text(size=16),
             legend.position="none") +
  scale_x_discrete(labels=c("subsistence" = "Subsistence mode", "longitude_family_r" = "Long of family origin", "latitude_family_r" = "Lat of family origin", "dist2ocean_r" = "Dist to ocean","dist2rivers_r" = "Dist to rivers", "longitude_r" = "Longitude", "altitude_r" = "Altitude", "dist2water_r" = "Dist to water", "clim_PC1" = "Climate PC1", "clim_PC2" = "Climate PC2", "dist2lakes_r" = "Dist to lakes", "latitude_r" = "Latitude", "log_popSize" = "Population size", "UVB_r" = "UVR","daltonism" = "daltonism")) +
       guides(fill=FALSE)
```

```{r plot randomforest blue no nbcolorcat acc, fig.width=5, fig.cap=capFig("Variable importance according to randomForest algorithm using the accuracy index.")}
# Plot the result
ggplot(mapping = aes(x = reorder(namevar, mean_all), y = mean_all), 
       data = vimp_permut1.df) +
       theme_bw() +
       geom_col(fill="skyblue", color="black") +
       labs(x ="", y = paste0("Mean of ",n_iter_gini," iterations")) +
       theme(axis.text.x = element_text(angle = 60, hjust = 1, size =13, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =13, face = "bold"),
             axis.title=element_text(size=16),
             legend.position="none") +
    scale_x_discrete(labels=c("subsistence" = "Subsistence mode", "longitude_family_r" = "Long of family origin", "latitude_family_r" = "Lat of family origin", "dist2ocean_r" = "Dist to ocean","dist2rivers_r" = "Dist to rivers", "longitude_r" = "Longitude", "altitude_r" = "Altitude", "dist2water_r" = "Dist to water", "clim_PC1" = "Climate PC1", "clim_PC2" = "Climate PC2", "dist2lakes_r" = "Dist to lakes", "latitude_r" = "Latitude", "log_popSize" = "Population size", "UVB_r" = "UVR","daltonism" = "daltonism")) +
       guides(fill=FALSE)
```

The `cForest` algorithm correctly classifies `r round(mean_acc_cf*100,2)` % of the populations (recall = `r round(mean_rec_cf*100,2)` and precision `r round(mean_prec_cf*100,2)`).

```{r plot cforest blue no nbcolorcat acc, fig.width=5, fig.cap=capFig("Variable importance according to cForest algorithm using the accuracy index.")}
# Plot the result
ggplot(mapping = aes(x = reorder(namevar, mean_all), y = mean_all), 
       data = vimp_permut2.df) +
       theme_bw() +
       geom_col(fill="skyblue", color="black") +
       labs(x ="", y = paste0("Mean of ",n_iter_gini," iterations")) +
       theme(axis.text.x = element_text(angle = 60, hjust = 1, size =13, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =13, face = "bold"),
             axis.title=element_text(size=16),
             legend.position="none") +
    scale_x_discrete(labels=c("subsistence" = "Subsistence mode", "longitude_family_r" = "Long of family origin", "latitude_family_r" = "Lat of family origin", "dist2ocean_r" = "Dist to ocean","dist2rivers_r" = "Dist to rivers", "longitude_r" = "Longitude", "altitude_r" = "Altitude", "dist2water_r" = "Dist to water", "clim_PC1" = "Climate PC1", "clim_PC2" = "Climate PC2", "dist2lakes_r" = "Dist to lakes", "latitude_r" = "Latitude", "log_popSize" = "Population size", "UVB_r" = "UVR","daltonism" = "daltonism")) +
       guides(fill=FALSE)
```

When the number of color categories is excluded, we find that *population size* and *UVR* and *daltonism* are the most important predictors, followed by *latitude* and *distance to lakes*. 


#### Conclusions

While the results from random forests and regression are congruent, especially concerning the importance of UVR and number of color categories/population size, we also found some differences:

-  daltonism and clim_PC1 are variables that have an impact on exists_blue in the random forest models, but not in the regression models,
-  latitude and longitude of language family origins have an impact on exists_blue in regression models, but not in the random forest models.


### Decision trees

We used simple decision trees as implemented by `ctree` [@ctree_2006].

```{r decision tree blue, fig.cap=capFig("Simple decision tree.")}
# pruned tree
# control <- rpart.control(minsplit = 6, minbucket = 6, maxdepth = 30)
# fit <- rpart(exists_blue ~ UVB + daltonism  + subsistence + longitude + latitude + log_popSize + clim_PC1 + clim_PC2 + altitude + dist2water + dist2ocean + dist2lakes + dist2rivers + n_colors, data = d_colors, method = 'class', model=TRUE, control=control)
#rpart.plot(fit, extra = 106)
tree <- ctree (exists_blue ~  UVB_r + daltonism  + subsistence + longitude_r + latitude_r + log_popSize + clim_PC1 + clim_PC2 + altitude_r + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r + n_colors + latitude_family_r + longitude_family_r, 
       data = d_colors, 
       control = ctree_control(testtype = "MonteCarlo", minsplit = 6, minbucket = 6, maxdepth=30))

# PLOT TREE
plot(tree,  gp = gpar(fontsize = 11))
```

To understand what this decision tree does, we plotted which populations have less/more than 0.726 for z-scored UVR, and less/more than 4.7 for dist2lakes_r.

```{r plot dec tree blue result 1}
# write number climate PC2 for visualization
nb_PC2 = 0.65
nb_UVB = 0.726
nb_distlake = 4.7

d_colors_map <- d_colors
# create range
d_colors_map$PC2_range <- cut(d_colors_map$clim_PC2, c(-Inf, nb_PC2, Inf), 
                                labels=c(paste("less than", nb_PC2), paste("more than", nb_PC2)))

d_colors_map$UVB_range <- cut(d_colors_map$UVB_r, c(-Inf, nb_UVB, Inf), 
                                labels=c(paste("less than", nb_UVB), paste("more than", nb_UVB)))

d_colors_map$dlakes_range <- cut(d_colors_map$dist2lakes_r, c(-Inf, nb_distlake, Inf), 
                                labels=c(paste("less than", nb_distlake), paste("more than", nb_distlake)))

```

```{r plot dec tree blue result 2, fig.cap=capFig("UVR (z-scored) as split by the decision tree.")}
# MAP UVB
ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(aes(x = d_colors_map$longitude, y = d_colors_map$latitude, color=d_colors_map$UVB_range, shape=d_colors_map$exists_blue)) +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(color="UVR (z-scored)", shape= "exists_blue")
```

```{r plot dec tree blue result 3, fig.cap=capFig("Log of distance to lakes as split by the decision tree.")}
# MAP dist2lakes
ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(aes(x = d_colors_map$longitude, y = d_colors_map$latitude, color=d_colors_map$dlakes_range, shape=d_colors_map$exists_blue)) +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(color="Log dist to lakes", shape= "exists_blue")
```

Thus, it seems that the distance to lakes is relevant only when there are exactly 6 color categories, in which case populations living more than about 25 km (= 2^4.7^) away from the closest lake do not have a specific word for exists_blue, whereas population living closer have a 75% chance to possess such a word. 
While we could not find a satisfing explanation for this "lake effect", we suggest some hypotheses (among many others) below:

 - populations living close to lakes see more often the blue color of the lake's waters. However, oceans and seas are also blue (and there is no "sea/ocean effect" in our data), and not all lakes necessarily have clean, blue water
 - distance to lakes is in fact a proxy for continents: indeed, the map of distance to lakes suggests that population in parts of South America, equatorial Africa and Papunesia tends to be far away from lakes. However, we did control macroarea as a random effect 
 - distance to lakes is a proxy for economic development: there are suggestions that closeness to large bodies of water may favour historical economic development (but not only lakes), but this hypothesis cannot be tested with the currently available data
 - distance to lakes as a proxy for latitude, but splitting latitude into bins did not support this idea
 - of course, it is entirely possible that the relationship between distance to lake and having a word for exists_blue is entirely accidental in out dataset.


### Other machine learning techniques


To find out if there are some others relations between the variables in our dataset and exists_blue that could have been missed with by regression, random forest and decision tree models, we compared their performance with that of Support Vector Machines (SVM; as implemented by the `e1071` package). 

**SVM** is a supervised machine learning technique, which can be used for both classification and regression. It uses the kernel trick to find a hyperplane in an N-dimensional space (N — the number of features) that distinctly classifies the data points.

We randomly splitted the dataset into a training and testing set, and we computed the precision, recall and classification rate from the model's confusion matrix. The training set contains 80% of the dataset, and we performed a stratified sampling on the variable *exists_blue*. Because our dataset is quite small, the choice of the training and testing set impacted the results. To avoid this problem, we ran those algorithm a 30 times to compute a mean classification rate, recall and precision.



```{r machine learning blue all covariates, include=FALSE}

#rsq <- function (x, y) cor(x, y) ^ 2

# ------- RANDOM FOREST --------- #

if( !file.exists("./cached_results/blue_mach_learning.RData") )
{
  # initialize variable
  acc_all <- data.frame(NULL) ; prec_all<- data.frame(NULL) ; rec_all <- data.frame(NULL) ; 
  
  # fit algorithm for N iteration
  for (k in 1:30){
    
    # split data in training and testing set
    data_split <- initial_split(d_colors, prop=0.80, strata = "exists_blue")
    data_train <- training(data_split)
    data_test <- testing(data_split)
  
    
    # -------  Random Forest  --------- #
    
    # fit randomforest on training dat
    rf.db = randomForest(exists_blue ~ UVB + daltonism  + subsistence + longitude_r + latitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2ocean_r + dist2lakes_r + dist2rivers_r + n_colors + latitude_family_r + longitude_family_r, data = data_train)
    
    # create confusion matrix
    pred <- predict(rf.db, data_test, type='class')
    ConfusionMatrix <- table(data_test$exists_blue, pred)

    # compute classification rate, precision, recall, save those data in table
    acc_all[k, "randomForest"] <- sum(diag(ConfusionMatrix)) / sum(ConfusionMatrix)
    prec_all[k, "randomForest"] <-  ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[1,2])
    rec_all[k, "randomForest"] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[2,1])

    
    # -------  BRMS  --------- #
  
    # fit model
    fit_train <- brm(exists_blue ~ UVB + daltonism  + subsistence + longitude_r + latitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2ocean_r + dist2lakes_r + dist2rivers_r + n_colors + latitude_family_r + longitude_family_r + (1 | macroarea) + (1 | glottocode_family), 
                        data=data_train, # very similar notation to lmer
                        family=bernoulli(link = "logit"),
                        cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
                        control=brms_control); 
  
    # create confusion matrix
    Pred <- predict(fit_train, type = "response", newdata= data_test, allow_new_levels=TRUE)
    Pred <- if_else(Pred[,1] > 0.5, 1, 0)
    ConfusionMatrix <- table(Pred, pull(data_test, exists_blue)) #`pull` results in a vector

    # compute classification rate, precision, recall and save those data in table
    acc_all[k, 'brms'] <- sum(diag(ConfusionMatrix)) / sum(ConfusionMatrix)
    prec_all[k, 'brms'] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[1,2])
    rec_all[k, 'brms'] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[2,1])
    
    
    # -------  SVM  --------- #
    
    # find best parameters for SVM
    tuned_parameters <- tune.svm(exists_blue ~ UVB + daltonism  + subsistence + longitude_r + latitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2ocean_r + dist2lakes_r + dist2rivers_r + n_colors + latitude_family_r + longitude_family_r, 
                                 data = data_train, 
                                 gamma = 10^(-5:-1), 
                                 cost =10^(-3:1))
  
    # fit SVM with tuned parameters on training data
    svm_model_after_tune <- svm(exists_blue ~ UVB + daltonism  + subsistence + longitude_r + latitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2ocean_r + dist2lakes_r + dist2rivers_r + n_colors + latitude_family_r + longitude_family_r, 
                                data_train, 
                                kernel="radial", 
                                cost = tuned_parameters$best.parameters[1,2], 
                                gamma = tuned_parameters$best.parameters[1,1])
  
    # create confusion matrix
    pred <- predict(svm_model_after_tune, data_test, type='class')
    ConfusionMatrix <- table(data_test$exists_blue, pred)
    
    # compute classification rate, precision, recall and save those data in table
    acc_all[k, "svm"] <- sum(diag(ConfusionMatrix)) / sum(ConfusionMatrix)
    prec_all[k, "svm"] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[1,2])
    rec_all[k, "svm"] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[2,1])
  }
  
  # mean those values
  Accuracy <- colMeans(acc_all)
  Precision <- colMeans(prec_all)
  Recall <- colMeans(rec_all)
 
  # ------- PUT ALL VALUES TOGETHER --------- #
  
  # create dataframe with vectors
  df_final <- data.frame(Accuracy, Precision, Recall)
  df_final$Algorithm <- rownames(df_final)
  
  # change dataframe format
  df_final_long <- gather(df_final, test, percentage, Accuracy:Recall, factor_key=TRUE)

  # cache these results:
  save(df_final_long,
       file="./cached_results/blue_mach_learning.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_mach_learning.RData")
}

```


**Model with all covariates:**

```{r plot mach learn blue all cov, fig.cap=capFig("Mean performance of classification with all covariates for brms, randomForest and SVM algorithms.")}

ggplot(data=df_final_long, aes(x=test, y=percentage*100, fill=Algorithm)) +
  theme_bw() +
  geom_bar(stat="identity", position=position_dodge()) +
  labs( x ="", y = "Percentage") +
  scale_fill_brewer(palette="Paired") +
  theme(axis.text.x = element_text(size =16, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =16, face = "bold"),
             axis.title=element_text(size=16),
             legend.title=element_text(size =16),
             legend.text=element_text(size =16)) +
  ylim(0,100)
```






```{r machine learning blue simple model, include=FALSE}

#rsq <- function (x, y) cor(x, y) ^ 2

# ------- RANDOM FOREST --------- #

if( !file.exists("./cached_results/blue_mach_learning_simple.RData") )
{
  
  # initialize variable
  acc_all <- data.frame(NULL) ; prec_all<- data.frame(NULL) ; rec_all <- data.frame(NULL) ; 
  
  # fit algorithm for N iteration
  for (k in 1:30){
    
    # split data in training and testing set
    data_split <- initial_split(d_colors, prop=0.80, strata = "exists_blue")
    data_train <- training(data_split)
    data_test <- testing(data_split)
  
    
    # -------  Random Forest  --------- #
    
    # fit randomforest on training dat
    rf.db = randomForest(exists_blue ~  n_colors + dist2lakes_r + I(n_colors^2), data = data_train)
    
    # create confusion matrix
    pred <- predict(rf.db, data_test, type='class')
    ConfusionMatrix <- table(data_test$exists_blue, pred)

    # compute classification rate, precision, recall, and save those data in table
    acc_all[k, "randomForest"] <- sum(diag(ConfusionMatrix)) / sum(ConfusionMatrix) # accuracy
    prec_all[k, "randomForest"] <-  ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[1,2]) # precision
    rec_all[k, "randomForest"] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[2,1]) # recall
    
    
    # -------  BRMS  --------- #
  
    # fit model
    fit_train <- brm(exists_blue ~ n_colors + dist2lakes_r + I(n_colors^2) + (1 | macroarea) + (1 | glottocode_family), 
                        data=data_train, # very similar notation to lmer
                        family=bernoulli(link = "logit"),
                        cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
                        control=brms_control); 
  
    # create confusion matrix
    Pred <- predict(fit_train, type = "response", newdata= data_test, allow_new_levels=TRUE)
    Pred <- if_else(Pred[,1] > 0.5, 1, 0)
    ConfusionMatrix <- table(Pred, pull(data_test, exists_blue)) #`pull` results in a vector

    # compute classification rate, precision, recall and save those data in table
    acc_all[k, 'brms'] <- sum(diag(ConfusionMatrix)) / sum(ConfusionMatrix)
    prec_all[k, 'brms'] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[1,2])
    rec_all[k, 'brms'] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[2,1])
    
    
    # -------  SVM  --------- #
    
    # find best parameters for SVM
    tuned_parameters <- tune.svm(exists_blue ~  n_colors + dist2lakes_r + I(n_colors^2), data = data_train, gamma = 10^(-5:-1), cost =10^(-3:1))
  
    # fit SVM with tuned parameters on training data
    svm_model_after_tune <- svm(exists_blue ~  n_colors + dist2lakes_r + I(n_colors^2), data_train, kernel="radial", cost=tuned_parameters$best.parameters[1,2], gamma=tuned_parameters$best.parameters[1,1])
  
    # create confusion matrix
    pred <- predict(svm_model_after_tune, data_test, type='class')
    ConfusionMatrix <- table(data_test$exists_blue, pred)
 
    # compute classification rate, precision, recall and save those data in table
    acc_all[k, "svm"] <- sum(diag(ConfusionMatrix)) / sum(ConfusionMatrix)
    prec_all[k, "svm"] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[1,2])
    rec_all[k, "svm"] <- ConfusionMatrix[2,2]/(ConfusionMatrix[2,2]+ConfusionMatrix[2,1])
  }
  
  # mean those values
  Accuracy <- colMeans(acc_all)
  Precision <- colMeans(prec_all)
  Recall <- colMeans(rec_all)
 
  # ------- PUT ALL VALUES TOGETHER --------- #
  
  # create dataframe with vectors
  df_final <- data.frame(Accuracy, Precision, Recall)
  df_final$Algorithm <- rownames(df_final)
  
  # change dataframe format
  df_final_long <- gather(df_final, test, percentage, Accuracy:Recall, factor_key=TRUE)

  # cache these results:
  save(df_final_long,
       file="./cached_results/blue_mach_learning_simple.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/blue_mach_learning_simple.RData")
}

```


**Model with only Number of color categories (linear and quadratic) and distance to lakes as covariates:**

```{r plot mach learn blue set of cov, fig.cap=capFig("Mean performance of classification for brms, randomForest and SVM algorithms for a model including only number of color categories (linear and quadratic) and distance to lakes.")}

ggplot(data=df_final_long, aes(x=test, y=percentage*100, fill=Algorithm)) +
  theme_bw() +
  geom_bar(stat="identity", position=position_dodge()) +
  labs( x ="", y = "Percentage") +
  scale_fill_brewer(palette="Paired") +
  theme(axis.text.x = element_text(size =16, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =16, face = "bold"),
             axis.title=element_text(size=16),
             legend.title=element_text(size =16),
             legend.text=element_text(size =16)) +
  ylim(0,100)
```


The performance rate is very similar for all algorithms. The classification accuracy on the testing set is high for *bayesian mixed-effect model*, *random forest* and *SVM*. 
Thus, a simple mixed-effect bayesian model is enough to catch the explanatory power of our variables for predicting the presence of a specific term for blue. 


<br/>

### Conclusions

See [Plotting the predictors] for data visualization, but the main conclusions are:

- as expected, given @berlin_basic_1991's work, the *number of color categories* is the most relevant variable for predicting the presence or absence of a specific word for blue in a population, it effect being positive; 
- surprisingly, *distance to lakes* seems to play an important (negative) role, even though the reasons remains unclear;
- *UVR* incidence has an important negative effect on the presence of a word for blue; in fact, its effect remains significant even when after controlling for cultural and ecological variables. However, depending on the specific technique and model comparison methods used, the strength of this effect varies somewhat;
- *latitude* is highly multicollinear with UVR incidence, and its (positive) impact on the presence of a word for blue is fully mediated by UVR incidence;
- finally, depending on the methods and the variables added as covariates, *population size*, *daltonism*, and the geographic coordinates of putative language family origins may also be relevant predictors for the presence of a word for blue.

<br/>

## Hypothesis 2: does UVR incidence influence the population frequency of abnormal color perception?

<br/>

### Mediation analysis

Then, we investigated the relation between UVR and daltonism. As a preliminary analysis, we conducted a *mediation analysis* [using the `mediation` package in `R`; @tingley_mediation_2014] to test whether UVR is a mediator between latitude and daltonism. 

Second, we carried out a *mediation analysis* [using the `brm` function from `brms` package in `R`: @burkner_advanced_2018 and the `mediation` function from `spatstat`; @mediation_spatstat] to test whether UVR is a mediator between latitude and daltonism, and between latitude of language family and daltonism. 


```{r mediation analysis dalto}
if( !file.exists("./cached_results/mediation_dalto.RData") )
{
  ## WITH ALL DATA, with normal regression: lat --> UVB_r --> exists_blue 
  model_mediator <- bf(UVB_r ~ latitude_r + (1 | macroarea) + (1 | glottocode_family)) + gaussian()
  model_outcome <- bf(daltonism_r ~ UVB_r + latitude_r + (1 | macroarea) + (1 | glottocode_family)) + Beta()
  dalto_lat_UV <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
            cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
            control=brms_control)
  
  ## WITH ALL DATA, with normal regression: lat² --> UVB_r --> exists_blue
  # model_mediator <- bf(UVB_r ~ I(latitude_r^2) + (1 | macroarea) + (1 | glottocode_family))  + gaussian()
  # model_outcome <- bf(daltonism_r ~ UVB_r + I(latitude_r^2) + (1 | macroarea) + (1 | glottocode_family)) + Beta()
  # dalto_lat2_UV <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
  #           cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
  #           control=brms_control)
  # mediation(dalto_lat2_UV)
  
  ## WITH ALL DATA, with normal regression: lat+lat²  --> UVB_r --> exists_blue 
  model_mediator <- bf(UVB_r ~ latitude_r + I(latitude_r^2) + (1 | macroarea) + (1 | glottocode_family))  + gaussian()
  model_outcome <- bf(daltonism_r ~ UVB_r + latitude_r + I(latitude_r^2) + (1 | macroarea) + (1 | glottocode_family)) + Beta()
  dalto_latlat2_UV <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
            cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
            control=brms_control)
  
  ### WITH latitude family
  
  ## WITH ALL DATA, with normal regression: latfam --> UVB_r --> exists_blue 
  model_mediator <- bf(UVB_r ~ latitude_family_r + (1 | macroarea)) + gaussian()
  model_outcome <- bf(daltonism_r ~ UVB_r + latitude_family_r + (1 | macroarea)) + Beta()
  dalto_latfam_UV <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
            cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
            control=brms_control)
  

  # ## WITH ALL DATA, with normal regression: latfam² --> UVB_r --> exists_blue
  # model_mediator <- bf(UVB_r ~ I(latitude_family_r^2) + (1 | macroarea) + (1 | glottocode_family))  + gaussian()
  # model_outcome <- bf(daltonism_r ~ UVB_r + I(latitude_family_r^2) + (1 | macroarea) + (1 | glottocode_family)) + Beta()
  # dalto_latfam2_UV <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
  #           cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
  #           control=brms_control)
  
  ## WITH ALL DATA, with normal regression: latfam+latfam²  --> UVB_r --> exists_blue 
  model_mediator <- bf(UVB_r ~ latitude_family_r + I(latitude_family_r^2) + (1 | macroarea))  + gaussian()
  model_outcome <- bf(daltonism_r ~ UVB_r + latitude_family_r + I(latitude_family_r^2) + (1 | macroarea)) + Beta()
  dalto_latfamlatfam2_UV <- brm(model_mediator + model_outcome + set_rescor(FALSE), data = d_colors,
            cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
            control=brms_control)

    # cache these results:
  save(dalto_lat_UV, dalto_latlat2_UV, dalto_latfam_UV, dalto_latfamlatfam2_UV,
       file="./cached_results/mediation_dalto.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/mediation_dalto.RData")
}

```

#### latitude --> UVR --> daltonism ?

##### Bayes model result and diagnostic

```{r show mediation lat dalto 1}
# Plot Bayes model
summary(dalto_lat_UV)
plot(dalto_lat_UV)
```

##### Mediation result

With prob = 0.90:
```{r show mediation lat dalto 2}
# Plot mediation
mediation(dalto_lat_UV)
plot_med(dalto_lat_UV)
```

With prob = 0.95:

```{r show mediation lat dalto 3}
mediation(dalto_latfam_UV, prob=0.95)

```


##### Comparaison of predicted and observed values for the mediator and outcome

The faint lines (yrep) are posterior predictive draws, and y is the observed distribution.

```{r show mediation lat dalto 4}
plot_answer_med(dalto_lat_UV, "daltonismbeta", "UVBz")

```


```{r show mediation lat2}
##### With latitude

##### With latitude^2

# mediation(dalto_lat2_UV)
# pp_check(dalto_lat2_UV, resp = 'daltonismbeta') + ggtitle('daltonism Outcome')
# pp_check(dalto_lat2_UV, resp = 'UVBz') + ggtitle('Mediator')

##### With latitude + latitude^2
# summary(dalto_latlat2_UV)
# mediation(dalto_latlat2_UV)
# bayes_R2(dalto_latlat2_UV)
# 
# plot_answer_med(dalto_latlat2_UV, "daltonismbeta", "UVBz")
# plot_med(dalto_latlat2_UV)
# plot(dalto_latlat2_UV)
```




#### Putative latitude of language family --> UVR --> daltonism ?

##### Bayes model result and diagnostic
```{r show mediation latfam dalto 1}
# Plot Bayes model
summary(dalto_latfam_UV)
plot(dalto_latfam_UV)
```

##### Mediation result

With prob = 0.9:
```{r show mediation latfam dalto 2}
# Plot mediation
mediation(dalto_latfam_UV)
plot_med(dalto_latfam_UV)
```

With prob = 0.95:
```{r show mediation latfam dalto 3}
mediation(dalto_latfam_UV, prob=0.95)

```


##### Comparaison of predicted and observed values for the mediator and outcome

The faint lines (yrep) are posterior predictive draws, and y is the observed distribution.

```{r show mediation latfam dalto 4}
plot_answer_med(dalto_latfam_UV, "daltonismbeta", "UVBz")

```



```{r show mediation latfam2}

##### With latitude


##### With latitude^2
# mediation(dalto_latfam2_UV)
# pp_check(dalto_latfam2_UV, resp = 'daltonismbeta') + ggtitle('daltonism Outcome')
# pp_check(dalto_latfam2_UV, resp = 'UVBz') + ggtitle('Mediator')

##### With latitude + latitude^2
# summary(dalto_latfamlatfam2_UV)
# mediation(dalto_latfamlatfam2_UV)
# bayes_R2(dalto_latfamlatfam2_UV)
# 
# plot_answer_med(dalto_latfamlatfam2_UV, "daltonismbeta", "UVBz")
# plot_med(dalto_latfamlatfam2_UV)
# plot(dalto_latfamlatfam2_UV)
```



#### Conclusions

The mediation analysis indicates that the effect of latitude (and latitude^2^) on the population frequency of abnormal color percption is fully mediated by UVR incidence (when macroarea and family are random effects).


### Regression models

#### UVR incidence only

We investigate here the effects of UVR incidence on daltonism. 

We use the same methods as used the previous part, namely Bayesian mixed-effects models (implemented in package `brms`; @brms_2018) with language family and macroarea as random effects. As *daltonism* is a percentage, we used *Beta* regression.


```{r brm daltonism uvb}
if( !file.exists("./cached_results/daltonism_uvb.RData") )
{
  # fit model, with only UVB as a fixed effect
  model_uvb_rs <- brm(daltonism_r ~ UVB_r + (1 + UVB_r | macroarea) + (1 + UVB_r | glottocode_family), 
                      data=d_colors,
                      family="beta",
                      cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control); 
  summary(model_uvb_rs);
  loo_uvb_rs <- loo(model_uvb_rs); waic_uvb_rs <- waic(model_uvb_rs); kfold_uvb_rs <- kfold(model_uvb_rs, chains=1);
  
  # Check if the random slopes are needed or not:
  model_uvb_nors <- update(model_uvb_rs, . ~ . - (1 + UVB_r | macroarea) - (1 + UVB_r | glottocode_family) + (1 | macroarea) + (1 | glottocode_family), cores=brms_ncores);
  summary(model_uvb_nors);

  loo_uvb_nors <- loo(model_uvb_nors); waic_uvb_nors <- waic(model_uvb_nors); kfold_uvb_nors <- kfold(model_uvb_nors, chains=1);
  (bf_rs_nors <- bayes_factor(model_uvb_rs, model_uvb_nors)); BF_interpretation(bf_rs_nors$bf, "with random slopes", "random intercepts only") 
  loo_compare(loo_uvb_rs, loo_uvb_nors);
  loo_compare(waic_uvb_rs, waic_uvb_nors);
  loo_compare(kfold_uvb_rs, kfold_uvb_nors);
  model_weights(model_uvb_rs, model_uvb_nors, weights = "waic");
  
  model_uvb <- model_uvb_nors; # random slopes are not needed
  loo_uvb <- loo_uvb_nors; waic_uvb <- waic_uvb_nors; kfold_uvb <- kfold_uvb_nors;
  
  # compute Rsquared
  Rsquared <- bayes_R2(model_uvb)
  
  # compute K-fold
  kf <- kfold(model_uvb, save_fits = TRUE, chains = 1)

  # define a loss function
  rmse <- function(y, yrep) {
    yrep_mean <- colMeans(yrep)
    sqrt(mean((yrep_mean - y)^2))
  }

  # predict responses and evaluate the loss
  kfp <- kfold_predict(kf)
  rmse_modeluvb <- rmse(y = kfp$y, yrep = kfp$yrep)


  # Check if UVB_r matters:
  model_uvb_0 <- update(model_uvb, . ~ . - UVB_r, cores=brms_ncores);
  loo_uvb_0 <- loo(model_uvb_0); waic_uvb_0 <- waic(model_uvb_0);
  (bf_uvb_0 <- bayes_factor(model_uvb, model_uvb_0)); BF_interpretation(bf_uvb_0$bf, "with UVB incidence", "without UVB incidence") 
  # compute K-fold and compare
  kf_0 <- kfold(model_uvb_0, save_fits = TRUE, chains = 1)
  (kfold_compare <- loo_compare(kf, kf_0))
  loo_compare(loo_uvb, loo_uvb_0);
  loo_compare(waic_uvb, waic_uvb_0);
  model_weights(model_uvb, model_uvb_0, weights = "waic");
  
  #plot(conditional_effects(model_uvb), ask = FALSE)  
  
  # cache these results:
  save(model_uvb_rs, model_uvb_nors,model_uvb, Rsquared, kf, rmse_modeluvb, model_uvb_0,  bf_uvb_0, kf_0, kfold_compare,
       file="./cached_results/daltonism_uvb.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/daltonism_uvb.RData")
}

summary(model_uvb)
(m_hdi <- hdi(model_uvb, ci=brms_ci)); rope(model_uvb, range=brms_rope, ci=brms_ci); p_rope(model_uvb, range=brms_rope);

```

As before, including random slopes did not make any significant contribution  (Bayes factor `r round(bf_rs_nors$bf,2)`, `r BF_interpretation(bf_rs_nors$bf, "with random slopes", "random intercepts only")`). Consequently, we retained the random intercepts-only model.

The effect of UVR on daltonism is *strong* and *negative*:

 -  Bayes factor `r sprintf("%.2f",bf_uvb_0$bf)`, `r BF_interpretation(bf_uvb_0$bf, "model with UVB incidence", "null model")`; `r brms_ci*100`% HDI for *&beta;*~UVR~ = [`r round(m_hdi$CI_low[m_hdi$Parameter == "b_UVB_r"],2)`, `r round(m_hdi$CI_high[m_hdi$Parameter == "b_UVB_r"],2)`]
 -  K-fold model comparaison: evidence for `r rownames(kfold_compare)[1]`: expected log predictive density (ELPD) = `r round(kfold_compare[2,1],2)` , SE = `r round(kfold_compare[2,2],2)`
 
The model with UVR explains R^2^ = `r round(Rsquared[1]*100,2)`% of the variance. The Root Mean Square Error of the prediction (RMSE) is `r round(rmse_modeluvb*100,2)`.


#### Adding covariates

We added covariates and selected the best model with Bayes factors on one hand, and with WAIC, LOO and KFOLD on the other hand. 

##### Using Bayes factors

The best model is:

```{r brm daltonism uvb best model}

# add longitude: slightly better (1.90561)
# add subsistence: slightly better (2.14809)
# add longitude and subsistence: slightly better (2.11731) 

if( !file.exists("./cached_results/daltonism_uvb_bestmodel.RData") )
{

  # With beta distribution: best model
  d_bestmodel <- brm(daltonism_r ~  exists_blue + latitude_r +  latitude_family_r + I(latitude_r^2) + I(latitude_family_r^2) + (1 | macroarea) + (1 | glottocode_family),
                     data=d_colors, # very similar notation to lmer
                     family="beta",
                     cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);

  # compute Rsquared
  Rsquared_best_model <- bayes_R2(d_bestmodel)

  # predict responses and evaluate the loss
  kf <- kfold(d_bestmodel, save_fits = TRUE, chains = 1)
  kfp <- kfold_predict(kf)
  rmse_bestmodel <- rmse(y = kfp$y, yrep = kfp$yrep)


  # cache these results:
  save(d_bestmodel, Rsquared_best_model, rmse_bestmodel, kf,
       file="./cached_results/daltonism_uvb_bestmodel.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/daltonism_uvb_bestmodel.RData")
}

summary(d_bestmodel)
hdi(d_bestmodel, ci=brms_ci); rope(d_bestmodel, range=brms_rope, ci=brms_ci); p_rope(d_bestmodel, range=brms_rope);


plot(
  d_bestmodel,
  pars = NA,
  combo = c("dens", "trace")
)

```

The best model using Bayes factor explains R^2^ = `r round(Rsquared_best_model[1]*100,2)`% of the variance. The Root Mean Square Error of the prediction (RMSE) is `r round(rmse_bestmodel*100,2)`.

Is *subsistence* predictive?

```{r brm daltonism uvb best model 2}

# add longitude: slightly better (1.90561)
# add subsistence: slightly better (2.14809)
# add longitude and subsistence: slightly better (2.11731) 

if( !file.exists("./cached_results/daltonism_uvb_bestmodel_other.RData") )
{

  # With beta distribution: other best model
d_bestmodel_1 <- brm(daltonism_r ~  subsistence + exists_blue + latitude_r +  latitude_family_r + I(latitude_r^2) + I(latitude_family_r^2) + (1 | macroarea) + (1 | glottocode_family),
                     data=d_colors, # very similar notation to lmer
                     family="beta",
                     cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);

  # predict responses and evaluate the loss
  kf1 <- kfold(d_bestmodel_1, save_fits = TRUE, chains = 1)

  # Bayes factor
  d_bf_bestmodel1 <- bayes_factor(d_bestmodel_1, d_bestmodel);
  
  # cache these results:
  save(d_bestmodel_1, kf1, d_bf_bestmodel1,
       file="./cached_results/daltonism_uvb_bestmodel_other.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/daltonism_uvb_bestmodel_other.RData")
}

  
# BF
cat(paste0("Bayes factor: best model with subsistence vs best model without subsistence: ",round(d_bf_bestmodel1$bf,3),"; ",BF_interpretation(d_bf_bestmodel1$bf, "best light model with subsistence", "best light model without subsistence"),".\n"));

# LOO
loo_compare(kf, kf1) 


```

There is an anecdotal evidence for the model with *subsistence*. The effect is not significant and negative: hunter-gatherers are more likely not to have a word for blue.

According to our mediation analysis, latitude (and latitude^2^) have an effect on daltonism only through the mediator UVR.

This complex model includes many variables. In order to check which variables are the most important, we computed the best model according to LOO, WAIC and KFOLD methods. 

##### Using WAIC, LOO and KFOLD

With this the best model is:

```{r brm daltonism bestmodel kfold}
if( !file.exists("./cached_results/daltonism_uvb_bestmodel_kfold.RData") )
{
  d_bestlightmodel <- brm(daltonism_r ~  exists_blue + UVB_r + I(latitude_family_r^2) + (1 | macroarea) + (1 | glottocode_family),
                         data=d_colors,
                         family="beta",  
                         cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);
  
  # compute Rsquared
  Rsquared_light_model <- bayes_R2(d_bestlightmodel)

  # predict responses and evaluate the loss
  kf4 <- kfold(d_bestlightmodel, save_fits = TRUE, chains = 1)
  kfp <- kfold_predict(kf4)
  rmse_lightmodel <- rmse(y = kfp$y, yrep = kfp$yrep)

  # cache these results:
  save(d_bestlightmodel, Rsquared_light_model, rmse_lightmodel, kf4,
       file="./cached_results/daltonism_uvb_bestmodel_kfold.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/daltonism_uvb_bestmodel_kfold.RData")
}

summary(d_bestlightmodel)
hdi(d_bestlightmodel, ci=brms_ci); rope(d_bestlightmodel, range=brms_rope, ci=brms_ci); p_rope(d_bestlightmodel, range=brms_rope);

plot(
  d_bestlightmodel,
  pars = NA,
  combo = c("dens", "trace")
)

```

The best model using K-fold explains R^2^ = `r round(Rsquared_light_model[1]*100,2)`% of the variance. The Root Mean Square Error of the prediction (RMSE) is `r round(rmse_lightmodel*100,2)`.

Adding *subsistence*:

```{r brm daltonism bestmodel kfold 2}
if( !file.exists("./cached_results/daltonism_uvb_bestmodel_kfold_other.RData") )
{
  d_bestlightmodel_1 <- brm(daltonism_r ~  subsistence + exists_blue + UVB_r + I(latitude_family_r^2) + (1 | macroarea) + (1 | glottocode_family),
                         data=d_colors,
                         family="beta",  
                         cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);
  
   # predict responses and evaluate the loss
  kf3 <- kfold(d_bestlightmodel_1, save_fits = TRUE, chains = 1)

  # Bayes factor
  d_bf_bestlightmodel1 <- bayes_factor(d_bestlightmodel_1, d_bestlightmodel);
  
  # cache these results:
  save(d_bestlightmodel_1, kf3, d_bf_bestlightmodel1,
       file="./cached_results/daltonism_uvb_bestmodel_kfold_other.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/daltonism_uvb_bestmodel_kfold_other.RData")
}

  
# BF
cat(paste0("Bayes factor: best model with subsistence vs best model without subsistence: ",round(d_bf_bestlightmodel1$bf,3),"; ",BF_interpretation(d_bf_bestlightmodel1$bf, "best light model with subsistence", "best light model without subsistence"),".\n"));

# LOO
loo_compare(kf3, kf4) 


```

Subsistence mode should not be added to this model.

UVR must be kept:

```{r brm daltonism bestmodel kfold nouvb}
if( !file.exists("./cached_results/daltonism_bestmodel_kfold_nouvb.RData") )
{
  # Best model (KFOLD) without UVB
   d_bestlightmodel_nouvb <- brm(daltonism_r ~  exists_blue + I(latitude_family_r^2) + (1 | macroarea) + (1 | glottocode_family),
                         data=d_colors,
                         family="beta",  
                         cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE, control=brms_control);
  
  # Bayes factor
  d_bf_bestlightmodel_nouvb <- bayes_factor(d_bestlightmodel, d_bestlightmodel_nouvb);
  
  # K-fold
  d_kfold_light_nouvb <- kfold(d_bestlightmodel_nouvb, chains = 1)
  
  # cache these results:
  save(d_bestlightmodel_nouvb,
       d_bf_bestlightmodel_nouvb, d_kfold_light_nouvb,
       file="./cached_results/daltonism_bestmodel_kfold_nouvb.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/daltonism_bestmodel_kfold_nouvb.RData")
}

# BF
cat(paste0("Bayes factor: best light model vs best light model without UVB: ",round(d_bf_bestlightmodel_nouvb$bf,3),"; ",BF_interpretation(d_bf_bestlightmodel_nouvb$bf, "best light model", "best light model without UVB"),".\n"));

# LOO
loo_compare(kf4, d_kfold_light_nouvb) 
```


### Plotting the predictors

**UVR**. UVR has a negative significant impact on daltonism; thus, the higher the UVR incidence on a population, the less green-red color blind males there will be in this population.

```{r plot predictor dalto uv, fig.cap=capFig("UVR incidence vs daltonism rate.")}
# plot with UVR
ggplot(d_colors, aes(x=UVB_r, y=daltonism))+ geom_point(aes(colour=exists_blue, size=subsistence)) + 
  theme_bw() +
  geom_smooth(colour="black") +
  theme(axis.text.x = element_text(size =15, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =15, face = "bold"),
             axis.title=element_text(size=15),
             legend.title=element_text(size =15),
             legend.text=element_text(size =15)) +
  labs(x = "UV incidence (z scored)", y="daltonism rate", color="Presence of a word for blue?", size="Hunter-Gatherer?") ;

```

```{r plot predictor dalto uv 2, fig.cap=capFig("Map of UVR incidence on daltonism rate. A low daltonism rate is a rate below the median of all daltonism rates (3.3), whereas a high daltonism rate codes for rates above 3.3.")}
# plot UVR
d_colors$dalto_range <- cut(d_colors$daltonism, c(-Inf, median(d_colors$daltonism),  Inf), 
                      labels=c("low", "high"))

ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(data=d_colors, aes(x = longitude, y = latitude, fill=UVB, shape=dalto_range), color="red") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
    labs(fill = "UVR incidence", shape="daltonism") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  scale_shape_manual(values=c("low"=24, "high"=21))

```

**Latitude**. Latitude is highly correlated with UVR incidence, and its effects on exists_blue fully mediated by it; The closer to the equator a population is, the less likely it is to include red-green color-blind males. 

```{r plot predictor dalto lat, fig.cap=capFig("Latitude on daltonism rate.")}
# plot with latitude
ggplot(d_colors, aes(x=latitude_r, y=daltonism))+ geom_point(aes(colour=exists_blue, size=subsistence)) + 
  theme_bw() +
  geom_smooth(colour="black") +
  labs(x="latitude") +
  theme(axis.text.x = element_text(size =15, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =15, face = "bold"),
             axis.title=element_text(size=15),
             legend.title=element_text(size =15),
             legend.text=element_text(size =15)) 

```


**subsistence**. With Bayes factor, subsistence slightly improve the fit. 

However, one should note that only `r nrow(d_colors[d_colors$subsistence=="yes",])` hunter-gatherers groups are present in our dataset.

Hunter-gatherer males are less color-blind, even in places where the UVR incidence is low. 
We may suggest that selective pressures for color perception is higher for hunter-gatherers than for agriculture-based communities.


```{r plot predictor dalto subsistence, fig.cap=capFig("Subsistence strategy on daltonism rate.")}

ggplot(d_colors, aes(x=subsistence, y=daltonism, color=subsistence)) + 
  theme_bw() +
  geom_point(alpha=0.4, position=position_jitter(h=0.0,w=0.3)) + 
  geom_violin(alpha=0.2) +
  theme(axis.text.x = element_text(size =15, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =15, face = "bold"),
             axis.title=element_text(size=15),
             legend.title=element_text(size =15),
             legend.text=element_text(size =15)) 

```


**exists_blue**. Population having a high red-green color blindness rate tends to have a specific term for blue in their vocabulary. 

According to our hypothesis, exists_blue would be a proxy for green-blue perception, but there would be no causal relationship between exists_blue and daltonism. 

```{r plot predictor dalto blue, fig.cap=capFig("Having a word for blue on daltonism rate.")}
ggplot(d_colors, aes(x=exists_blue, y=daltonism, color=exists_blue)) + 
  theme_bw() +
  geom_point(alpha=0.4, position=position_jitter(h=0.0,w=0.3)) + 
  geom_violin(alpha=0.2) +
  theme(axis.text.x = element_text(size =15, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =15, face = "bold"),
             axis.title=element_text(size=15),
             legend.title=element_text(size =15),
             legend.text=element_text(size =15)) 

```


**Latitude of language family**. 

It is significant included in the set of relevant predictors. 

```{r plot predictor dalto latfam, fig.cap=capFig("Map of latitude of language family origins on daltonism rate.")}
# plot latitude of language family origins
ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(data=d_colors, aes(x = longitude, y = latitude, fill=latitude_family_r, shape=dalto_range), color="red") +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(fill = "Lat of lang fam origin", shape="daltonism") +
  scale_fill_gradient(low = "lightskyblue1", high = "navyblue") + 
  scale_shape_manual(values=c("low"=24, "high"=21))
```


```{r plot predictor dalto latfam2, fig.cap=capFig("Location of putative origins of language family on daltonism rate.")}
d_colors3 <- d_colors
d_colors3$supra_fam_new <- d_colors3$macro_family
d_colors3$supra_fam_new <- as.character(d_colors3$supra_fam_new)
for (el in as.character(unique(d_colors3$macro_family))){
  if (nrow(d_colors3[d_colors3$macro_family==el,]) == 1) {
    d_colors3$supra_fam_new[d_colors3$macro_family==el] <- "z_Isolated"
  }
}

# violin plot: latitude on the left
ggplot() + 
  theme_bw() + 
  geom_point(data=d_colors3, aes(x=latitude_family_r, y=daltonism, color=supra_fam_new)) + 
  geom_smooth(data=d_colors3, aes(x=latitude_family_r, y=daltonism)) +
  labs(x="Latitude of putative lang fam origins") +
  scale_colour_viridis_d() +
  annotate("text", x=0.3746066, y=9.1, label= "Uralic") + 
  annotate("text", x=0.9883615, y=5.8, label= "Niger-Congo") + 
  annotate("text", x=0.988573, y=7.2, label= "Afro-Asiatic") + 
  annotate("text", x=0.9152412, y=4.7, label= "Austronesian") + 
  annotate("text", x=0.8980276, y=7, label= "Sino-Tibetan") + 
  annotate("text", x = 0.8290376, y=11.1, label = "Indo-European")


# subdf1 <- data.frame(ID=d_colors3$supra_fam_new, daltonism = d_colors3$daltonism, latitude_family = d_colors3$latitude_family_r)
# subdf1 <- aggregate(list(dalto=subdf1$daltonism), by=list(ID=subdf1$ID, lat=subdf1$latitude_family), FUN=max)
# subdf1 <- subdf1[subdf1$ID != "z_Isolated",] 
#   geom_text_repel(data= subdf1, aes(x = lat, y=dalto, label=ID),hjust=0, vjust=0, size=4) 

```



### Variable importance with Random Forests

We used random forests to estimate of the relative importance of our variables in predicting the daltonism rate.
Variable importance was computed with *gini-based* and *permutation-based* indexes through the same process as before. 

For more information, please see [Random forests].

```{r variable importance daltonism}
if( !file.exists("./cached_results/daltonism_var_importance.RData") )
{
  # choose variables to analyze
  list_variables <- c("UVB_r" , "exists_blue"  , "subsistence" , "latitude_r", "longitude_r" , "log_popSize" , "clim_PC1", "clim_PC2" , "altitude_r"  , "dist2water_r", "dist2ocean_r" , "dist2lakes_r", "dist2rivers_r"  , "latitude_family_r", "longitude_family_r")
  
  # initialize empty dataframe and list for gini index (randomForest algorithm)
  vimp_gini <- data.frame(matrix(NA, nrow = 1, ncol = length(list_variables)))
  colnames(vimp_gini) <- list_variables
  mse_rf <- NULL; mae_rf <- NULL; rmse_rf <- NULL ; rsquared_rf <- NULL

  # initialize empty dataframe for accuracy index (randomForest algorithm)
  vimp_permut1 <- data.frame(matrix(NA, nrow = 1, ncol = length(list_variables)))
  colnames(vimp_permut1) <- list_variables
  
  # initialize empty dataframe for accuracy index (cForest algorithm)
  vimp_permut2 <- data.frame(matrix(NA, nrow = 1, ncol = length(list_variables)))
  colnames(vimp_permut2) <- list_variables
  mse_cf <- NULL; mae_cf <- NULL; rmse_cf <- NULL ; rsquared_cf <- NULL

  # choose number of iteration
  dn_iter_gini <- 30;
  
  for (it in 1:dn_iter_gini)
    {
    
    # compute random forest algorithm on all variable except multicollinear
    rf.db = randomForest(daltonism ~  UVB_r + exists_blue  + subsistence + latitude_r + longitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2water_r + dist2ocean_r + dist2lakes_r + dist2rivers_r  + latitude_family_r + longitude_family_r, data = d_colors, importance=TRUE)
    
    # check the difference between actual and predicted value
    vec_original_predicted <- abs(d_colors$daltonism - rf.db$predicted )
  
    # compute mse, mae, rmse, Rsquared and save those data in table
    mse_rf[it] <- mean((vec_original_predicted)^2)
    mae_rf[it] <- mean(abs(vec_original_predicted))
    rmse_rf[it] <- sqrt(mean((vec_original_predicted)^2))
    rsquared_rf[it] <- 1-(sum((vec_original_predicted)^2)/sum((d_colors$daltonism - mean(d_colors$daltonism))^2))

    # compute gini-based variable importance (randomForest)
    var_imp_gini <- data.frame(importance(rf.db, type=2))
    
    # compute accuracy-based variable importance( andomForest)
    var_imp_permut <- data.frame(importance(rf.db, type=1))
    
    # use an other algorithm to compute the accuracy-based variable importance
     obj <- cforest(daltonism ~  UVB_r + exists_blue  + subsistence + latitude_r + longitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2water_r  + dist2ocean_r + dist2lakes_r + dist2rivers_r  + latitude_family_r + longitude_family_r, data = d_colors)
    
    # check the difference between actual and predicted value
    oobPredicted <- predict(obj,OOB=T)
    vec_original_predicted <- abs(d_colors$daltonism - oobPredicted )
  
    # compute mse, mae, rmse, Rsquared and save those data in table
    mse_cf[it] <- mean((vec_original_predicted)^2)
    mae_cf[it] <- mean(abs(vec_original_predicted))
    rmse_cf[it] <- sqrt(mse_cf[it])
    rsquared_cf[it] <- 1-(sum((vec_original_predicted)^2)/sum((d_colors$daltonism - mean(d_colors$daltonism))^2))

    # compute accuracy-based variable importance (cForest)
    var_imp_permut2 <- varimp(obj)

    # fill dataframe with variables importance values
    for (variable in list_variables){
      vimp_gini[it,variable] <- var_imp_gini[variable, 1]
      vimp_permut1[it,variable] <- var_imp_permut[variable, 1]
      vimp_permut2[it,variable] <- as.numeric(var_imp_permut2[variable])
      
    }
  }
  
  # mean all iterations
  vimp_gini <- colMeans(vimp_gini, na.rm=TRUE)
  vimp_gini.df <- data.frame(namevar = as.character(names(vimp_gini)), mean_all = as.numeric(vimp_gini))
  
  vimp_permut1 <- colMeans(vimp_permut1, na.rm=TRUE)
  vimp_permut1.df <- data.frame(namevar = as.character(names(vimp_permut1)), mean_all = as.numeric(vimp_permut1))
  
  vimp_permut2 <- colMeans(vimp_permut2, na.rm=TRUE)
  vimp_permut2.df <- data.frame(namevar = as.character(names(vimp_permut2)), mean_all = as.numeric(vimp_permut2))
  
  mean_mse_rf <- mean(mse_rf)
  mean_mae_rf <- mean(mae_rf)
  mean_rmse_rf <- mean(rmse_rf)
  rsquared_rf <- mean(rsquared_rf)
  
  mean_mse_cf <- mean(mse_cf)
  mean_mae_cf <- mean(mae_cf)
  mean_rmse_cf <- mean(rmse_cf)
  rsquared_cf <- mean(rsquared_cf)
  
  # cache these results:
  save(list_variables, vimp_gini.df, vimp_permut1.df, vimp_permut2.df, dn_iter_gini, mean_mse_rf, mean_mae_rf, mean_rmse_rf, rsquared_rf, mean_mse_cf, mean_mae_cf, mean_rmse_cf, rsquared_cf,
       file="./cached_results/daltonism_var_importance.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/daltonism_var_importance.RData")
}
```

The Root Mean Squared Error (RMSE) for `randomForest` function is `r round(sqrt(mean_mse_rf)*100, 2)` and the R-squared explained variance is `r round(rsquared_rf*100, 2)`.

```{r plot rforest dalto gini, fig.width=5, fig.cap=capFig("Variable importance according to randomForest algorithm using the Gini index.")}
# Plot the result for Gini
ggplot(mapping = aes(x = reorder(namevar, mean_all), y = mean_all, fill=reorder(namevar, mean_all)), 
       data = vimp_gini.df) +
       theme_bw() +
       geom_col(fill="tan1", color="black") +
       theme(axis.text.x = element_text(angle = 60, hjust = 1, size =12, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =12, face = "bold"),
             axis.title=element_text(size=15),
             legend.position="none") +
      labs(x ="", y = paste0("Mean of ",dn_iter_gini," iterations")) +
    scale_x_discrete(labels=c("subsistence" = "Subsistence mode", "longitude_family_r" = "Long of family origin", "latitude_family_r" = "Lat of family origin", "dist2ocean_r" = "Dist to ocean","dist2rivers_r" = "Dist to rivers", "longitude_r" = "Longitude", "altitude_r" = "Altitude", "dist2water_r" = "Dist to water", "clim_PC1" = "Climate PC1", "clim_PC2" = "Climate PC2", "dist2lakes_r" = "Dist to lakes", "latitude_r" = "Latitude", "log_popSize" = "Population size", "UVB_r" = "UVR","exists_blue" = "exists_blue", "n_colors"="Nb of colour categ")) +
      guides(fill=FALSE)
```

```{r plot rforest dalto acc, fig.width=5, fig.cap=capFig("Variable importance according to randomForest algorithm using the accuracy index.")}

# Plot the result
ggplot(mapping = aes(x = reorder(namevar, mean_all), y = mean_all, fill=reorder(namevar, mean_all)), 
       data = vimp_permut1.df) +
       theme_bw() +
       geom_col(fill="tan1", color="black") +
       labs(x ="", y = paste0("Mean of ",dn_iter_gini," iterations")) +
        theme(axis.text.x = element_text(angle = 60, hjust = 1, size =13, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =13, face = "bold"),
             axis.title=element_text(size=16),
             legend.position="none") +
  scale_x_discrete(labels=c("subsistence" = "Subsistence mode", "longitude_family_r" = "Long of family origin", "latitude_family_r" = "Lat of family origin", "dist2ocean_r" = "Dist to ocean","dist2rivers_r" = "Dist to rivers", "longitude_r" = "Longitude", "altitude_r" = "Altitude", "dist2water_r" = "Dist to water", "clim_PC1" = "Climate PC1", "clim_PC2" = "Climate PC2", "dist2lakes_r" = "Dist to lakes", "latitude_r" = "Latitude", "log_popSize" = "Population size", "UVB_r" = "UVR","exists_blue" = "exists_blue", "n_colors"="Nb of colour categ")) +
      guides(fill=FALSE)
```

The Root Mean Squared Error (RMSE) for `cForest` function is `r round(sqrt(mean_mse_cf)*100, 2)` and the R-squared explained variance is `r round(rsquared_cf*100, 2)`.

```{r plot cforest dalto acc, fig.width=5, fig.cap=capFig("Variable importance according to cforest algorithm using the accuracy index.")}

# Plot the result
ggplot(mapping = aes(x = reorder(namevar, mean_all), y = mean_all, fill=reorder(namevar, mean_all)), 
       data = vimp_permut2.df) +
       theme_bw() +
       geom_col(fill="tan1", color="black") +
       labs(x ="", y = paste0("Mean of ",dn_iter_gini," iterations")) +
        theme(axis.text.x = element_text(angle = 60, hjust = 1, size =13, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =13, face = "bold"),
             axis.title=element_text(size=16),
             legend.position="none") +
  scale_x_discrete(labels=c("subsistence" = "Subsistence mode", "longitude_family_r" = "Long of family origin", "latitude_family_r" = "Lat of family origin", "dist2ocean_r" = "Dist to ocean","dist2rivers_r" = "Dist to rivers", "longitude_r" = "Longitude", "altitude_r" = "Altitude", "dist2water_r" = "Dist to water", "clim_PC1" = "Climate PC1", "clim_PC2" = "Climate PC2", "dist2lakes_r" = "Dist to lakes", "latitude_r" = "Latitude", "log_popSize" = "Population size", "UVB_r" = "UVR","exists_blue" = "exists_blue", "n_colors"="Nb of colour categ")) +
      guides(fill=FALSE)


```

Thus, we find that *UVR* incidence is the most important predictor, but also that *latitude* and the presence of a *blue* word matter. 
Depending on the method used, *climate PC1*, *climate PC2*, *longitude* and *subsistence* strategy are also included in the set of relevant predictors.

These results are consistent with the results from the regression analysis, except for the inclusion here of climate variables; moreover, the latitude and longitude of language family origins do not appear to be important variables here.


### Decision tree

We used simple decision trees as implemented by `ctree` [@ctree_2006].

````{r decision tree dalto, fig.cap=capFig("Simple decision tree.")}
tree2 <- ctree (daltonism ~  UVB_r + exists_blue  + subsistence + latitude_r + longitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2ocean_r + dist2lakes_r + dist2rivers_r + n_colors + latitude_family_r + longitude_family_r, 
                data = d_colors, 
                control = ctree_control(testtype = "MonteCarlo", minsplit = 6, minbucket = 6, maxdepth=30))

plot(tree2,  gp = gpar(fontsize = 11))
```

To understand what this decision tree does, we plotted which populations have less/more than -0.337 and -1.315 for z-scored UVR, less/more than 8.275 for dist2ocean_rs, and less/more than 0.793 for latitude of language family origin.

```{r plot dec tree output dalto 1}
# write number climate PC2 for visualization
nb_lon = 0.574
nb_UVB = -0.337
nb_UVB2 = -1.315
nb_distocean = 8.275

d_colors_map <- d_colors
# create range
d_colors_map$lon_range <- cut(d_colors_map$longitude_r, c(-Inf, nb_lon, Inf), 
                                labels=c(paste("less than", nb_lon), paste("more than", nb_lon)))

d_colors_map$UVB_range2 <- cut(d_colors_map$UVB_r, c(-Inf, nb_UVB, Inf), 
                                labels=c(paste("less than", nb_UVB), paste("more than", nb_UVB)))

d_colors_map$UVB_range3 <- cut(d_colors_map$UVB_r, c(-Inf, nb_UVB2, Inf), 
                                labels=c(paste("less than", nb_UVB2), paste("more than", nb_UVB2)))

d_colors_map$docean_range <- cut(d_colors_map$dist2ocean_r, c(-Inf, nb_distocean, Inf), 
                                labels=c(paste("less than", nb_distocean), paste("more than", nb_distocean)))
```

```{r plot dec tree output dalto 2, fig.cap=capFig("UVR (z-scored) as split by the decision tree.")}
# MAP UVB
ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(data= d_colors_map, aes(x =longitude, y = latitude, color=UVB_range2, shape=dalto_range)) +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(color="UVR")

# MAP UVB
ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(data= d_colors_map, aes(x =longitude, y = latitude, color=UVB_range3, shape=dalto_range)) +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
  labs(color="UVR")
```


```{r plot dec tree output dalto 3, fig.cap=capFig("Log of distance to oceans as split by the decision tree.")}
# MAP dist2ocean
ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(data=d_colors_map, aes(x = longitude, y = latitude, color=docean_range, shape=dalto_range)) +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
    labs(color="dist to ocean")
```

```{r plot dec tree output dalto 4, fig.cap=capFig("Latitude of language family origins as split by the decision tree.")}

# MAP longitude 
ggplot() + theme_bw() +
  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group) ,fill = "grey") +
  geom_point(data=d_colors_map, aes(x = longitude, y =latitude, color=lon_range, shape=dalto_range)) +
  theme(legend.position = c(0.75, 0.5), 
      legend.justification = c(1, 1), 
      legend.title = element_text(size = 9), 
      legend.text = element_text(size = 10)) +
   labs(color="Longitude")

```


Here, *UVR* is the main and first node in the tree; as suggested by the regression analysis, *exists_blue* and *subsistence* mode play important roles:

-  when UVR incidence is high, populations having a word for *exists_blue* tend to have higher frequencies of abnormal color perception than populations lacking a word for blue, 
-  when UVR incidence is low and the population's *subsistence* strategy is based on hunting, gathering and foraging, the frequency of abnormal color perception is likely to be low.



### Other machine learning techniques

Here, we compare the performance of *Bayesian mixed-effects models*, *RandomForest* algorithm and *Support Vector Machine* (SVM).
For more information about:

- *Bayesian mixed-effects models*, go to [Regression models]
- *Random Forest* algorithm, go to [Random forests]
- *SVM* algorithm, go to [Other machine learning techniques]


*daltonism* is a continuous variable, so we will use 4 measures of **regression** model accuracy:

 -  **MAE** (Mean absolute error) represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.
 -  **MSE** (Mean Squared Error) represents the difference between the original and predicted values extracted by squared the average difference over the data set.
 -  **RMSE** (Root Mean Squared Error) is the error rate by the square root of MSE.
 -  **R-squared** (Coefficient of determination) represents the coefficient of how well the values fit compared to the original values. The value from 0 to 1 interpreted as percentages. The higher the value is, the better the model is.

<img src="./input_files/formulas.jpg" alt="Formulas" style="height:in;"/>


```{r machine learning daltonism, include=FALSE}
#rsq <- function (x, y) cor(x, y) ^ 2

# ------- RANDOM FOREST --------- #

if( !file.exists("./cached_results/daltonism_mach_learning.RData") )
{
  
  # initialize variable
  mae_all<- data.frame(NULL) ; rmse_all<- data.frame(NULL) ; rsquared_all <- data.frame(NULL)  ; mse_all <- data.frame(NULL) ;

  
  # fit algorithm for N iteration
  for (k in 1:30){
    
    # split data in training and testing set
    data_split <- initial_split(d_colors, prop=0.80, strata = "daltonism_r")
    data_train <- training(data_split)
    data_test <- testing(data_split)
    
    # ----- RANDOM FOREST ----- #
    
    # fit randomforest on training dat
    rf.db = randomForest(daltonism_r ~ UVB + exists_blue  + subsistence + longitude_r + latitude_r + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2ocean_r + dist2lakes_r + dist2rivers_r + n_colors + latitude_family_r + longitude_family_r, data = data_train)
    
    # predict on testing set
    pred <- predict(object = rf.db, newdata = data_test)
    
    # check the difference between actual and predicted value
    vec_original_predicted <- abs(data_test$daltonism_r - pred )
    
    # compute mse, mae, rmse, Rsquared and save those data in table
    mse_all[k, "randomForest"] <- mean((vec_original_predicted)^2)
    mae_all[k, "randomForest"] <- mean(abs(vec_original_predicted))
    rmse_all[k, "randomForest"] <- sqrt(mean((vec_original_predicted)^2))
    rsquared_all[k, "randomForest"] <- 1-(sum((vec_original_predicted)^2)/sum((data_test$daltonism_r - mean(data_test$daltonism_r))^2))
    
    # ----- BRMS ---- #
    
    # fit brms model on training data
    brms_model <- brm(daltonism_r ~ UVB + exists_blue  + subsistence + longitude_r + latitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2ocean_r + dist2lakes_r + dist2rivers_r + n_colors + latitude_family_r + longitude_family_r + (1 | macroarea) + (1 | glottocode_family), 
                      family="beta",
                     data=data_train,
                     cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
                     control=brms_control); 
  
    # predict on testing set
    pred <- predict(brms_model, type = "response", newdata= data_test, allow_new_levels=TRUE)
    
    # check the difference between actual and predicted value
    vec_original_predicted <- abs(data_test$daltonism_r - pred[,1] )

    # compute mse, mae, rmse, R2 and save those data in table
    mse_all[k, "brms"] <- mean((vec_original_predicted)^2)
    mae_all[k, "brms"] <- mean(abs(vec_original_predicted))
    rmse_all[k, "brms"] <- sqrt(mean((vec_original_predicted)^2))
    rsquared_all[k, "brms"] <-  1-(sum((vec_original_predicted)^2)/sum((data_test$daltonism_r - mean(data_test$daltonism_r))^2))
    
    # ------- SVM -------- #
    
    # find best parameters for SVM
    tuned_parameters <- tune.svm(daltonism_r ~ UVB + exists_blue  + subsistence + longitude_r + latitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2ocean_r + dist2lakes_r + dist2rivers_r + n_colors + latitude_family_r + longitude_family_r, data = data_train, gamma = 10^(-5:-1), cost = 10^(-3:1))
  
    # fit SVM with tuned parameters on training data
    svm_model_after_tune <- svm(daltonism_r ~ UVB + exists_blue  + subsistence + longitude_r + latitude_r  + log_popSize + clim_PC1 + clim_PC2 + altitude_r  + dist2ocean_r + dist2lakes_r + dist2rivers_r + n_colors + latitude_family_r + longitude_family_r, data_train, kernel="radial", cost=tuned_parameters$best.parameters[1,2], gamma=tuned_parameters$best.parameters[1,1])
  
    # predict on testing set
    pred <- predict(object = rf.db, newdata = data_test)
  
    # check the difference between actual and predicted value
    vec_original_predicted <- abs(data_test$daltonism_r - pred)
    
    # compute mse, mae, rmse, R2 and save those data in table
    mse_all[k, "SVM"] <-  mean((vec_original_predicted)^2)
    mae_all[k, "SVM"] <- mean(abs(vec_original_predicted))
    rmse_all[k, "SVM"] <- sqrt(mean((vec_original_predicted)^2))
    rsquared_all[k, "SVM"] <- 1-(sum((vec_original_predicted)^2)/sum((data_test$daltonism_r - mean(data_test$daltonism_r))^2))
  }
  
  # ------- PUT ALL VALUES TOGETHER --------- #
  
  # mean those values
  MSE <- colMeans(mse_all)
  MAE <- colMeans(mae_all)
  RMSE <- colMeans(rmse_all)
  RSquared <- colMeans(rsquared_all)
 
  # create dataframe with vectors
  df_final <- data.frame(MSE, MAE, RMSE, RSquared)
  df_final$Algorithm <- rownames(df_final)

  # change dataframe format
  df_final_long <- gather(df_final, test, percentage, MSE:RSquared, factor_key=TRUE)

  # cache these results:
  save(df_final_long,
       file="./cached_results/daltonism_mach_learning.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/daltonism_mach_learning.RData")
}

```

**Model with all covariates:**

*MAE and RMSE*:

```{r plot mach learn dalto all cov mae rmse, fig.cap=capFig("Measures of model regression accuracy (MAE and RMSE), with the model including all covariates.")}
ggplot(data=df_final_long[df_final_long$test=='MAE' | df_final_long$test=='RMSE',], aes(x=test, y=percentage*100, fill=Algorithm)) +
  theme_bw() +
  geom_bar(stat="identity", position=position_dodge()) +
  labs( x ="", y = "") +
  theme(axis.text.x = element_text(size =13, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =13, face = "bold"),
             axis.title=element_text(size=13),
             legend.title=element_text(size =13),
             legend.text=element_text(size =13)) +
  scale_fill_brewer(palette="Paired")
```

The unit in this graph is linked to *daltonism* unit: as a reminder, *daltonism* varies between `r min(d_colors$daltonism)` and `r max(d_colors$daltonism)` (percentage of color-blind males in the population). The values for *MSE*, *RMSE* and *MAE* in a good model tend to be close to 0.

However, the unit for *Rsquared* is a percentage varying between 0 and 100, where a good model tends to be close to 100:

*Rsquared*:

```{r plot mach learn dalto all cov rsquared, fig.cap=capFig("Measures of model regression accuracy (R-squared), with the model including all covariates.")}
ggplot(data=df_final_long[df_final_long$test=='RSquared',], aes(x=Algorithm, y=percentage*100, fill=Algorithm)) +
  geom_bar(stat="identity", position=position_dodge()) +
  theme_bw() +
  labs( x ="", y = "Percentage of variance") +
  scale_fill_brewer(palette="Paired") +
  guides(fill=FALSE) +
  theme(axis.text.x = element_text(size =15, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =15, face = "bold"),
             axis.title=element_text(size=17),
             legend.title=element_text(size =15),
             legend.text=element_text(size =15)) +
  ylim(c(0, 100))

```


```{r machine learning daltonism simple model, include=FALSE}
#rsq <- function (x, y) cor(x, y) ^ 2

# ------- RANDOM FOREST --------- #

if( !file.exists("./cached_results/daltonism_mach_learning_simple.RData") )
{
  
  # initialize variable
  mae_all<- data.frame(NULL) ; rmse_all<- data.frame(NULL) ; rsquared_all <- data.frame(NULL) ; mse_all <- data.frame(NULL) ;
  
  # fit algorithm for N iteration
  for (k in 1:30){
    
    # split data in training and testing set
    data_split <- initial_split(d_colors, prop=0.80, strata = "daltonism_r")
    data_train <- training(data_split)
    data_test <- testing(data_split)
    
    # ---- RANDOM FOREST ----- #
    
    # fit randomforest on training dat
    rf.db = randomForest(daltonism_r ~  UVB_r + exists_blue  + latitude_family_r , data = data_train)
    
    # predict on testing set
    pred <- predict(object = rf.db, newdata = data_test)
    
    # check the difference between actual and predicted value
    vec_original_predicted <- abs(data_test$daltonism_r - pred )
    
    # compute mse, mae, rmse, Rsquared and save those data in table
    mse_all[k, "randomForest"] <- mean((vec_original_predicted)^2)
    mae_all[k, "randomForest"] <- mean(abs(vec_original_predicted))
    rmse_all[k, "randomForest"] <- sqrt(mean((vec_original_predicted)^2))
    rsquared_all[k, "randomForest"] <- 1-(sum((vec_original_predicted)^2)/sum((data_test$daltonism_r - mean(data_test$daltonism_r))^2))
    
    # ------ BRMS ----- #
    
    # fit brms model on training data
    brms_model <- brm(daltonism_r ~  UVB_r  + exists_blue  + I(latitude_family_r^2) + (1 | macroarea) + (1 | glottocode_family), 
         data=data_train,
         family="beta",
         cores=brms_ncores, iter=brms_iter, warmup=brms_warmup, thin=brms_thin, save_all_pars=TRUE,
         control=brms_control); 
  
    # predict on testing set
    pred <- predict(brms_model, type = "response", newdata= data_test, allow_new_levels=TRUE)
    
    # check the difference between actual and predicted value
    vec_original_predicted <- abs(data_test$daltonism_r - pred[,1] )
    
    # compute mse, mae, rmse, R2 and save those data in table
    mse_all[k, "brms"] <- mean((vec_original_predicted)^2)
    mae_all[k, "brms"] <- mean(abs(vec_original_predicted))
    rmse_all[k, "brms"] <- sqrt(mean((vec_original_predicted)^2))
    rsquared_all[k, "brms"] <- 1-(sum((vec_original_predicted)^2)/sum((data_test$daltonism_r - mean(data_test$daltonism_r))^2))
    
    # ----- SVM ------ #
    
    # find best parameters for SVM
    tuned_parameters <- tune.svm(daltonism_r ~  UVB_r + exists_blue  + latitude_family_r , data = data_train, gamma = 10^(-5:-1), cost = 10^(-3:1))
  
    # fit SVM with tuned parameters on training data
    svm_model_after_tune <- svm(daltonism_r ~  UVB_r + exists_blue  + I(latitude_family_r^2) , data_train, kernel="radial", cost=tuned_parameters$best.parameters[1,2], gamma=tuned_parameters$best.parameters[1,1])
  
    # predict on testing set
    pred <- predict(object = rf.db, newdata = data_test)
  
    # check the difference between actual and predicted value
    vec_original_predicted <- abs(data_test$daltonism_r - pred)
    
    # compute mse, mae, rmse, R2 and save those data in table
    mse_all[k, "SVM"] <- mean((vec_original_predicted)^2)
    mae_all[k, "SVM"] <- mean(abs(vec_original_predicted))
    rmse_all[k, "SVM"] <- sqrt(mean((vec_original_predicted)^2))
    rsquared_all[k, "SVM"] <- 1-(sum((vec_original_predicted)^2)/sum((data_test$daltonism_r - mean(data_test$daltonism_r))^2))
    
  }
  
  # mean those values
  MSE <- colMeans(mse_all)
  MAE <- colMeans(mae_all)
  RMSE <- colMeans(rmse_all)
  RSquared <- colMeans(rsquared_all)
 
  # ------- PUT ALL VALUES TOGETHER --------- #
  
  # create dataframe with vectors
  df_final <- data.frame(MSE, MAE, RMSE, RSquared)
  df_final$Algorithm <- rownames(df_final)
  
  # change dataframe format
  df_final_long <- gather(df_final, test, percentage, MSE:RSquared, factor_key=TRUE)

  # cache these results:
  save(df_final_long,
       file="./cached_results/daltonism_mach_learning_simple.RData", compress="xz", compression_level=9)
} else
{
  load("./cached_results/daltonism_mach_learning_simple.RData")
}

```

**Model with only UVR, presence of a blue word, quadratic effect of latitude of putative language family origin:**

*MAE and RMSE*:

```{r plot mach learn dalto set of cov mae rmse, fig.cap=capFig("Measures of model regression accuracy (MAE and RMSE), with the model including only UVR, exists_blue, latitude² of language family origins.")}
ggplot(data=df_final_long[df_final_long$test=='MAE' | df_final_long$test=='RMSE',], aes(x=test, y=percentage*100, fill=Algorithm)) +
  theme_bw() +
  geom_bar(stat="identity", position=position_dodge()) +
   theme(axis.text.x = element_text(size =13, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =13, face = "bold"),
             axis.title=element_text(size=13),
             legend.title=element_text(size =13),
             legend.text=element_text(size =13)) +
  labs( x ="", y = "") +
  scale_fill_brewer(palette="Paired")
```


```{r plot mach learn dalto set of cov rsquared, fig.cap=capFig("Measures of model regression accuracy (R-squared),with the model including only UVR, exists_blue, latitude² of language family origins.")}
ggplot(data=df_final_long[df_final_long$test=='RSquared',], aes(x=Algorithm, y=percentage*100, fill=Algorithm)) +
  geom_bar(stat="identity", position=position_dodge()) +
  theme_bw() +
  labs( x ="", y = "Percentage of variance") +
  scale_fill_brewer(palette="Paired") +
   theme(axis.text.x = element_text(size =15, face = "bold"), 
             axis.text.y = element_text(hjust = 1, size =15, face = "bold"),
             axis.title=element_text(size=17),
             legend.title=element_text(size =15),
             legend.text=element_text(size =15)) +
  guides(fill=FALSE) +
  ylim(c(0, 100))

```

The performance rate is very similar for all algorithms, and the explained variance is quite low for *bayesian mixed-effect model*, *random forest* and *SVM*. Thus, a simple mixed-effect bayesian model is enough to catch the explanatory power of our variables for predicting daltonism. 

However, our variables do not explain a full account of the variance of daltonism rate, and the error rate is still quite high.

*Note:* In previous parts, when we performed *bayesian mixed-effect model* and *random forest* algorithm, we computed the R-squared and we got estimates varying between 30 and 45%. However, these estimation were made on the full dataset, and not on only testing set. 


```{r echo=FALSE, message=FALSE, warning=FALSE}
# What I wrote for the previous results...

#The regression performance of *SVM* is higher than regression performance for *randomForest* and *brms* when all covariates are included. However, when only *UVR*, *subsistence mode*, *exists_blue*, *latitude* and *longitude* of language family are present, all algorithm give the same performance. 

#*SVM* can create complex relationships between the variables and exploit their full potential. We assume that some information present in the data can not be captured with simple bayesian mixed-effect models or random forest algorithm.
```

### Conclusions

Thus:

- *UVR* incidence is strongly negatively related to the population frequency of red-green abnormal color perception, even after controlling for various ecological and cultural confounds; 
- *subsistence strategy* is also significantly associated, with hunter-gatherers being less likely to have red-green abnormal color perception; 
- having a *dedicated word for blue* is also positively related, and we suggest that this is in fact a proxy for high levels of abnormal red-green color perception (as per the 1^st^ hypothesis);
- the influence of *latitude* is fully mediated by UVR incidence;
- depending on the particular method used, *climate* and the putative geographic origin of language family may be relevant predictors.

<br/>


# Discussion and conclusions


In this study, I investigated if differences in eye physiology across populations may help explain differences in the colour lexicon and on the frequency of colour blindness. By performing Point Pattern Analysis, mediation analysis, mixed-effects Bayesian regressions, decision trees/random forests and Support Vector Machines on a set of 142 populations, I explored the relationship between UV incidence and the presence of a specific term for blue on the one hand, and between UV incidence and the frequency of red-green abnormal colour perception in males, on the other hand. 

## 1. Point Pattern Analysis

Point pattern analysis operates in a specific conceptual framework where points are events with an expected probability of occurrence at any location. However, the location of populations is constrained by many factors. Analysis of spatial randomness and dependence added to the covariate analysis highlighted the obvious repartition of languages on earth: humans inhospitable places and choose to live on continents (as opposed to water). Our dataset sample is quite representative of the world populations, although our sample seems very rich in Europe and India, whereas other regions like Australia and Papua New Guinea are not as well-covered. 

We also found that our points attributes (i. e. exists_blue and daltonism) are not dispersed randomly across populations. Indeed, both spatial autocorrelation and neighbouring coefficient analysis indicate that the same attributes tend to be located in the same areas. 

Furthermore, genetic distance (computed with ultrametric data imputation) results in the highest autocorrelation for colour blindness and the presence of a specific blue term. As inherited red-green colour blindness is due to variation at the level of the DNA sequence (@stelzer_genecards_2016), it is tempting to suggest that genetic distance is consistent with the physiological explanation of colour blindness. Indeed, we assume that overall genetic distances reflect population demographic history, which contains the history of the color-blindness-related DNA sequences. 

Yet, the association between genetic distance and the distribution of a word for blue is not straightforward to interpret. As (@komarova_evolutionary_nodate) suggest in their evolutionary models, colour blindness potentially affects the colour lexicon. (@lindsey_color_2002) also claimed that abnormal colour perception in the green-blue range would change colour vocabulary, and especially the presence of a specific term for blue. One could also say that populations that have a short genetic distance between each other would share a common history and culture which explain similarities in colour lexicons.

## 2. Hypothesis 1

**Introduction:**

There is a lot of evidence supporting the universalist perspective (@heider_structure_1972; @berlin_basic_1991; @regier_language_2009), which proposes that the development of the colour lexicon is very similar across all languages and human groups, due to shared commonalities in the eye physiology and cognition. In this thesis, I do not directly challenge this finding, but try to understand how patterns of variation between languages in the presence of a specific term for "blue" could emerge. Contrary to previous universalist work, which assume that the physiological basis of vision in the eye is the same for all humans, I do not bring the eye physiology as a factor driving similarities for colour vocabularies in the world. Instead, I assume that differences in the eye physiology could be responsible for changes in colour perception; in doing so, I bring the eye physiology as a factor driving variation across languages.

**Our result's summary:**

As suggested by (@lindsey_color_2002), I found that UV incidence is a predictor for exists_blue. However, other variables in our dataset are also predictive for the presence of a specific term for blue, such as the number of colour categories which has a strong positive effect. Other cultural (population size and subsistence mode), environmental (distance to lakes) and geographical variables (latitude and longitude of putative language family origins) are also included in the set of relevant predictors. Finally, the mediation analysis showed that the effect of latitude on exists_blue is fully mediated by UV incidence. 

**Explanation of our results according to the relativist perspective, and relative contribution of different factor to the construction of colour categories:**

The relativist perspective, as presented by the work of @davidoff_colour_1999 and @roberson_development_2004, has highlighted the idea that different environments and cultures may be the source of colour lexicons differences. In this thesis, the addition of other (1) cultural, (2) environmental and (3) genetic variables in the model put forward their relative contribution to understanding the presence of a specific word for blue. 

 (1) *Cultural.* @berlin_basic_1991 proposed that the development of dyeing techniques would lead societies to have a greater need for additional colour terms. Also, several studies suggest a significant association between population size and cultural complexity (@shennan_demography_2001; @henrich_demography_2004; @powell_late_2009; @premo_modeling_2010; @derex_experimental_2013). Following those two assumptions, both our variables number of colour categories and population size would be indicators for cultural complexity in our models. Consequently, the important role of the number of colour categories and the population size in our results, brings forward cultural complexity as one of the main contributors to differences between the colour lexicons in what concerns the presence of a specific word for blue, consistent with @ember_size_1978's findings. @gibson_color_2017 also emphasized the role of culture in the evolution of the colour lexicon. They suggest that « *salient* » objects – to which humans have a need to refer to - are usually characterized by their warm colours (red, yellow...), whereas « *uninformative* » backgrounds are usually cool-coloured (for example with blue, green, and brown colours), leading to the under-representation of cool colours in color lexicons. Because industrialization generate objects distinguishable solely based on colour, they postulate industrialized cultures would have a greater need for additional colour categories, resulting in the appearance of extra categories for « blue », « brown » and « grey » colours.

 (2) *Environmental.* Likewise, the distance to lakes would contribute to the presence of a specific term for blue; however, we do not have any explanation about the reasons underlying this finding. According to @goldstone_perceptual_1998, what is available in the perceptual input affects the development of a particular perceptual skill: thus, it is likely that our physical surroundings could play a role in colour lexicons formation. Though, the lack of predictive strength for the climate variables (which determined if the populations live in a tropical, cool, desertic, or oceanic climates) and other distance to water variables suggest that the effect of our physical surrounding on colour lexicons development is low. It is also possible that we did not include the right measures, but as our variables include a lot of information, it is not very likely to be the case.

 (3) *Genetic.* The genetic distance variables included in the model, which give a partial account of the history of people migrations, did not contribute to the presence of a specific word for blue. However, the multi-dimensional scaling applied to the genetic distance matrices did not result in a high goodness-of-fit, and we believe the variables used were not fully reflecting genetic distances. Also, we know that the history of migrations and contact contributed significantly to colour vocabulary. For example, it is believed that the Japanese started to separate blue and green colours when educational materials were imported from the Allied after WW2, and after coloured pencils have been imported to Japan (@bhatia_crayola-fication_2018). 

Those findings are fully compatible with our first hypothesis, namely that UV incidence drives changes in the colour lexicon. Indeed, we assume that the observed variation in the colour lexicon of the world's languages does not depend on a single underlying factor. In this thesis, I find UV incidence as a significant predictor for the presence of a specific blue term, even after controlling for various ecological and cultural confounds. Thus, even though UV incidence is not the main factor impacting the presence or absence of a specific term, this analysis tends to support the idea that UV incidence has its own contribution to the structure of the colour vocabulary, of course, among other predictors. 

**Causal pattern:**

The pattern found suggests that there may be a causal relationship going from UV incidence to exists_blue, the inverse causal relationship being very unlikely. The chain of causal events in Illustration 6 proposed by @@lindsey_color_2002 is a possible explanation for this relationship, even if I do not exclude the possibility that UV incidence could affect the presence of a specific blue term based on other causal assumptions. As an example, one can postulate that UV incidence could partially impact food production system, thus affecting economic development; however, I believe that our climate variables would reflect more this sort of causal assumptions. Also, UV incidence and exists_blue could be proxies for other variables, but this is probably less likely as we controlled for several cultural and environmental variables. Thus, the causal assumptions linking UV incidence to macular pigment density, and macular pigment density to abnormal colour perception in the green-blue range, remains, in our view, the most probable explanation for the potential effect of UV incidence on colour naming. 

**Role of physiology:**

Therefore, some physical variables  – such as UV incidence – could have an indirect role on semantic categories through their effects on physiology. Geographical location, which partly determines the amount of UV incidence reaching the eye, would also indirectly contribute to colour naming through its mediation relationship with UV incidence and, consequently, physiology.


## 3. Hypothesis 2

**Result's summary:**

Using the same methods, we found that UV incidence is strongly negatively related to the population frequency of red-green abnormal colour perception, even after controlling for various ecological and cultural confounds. UV incidence is the strongest predictor according to both random forests and Bayesian models (if latitude is excluded). Indeed, the effect of latitude on daltonism is fully mediated by UV incidence. 

**Causal pattern:**

The strength of UV incidence as a predictive factor for daltonism may suggest, at face value, that UV incidence plays a role in determining male red-green colour blindness rate. If UV incidence and daltonism are not proxies for other unmeasured variables, then a possible causal relationship may go from UV incidence to red-green colour blindness: UV radiations can cause DNA mutations (@pfeifer_mutations_2005), with skin cancer being an example, but the most common colour blindness conditions are inherited and there is no evidence that UV incidence could cause DNA mutations in gametes. Moreover, the red-green colour blindness mutation is present only in places where the UV incidence rate is low. Therefore, UV incidence cannot be a direct explanation for mutations in the opsin genes. The chain of causality proposed by D. T. @brown_color_2004 and summarized in Illustration 6 becomes then a much more probable explanation describing the strong impact of UV incidence on daltonism. However, I do not set aside the possibility that UV incidence and daltonism are linked through other causal assumptions.

Together, these results put forward the role of physical factors in changing the frequency of  genetic changes by generating different selective pressures in different populations. Like colour vocabulary, colour blindness can be linked to geographical location through its mediating relationship with UV incidence and eye physiology (see Illustration 6). 

**Relative contribution of other variables:**

*Subsistence strategy.* Other variables also play a significant role in predicting daltonism. Corroborating with our hypothesis, subsistence strategy is slightly associated, with hunter-gatherers being less likely to have red-green abnormal colour perception. We can suggest that the selective pressure for hunter-gatherers is higher than in populations based on food production. Interestingly, hunter-gatherers in our dataset have a fairly low colour-blindness rate irrespective of UV incidence, meaning that selective pressure on colour perception may indeed exist. However, even though we attempted to have a representative sample of the world's language, there were few examples of hunter-gatherers communities, making it difficult to conclude about their potential effect in our analysis. 

*exists_blue.* Having a dedicated word for blue is significantly positively related to the frequency of red-green abnormal colour perception: population having a high red-green colour blindness rate tend to have a specific term for blue in their vocabulary. We suggest that this is, in fact, a proxy for high levels of acquired abnormal blue-green colour perception (see 1st hypothesis and Illustration 9). One can also suggest other explanations not linked to our hypothesis: for example, it is possible that red-green colour blindness has a direct impact on colour categorisation. 
	

## 4. The link between the two hypothesis

Our hypotheses did not set a causal effect going from red-green colour-blindness to presence of a blue word in colour lexicons, but only a correlation between the two as they would be both linked to abnormal blue-green colour perception. Yet, we believe that inter-individual variation in abnormal colour perception (such as the inherited abnormal red-green colour blindness) can possibly also lead to a reorganization in colour semantics. 

Mutations on opsin genes, causing deuteranopia, deuteranomaly, protanopia or protanomaly, are known to be responsible for the presence of atypical cone cells, thus affecting the eye physiology. Due to a lack of information about the population frequency specifically of the variants involved in abnormal color perception, I do not have any evidence for mutations on opsin genes to be indirect contributors to colour lexicons through their mediating effect on the eye physiology. However, our results suggest that a defect of perception in the blue-green colour dimension contributes to the development of the colour lexicon. As genetic mutations in the opsin genes are known to be responsible for the presence of red-green colour blind individuals, one may wonder if genetics could remodel colour categories at a higher scale. Computer simulations (@komarova_evolutionary_nodate) suggest that the presence of colour blind agents in a population can cause changes in the number of colour categories and/or the location of colour category boundaries. 

Real world's data do not put forward any evidence for colour lexicons differences in populations where deuteranomaly rates are high; however, places with high red-green abnormal colour perception rates correlate with places where the dyeing system is very developed, making it difficult to draw conclusions. 


The causal relationships suggested by the results of this analysis can be represented graphically as shown in the figure below:

<img src="./input_files/conclusion.png" alt="Results" style="height:in;"/>


## 5. Limitations

However, our analysis has several shortcomings. 

 (1) *Genetic distance matrices*. First, genetic distance matrices computed from ALFRED were computed with only 478 microhaplotypes and SNPs and, therefore, are not representative of the whole human genome. Though, the SNPs selected were extracted from a set of AISNP (Ancestry Inference SNP), which are loci very informative for population history and migration. Also, the matching of populations in our dataset and the populations in ALFRED is imperfect (see Annexe Genetic distance matrix, section Step 4: Link populations from FST table to our dataset’s population. for more information). Also, there is no genetic frequency data concerning the opsin genes, responsible for colour perception. 

 (2) *Location of boundaries in colour categorization*.  Second, our current database is not sufficient to test the claims put forward in computational studies, namely that abnormal colour perception can lead to a redefining of colour categories boundaries, as the exact location of category boundaries are not present in our dataset. 

 (3) *Index of level of exposure to dyeing techniques*. Third, this study lacks an index for population exposure to manufactured goods and their level of exposure to dyeing techniques. We hoped that some of these distinctions may be reflected in the subsistence mode, with hunter-gatherer population having less access to dyed products, and population size, where populations bigger than a few hundred thousand people being more likely to have access to dyeing techniques. Country development indexes were not relevant in our case, as we mainly studied specific populations inside countries. More generally, indexes of development for specific populations or isolated communities are mainly old-fashioned and can create ethical debates. A specific measure for exposure to dyeing technology was too specific to be found in the anthropological databases. 

 (4) *Unique location of each population*. Fifth, the different measures for distance to water (distance to rivers, distance to oceans, ditsance to lakes), as well as UV incidence, could only be computed with a fixed latitude and longitude for each population. However, population with more than a few hundreds people can occupy a large territory. Thus, we expect those variables to be a global estimation (an order of magnitude) for how distant they live from lakes, rivers or oceans, and not a precise indication. 

 (5) *Colour blindness test*. Finally, we assume that our measure of daltonism may be faulty in some ways. Even though the tests used are standardized, this measure can be submitted to variation due to different experimenters, contexts and locations. Though, out of the numerous populations where data for colour blindness was present, a meticulous and rigorous work has been made (@meeussen_colour_2015) to select only populations where the data was of sufficient quality. 


## 6. Conclusion

We brought some evidence that real-world data are consistent with the hypothesis that both red-green colour blindness and the presence of a specific term for blue could be partly determined by UV incidence. Colour vocabulary is directly influenced by environmental and cultural factors, but would also be indirectly influenced through their mediating relationship with human physiology. Physiology, which has already been designated to potentially shape the structure of languages' phonemes (@moisik_anatomical_2017; @dediu_weak_2019; @blasi_human_2019), is then likely to also affect languages' semantic categories. Likewise, physiology would also be a mediator between UV incidence and inherited red-green abnormal colour perception.

Together, our findings suggest that individual-level effects (here, an acquired defect in blue-green colour range) can reshape both colour lexicons and inherited red-green abnormal colour perception. Understanding to which extent thrichromacy is positively selected depending on the subsistence strategy would give additional information to our findings concerning the distribution of abnormal red-green colour blindness in the world.

Also, it would be interesting to investigate to what extent other individual effects, such as the presence of colour-blind individuals in the population, could influence colour categories development. Specifically, it would be interesting to study closer the evolution of colour lexicons on Pingelap island. Rare cases of tetrachromatic individuals (@deeb_molecular_2005), if they expand in the population, would also be a relevant field of investigation for colour lexicons changes. 

In a cross-linguistic work, @malt_how_2013 provide some evidence for a great diversity in how languages label categories. Both universal and relativist factors can explain the pattern of semantic categories seen across the world. However, studying their effect through their mediating effect of physiology provide relevant leads for further exploration. 



# Appendices


## Appendix I. Distance matrices

We use the *fixation Index* (F~ST~) as a measure of the genetic distance between pairs of populations, with high values indicating a high differentiation betwen populations; there are multiple approaches (see, for example, [Wikipedia](https://en.wikipedia.org/wiki/Fixation_index), @holsinger_genetics_2009), but we used here the method implemented in  [`Arlequin 3.5`](http://cmpg.unibe.ch/software/arlequin35/) [@excoffier_arlequin_2010] as detailed below. 

As *primary data*, we used a subset of the [ALFRED (The ALlele FREquency Database)](https://alfred.med.yale.edu/alfred/index.asp) [@rajeevan_alfred_2003], the [FROG-kb (Forensic Resource/Reference on Genetics knowledge base)](http://frog.med.yale.edu/FrogKB/), aimed at forensic applications.
More precisely, we downloaded (as of spring 2020) the following files:

| Panel                                           | File (hyperlink)                                                                 | N | #SNPs[^SNP] | #MHs[^Microhaplotype] |
|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------|----:|----:|----:|
| Microhaplotypes                                 | [*Microhap_alleleF_198.txt*](https://alfred.med.yale.edu/alfred/selectDownload/Microhap_alleleF_198.txt) | 96  | 0   | 198 |
| Seldin's list of 128 AISNPs                     | [*Seldin128_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Seldin128_alleleF.txt)         | 70  | 128 | 0   |
| SNPforID 34-plex                                | [*SNPForId34_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/SNPForId34_alleleF.txt)       | 53  | 34  | 0   |
| KiddLab - Set of 55 AISNPs                      | [*KiddLab55_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/KiddLab55_alleleF.txt)         | 139 | 55  | 0   |
| Kayser's set of 24 Ancestry Informative Markers | [*Kayser24_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Kayser24_alleleF.txt)           | 73  | 24  | 0   |
| Daniele Podini's list of 32 AISNPs              | [*Podini32_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Podini32_alleleF.txt)           | 111 | 29  | 0   |
| Eurasiaplex 23 SNP Panel                        | [*Eurasiaplex23_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Eurasiaplex23_alleleF.txt) | 76  | 23  | 0   |
| Nievergelt's Set of 41AIMs                      | [*Nievergelt41_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Nievergelt41_alleleF.txt)   | 123 | 41  | 0   |
| Overlap set of AISNPs                           | [*Overlap44_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Overlap44_alleleF.txt)         | 72  | 44  | 0   |
| Li's panel of 74 AIMS                           | [*Li74_alleleF.txt*](http://frog.med.yale.edu/FrogKB/download/allele/Li74_alleleF.txt)                   | 67  | 73  | 0   |

These text files were combined into a single file, and we used a custom `R` script (available online at [`mathjoss/Alfred2FST/alfredtxt2arlequin.R`](https://github.com/mathjoss/Alfred2FST/blob/master/alfredtxt2arlequin.R)) to extract one `Arlequin file` (`.arp`) per microhaplotype and per SNP. 
These `.arp` files were then processed with [`Arlequin 3.5`](http://cmpg.unibe.ch/software/arlequin35/) [@excoffier_arlequin_2010] (the parameters can be found in [mathjoss/Alfred2FST/arl_run.ars](https://github.com/mathjoss/Alfred2FST/blob/master/arl_run.ars)), using the output tables `Population average pairwise difference` (and, more precisely, the *corrected average pairwise difference* component), one per input `.arp` file. We averaged these tables using two other custom `R` scripts (available at [mathjoss/Alfred2FST/mean_all_files.R](https://github.com/mathjoss/Alfred2FST/blob/master/mean_all_files.R) and [mathjoss/Alfred2FST/mean_all_files_part2.R](https://github.com/mathjoss/Alfred2FST/blob/master/mean_all_files_part2.R)).
This procedure resulted in average pairwise distance between 145 unique populations, using 96 unique microhaplotypes and 382 unique SNP.

As there is missing data, we used the *additive* and the *ultrametric* methods of data imputation for distance matrices [@de_soete_ultrametric_1984; @lapointe_estimating_1995], followed by *Multi-Dimensional Scaling* (*MDS*); see the `R` script at [mathjoss/Alfred2FST/extract_MDS.R](https://github.com/mathjoss/Alfred2FST/blob/master/extract_MDS.R) for details. 

Finally, we mapped manually the 145 unique populations with F~ST~ distances to the 142 populations in our primary dataset `data_colors.csv`, but the mapping is far from trivial and unambiguous: for example, some ALFRED populations are rather large and imprecise (e.g., "African Americans", "Indian Mixed" ...). 
Consequently, we applied the following procedure: 

  (1) Exact population were linked to each other (e.g., "Basque" and "Yoruba" are present in both);
  (2) For populations in the primary dataset but not present in the F~ST~ distances matrix, we selected its most appropriate general category (e.g. for "Czech" we used "Europeans Mixed", and for "Gujarati", "Indian mixed");
  (3) If there is no appropriate general category, we attempted to find the closest matching population in the F~ST~ distances matrix (generally based on geographic distance and demographic history, as inferred from various sources).
  
The results of the first two matching rules (1 & 2) are in the *genetic_incomplete* column of the `data_colors.csv` primary dataset, while those of all the three rules (1, 2 & 3) are in the *genetic_full* column -- we used this "fully" matched column for further analyses.


[^SNP]: A [*Single Nucleotide Polymorphism* (*SNP*)](https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism) is a common and simple type of genetic variation representing the substitution of a single nucleotide at a specific position in the genome.

[^Microhaplotype]: A [*haplotype*](https://en.wikipedia.org/wiki/Haplotype) has several related meaning, in principle refering to a set of genetic variants that tend to be inherited together; a *microhaplotype* (microhap or *MH*) is a set of small set of closely linked SNPs useful in forensic applications [@oldoni_microhaplotypes_2019]. 

[^openstreetmaps]: Unfortunately, the primary data from [OpenStreetMap](http://openstreetmapdata.com/), the ["Reduced waterbodies as raster masks"](http://openstreetmapdata.com/data/water-reduced-raster) seems to no longer be available for download as of July 2020, so we used here the original data downloaded in March 2018 by Dan Dediu and used in the @bentz_evolution_2018 paper.

[^uv]: Unfortunately, the primary data from [TOMS](http://toms.gsfc.nasa.gov/ery_uv/new_uv/) seems to no longer be available for download as of July 2020, so we used here the original data downloaded in 2012 by Dan Dediu and used in the @meeussen_colour_2015 MSc thesis.

# References


